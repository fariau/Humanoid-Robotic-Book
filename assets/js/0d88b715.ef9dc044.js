"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[220],{8242:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var r=i(4848),s=i(8453);const o={sidebar_position:1,title:"Introduction to Physical AI & Humanoid Robotics",description:"Foundations of Physical AI and Humanoid Robotics",slug:"/intro"},t="Introduction to Physical AI & Humanoid Robotics",a={id:"intro/index",title:"Introduction to Physical AI & Humanoid Robotics",description:"Foundations of Physical AI and Humanoid Robotics",source:"@site/docs/intro/index.mdx",sourceDirName:"intro",slug:"/intro",permalink:"/Humanoid-Robotic-Book/docs/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro/index.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Introduction to Physical AI & Humanoid Robotics",description:"Foundations of Physical AI and Humanoid Robotics",slug:"/intro"},sidebar:"tutorialSidebar",next:{title:"ROS 2 Fundamentals",permalink:"/Humanoid-Robotic-Book/docs/ros2"}},l={},c=[{value:"Overview of Physical AI",id:"overview-of-physical-ai",level:2},{value:"Historical Context",id:"historical-context",level:3},{value:"Humanoid Robotics: The Ultimate Challenge",id:"humanoid-robotics-the-ultimate-challenge",level:2},{value:"Key Components of Humanoid Robots",id:"key-components-of-humanoid-robots",level:3},{value:"Core Technologies",id:"core-technologies",level:2},{value:"Perception Systems",id:"perception-systems",level:3},{value:"Computer Vision",id:"computer-vision",level:4},{value:"Tactile Sensing",id:"tactile-sensing",level:4},{value:"Control Systems",id:"control-systems",level:3},{value:"Balance and Locomotion",id:"balance-and-locomotion",level:4},{value:"ROS2 Integration",id:"ros2-integration",level:2},{value:"Simulation Environment",id:"simulation-environment",level:2},{value:"NVIDIA Isaac Integration",id:"nvidia-isaac-integration",level:2},{value:"Vision Language Action (VLA) Models",id:"vision-language-action-vla-models",level:2},{value:"Hardware Considerations",id:"hardware-considerations",level:2},{value:"For NVIDIA Jetson Users",id:"for-nvidia-jetson-users",level:3},{value:"For High-End GPU Users",id:"for-high-end-gpu-users",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Exercise 1: Basic ROS2 Node",id:"exercise-1-basic-ros2-node",level:3},{value:"Exercise 2: Perception Pipeline",id:"exercise-2-perception-pipeline",level:3},{value:"Exercise 3: Balance Controller",id:"exercise-3-balance-controller",level:3},{value:"Exercise 4: Gazebo Simulation",id:"exercise-4-gazebo-simulation",level:3},{value:"Exercise 5: Hardware Optimization",id:"exercise-5-hardware-optimization",level:3},{value:"MCQs Quiz",id:"mcqs-quiz",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"introduction-to-physical-ai--humanoid-robotics",children:"Introduction to Physical AI & Humanoid Robotics"}),"\n",(0,r.jsx)(n.h2,{id:"overview-of-physical-ai",children:"Overview of Physical AI"}),"\n",(0,r.jsx)(n.p,{children:"Physical AI represents a revolutionary convergence of artificial intelligence and physical systems. Unlike traditional AI that operates primarily in digital spaces, Physical AI integrates intelligence directly into physical agents and environments, creating systems that can perceive, reason, and act in the real world with unprecedented sophistication."}),"\n",(0,r.jsx)(n.p,{children:"The field encompasses several key areas:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embodied Cognition"}),": The idea that intelligence emerges from the interaction between an agent and its physical environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensorimotor Learning"}),": How agents learn through physical interaction with their surroundings"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Decision Making"}),": Processing sensory information and making decisions in real-time for physical tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adaptive Control Systems"}),": Systems that can adjust their behavior based on environmental changes"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"historical-context",children:"Historical Context"}),"\n",(0,r.jsx)(n.p,{children:"The journey toward Physical AI began in the 1950s with early robotics research, but has accelerated dramatically in recent years due to advances in:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Machine Learning"}),": Particularly deep learning and reinforcement learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensing Technologies"}),": LIDAR, computer vision, and tactile sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Power"}),": GPUs and specialized AI hardware"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulation Environments"}),": High-fidelity physics simulators"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"humanoid-robotics-the-ultimate-challenge",children:"Humanoid Robotics: The Ultimate Challenge"}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots represent one of the most ambitious goals in robotics. These systems attempt to replicate human form and function, offering several advantages:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Intuitive Interaction"}),": Humans find it easier to interact with humanoid systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment Compatibility"}),": Designed to operate in human-centric environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Social Acceptance"}),": More readily accepted by humans in social contexts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Versatility"}),": Capable of performing diverse tasks similar to humans"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-components-of-humanoid-robots",children:"Key Components of Humanoid Robots"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Humanoid Robot] --\x3e B[Locomotion System]\r\n    A --\x3e C[Manipulation System]\r\n    A --\x3e D[Sensory System]\r\n    A --\x3e E[Cognitive System]\r\n    A --\x3e F[Communication System]\r\n\r\n    B --\x3e B1[Legs & Feet]\r\n    B --\x3e B2[Balancing Control]\r\n\r\n    C --\x3e C1[Arms & Hands]\r\n    C --\x3e C2[Grasping Mechanisms]\r\n\r\n    D --\x3e D1[Cameras]\r\n    D --\x3e D2[LIDAR]\r\n    D --\x3e D3[Tactile Sensors]\r\n    D --\x3e D4[Microphones]\r\n\r\n    E --\x3e E1[Path Planning]\r\n    E --\x3e E2[Object Recognition]\r\n    E --\x3e E3[Decision Making]\r\n\r\n    F --\x3e F1[Speech Synthesis]\r\n    F --\x3e F2[Gesture Control]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"core-technologies",children:"Core Technologies"}),"\n",(0,r.jsx)(n.h3,{id:"perception-systems",children:"Perception Systems"}),"\n",(0,r.jsx)(n.p,{children:"Modern humanoid robots employ sophisticated perception systems that include:"}),"\n",(0,r.jsx)(n.h4,{id:"computer-vision",children:"Computer Vision"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\ndef detect_objects(frame):\r\n    """\r\n    Detect and identify objects in the robot\'s visual field\r\n    Optimized for [USER_GPU] hardware\r\n    """\r\n    # Load pre-trained model\r\n    net = cv2.dnn.readNetFromDarknet(\'config.cfg\', \'weights.weights\')\r\n\r\n    # Preprocess frame\r\n    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\r\n    net.setInput(blob)\r\n    outputs = net.forward()\r\n\r\n    return outputs\r\n\r\n# Example usage\r\ncap = cv2.VideoCapture(0)\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if ret:\r\n        objects = detect_objects(frame)\r\n        # Process detected objects\n'})}),"\n",(0,r.jsx)(n.h4,{id:"tactile-sensing",children:"Tactile Sensing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- ROS2 URDF configuration for tactile sensors --\x3e\r\n<robot name="humanoid_with_tactile">\r\n  <link name="hand">\r\n    <visual>\r\n      <geometry>\r\n        <mesh filename="hand.dae"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <mesh filename="hand_collision.dae"/>\r\n      </geometry>\r\n    </collision>\r\n  </link>\r\n\r\n  \x3c!-- Tactile sensor array --\x3e\r\n  <gazebo reference="hand">\r\n    <sensor type="contact" name="tactile_sensor">\r\n      <always_on>true</always_on>\r\n      <update_rate>30</update_rate>\r\n      <contact>\r\n        <collision>hand_collision</collision>\r\n      </contact>\r\n    </sensor>\r\n  </gazebo>\r\n</robot>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"control-systems",children:"Control Systems"}),"\n",(0,r.jsx)(n.h4,{id:"balance-and-locomotion",children:"Balance and Locomotion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy import signal\r\n\r\nclass BalanceController:\r\n    def __init__(self, robot_mass, com_height):\r\n        self.mass = robot_mass\r\n        self.com_height = com_height\r\n        self.gravity = 9.81\r\n\r\n    def compute_zmp(self, com_pos, com_vel, com_acc):\r\n        """\r\n        Compute Zero Moment Point for balance control\r\n        """\r\n        zmp_x = com_pos[0] - (self.com_height / self.gravity) * com_acc[0]\r\n        zmp_y = com_pos[1] - (self.com_height / self.gravity) * com_acc[1]\r\n        return np.array([zmp_x, zmp_y, 0])\r\n\r\n    def compute_foot_trajectory(self, start_pos, end_pos, step_height=0.1, steps=20):\r\n        """\r\n        Generate smooth foot trajectory for walking\r\n        """\r\n        t = np.linspace(0, 1, steps)\r\n        x_traj = start_pos[0] + (end_pos[0] - start_pos[0]) * t\r\n        y_traj = start_pos[1] + (end_pos[1] - start_pos[1]) * t\r\n        z_traj = start_pos[2] + step_height * np.sin(np.pi * t)\r\n\r\n        return np.column_stack([x_traj, y_traj, z_traj])\r\n\r\n# Example usage\r\ncontroller = BalanceController(robot_mass=75.0, com_height=0.85)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"ros2-integration",children:"ROS2 Integration"}),"\n",(0,r.jsx)(n.p,{children:"ROS2 serves as the middleware for humanoid robotics, providing:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Message Passing"}),": Efficient communication between nodes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Device Drivers"}),": Standardized interfaces for sensors and actuators"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulation Tools"}),": Gazebo integration for testing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visualization"}),": RViz for debugging and monitoring"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install ROS2 dependencies for humanoid robotics\r\nsudo apt update\r\nsudo apt install ros-humble-desktop-full\r\nsudo apt install ros-humble-gazebo-ros-pkgs\r\nsudo apt install ros-humble-navigation2\r\nsudo apt install ros-humble-nav2-bringup\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# ROS2 node example for humanoid robot control\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import String\r\n\r\nclass HumanoidController(Node):\r\n    def __init__(self):\r\n        super().__init__('humanoid_controller')\r\n\r\n        # Publishers\r\n        self.joint_pub = self.create_publisher(JointState, 'joint_states', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\r\n\r\n        # Subscribers\r\n        self.imu_sub = self.create_subscription(\r\n            String, 'imu_data', self.imu_callback, 10)\r\n\r\n        # Timer for control loop\r\n        self.timer = self.create_timer(0.01, self.control_loop)\r\n\r\n    def control_loop(self):\r\n        # Main control logic here\r\n        joint_msg = JointState()\r\n        joint_msg.name = ['hip_joint', 'knee_joint', 'ankle_joint']\r\n        joint_msg.position = [0.1, 0.2, 0.3]\r\n        self.joint_pub.publish(joint_msg)\r\n\r\n    def imu_callback(self, msg):\r\n        # Process IMU data for balance control\r\n        pass\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    controller = HumanoidController()\r\n    rclpy.spin(controller)\r\n    controller.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"simulation-environment",children:"Simulation Environment"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo provides a realistic physics simulation environment for testing humanoid robots before deployment on real hardware."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Gazebo world configuration --\x3e\r\n<sdf version="1.7">\r\n  <world name="humanoid_world">\r\n    <include>\r\n      <uri>model://ground_plane</uri>\r\n    </include>\r\n    <include>\r\n      <uri>model://sun</uri>\r\n    </include>\r\n\r\n    \x3c!-- Humanoid robot model --\x3e\r\n    <include>\r\n      <uri>model://humanoid_robot</uri>\r\n      <pose>0 0 1 0 0 0</pose>\r\n    </include>\r\n\r\n    \x3c!-- Physics engine configuration --\x3e\r\n    <physics type="ode">\r\n      <max_step_size>0.001</max_step_size>\r\n      <real_time_factor>1.0</real_time_factor>\r\n      <real_time_update_rate>1000</real_time_update_rate>\r\n    </physics>\r\n  </world>\r\n</sdf>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"nvidia-isaac-integration",children:"NVIDIA Isaac Integration"}),"\n",(0,r.jsx)(n.p,{children:"For GPU-accelerated AI, NVIDIA Isaac provides specialized tools and libraries:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS"}),": GPU-optimized perception and navigation nodes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac Sim"}),": High-fidelity simulation environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deep Learning"}),": Optimized for NVIDIA GPUs"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Isaac ROS example for object detection\r\nfrom isaac_ros_tensor_list_interfaces.msg import TensorList\r\nfrom sensor_msgs.msg import Image\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass IsaacObjectDetector:\r\n    def __init__(self):\r\n        # Initialize Isaac-specific components\r\n        # Optimized for [USER_GPU] hardware\r\n        pass\r\n\r\n    def process_tensor(self, tensor_list: TensorList):\r\n        """\r\n        Process tensors from Isaac perception pipeline\r\n        """\r\n        for tensor in tensor_list.tensors:\r\n            if tensor.name == \'detections\':\r\n                # Process object detections\r\n                detections = self.decode_detections(tensor.data)\r\n                return detections\r\n        return []\n'})}),"\n",(0,r.jsx)(n.h2,{id:"vision-language-action-vla-models",children:"Vision Language Action (VLA) Models"}),"\n",(0,r.jsx)(n.p,{children:"VLA models represent the cutting edge of Physical AI, combining visual perception, language understanding, and action generation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:'graph LR\r\n    A[Visual Input] --\x3e C[VLA Model]\r\n    B[Language Input] --\x3e C\r\n    C --\x3e D[Action Output]\r\n\r\n    subgraph "VLA Model Architecture"\r\n        C1[Visual Encoder]\r\n        C2[Language Encoder]\r\n        C3[Action Decoder]\r\n        C1 --\x3e C3\r\n        C2 --\x3e C3\r\n    end\n'})}),"\n",(0,r.jsx)(n.h2,{id:"hardware-considerations",children:"Hardware Considerations"}),"\n",(0,r.jsx)(n.p,{children:"Different hardware configurations significantly impact performance:"}),"\n",(0,r.jsx)(n.h3,{id:"for-nvidia-jetson-users",children:"For NVIDIA Jetson Users"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Optimized for Jetson hardware\r\nimport jetson.inference\r\nimport jetson.utils\r\n\r\ndef jetson_optimized_detection():\r\n    """\r\n    Use Jetson\'s built-in AI accelerators for efficient processing\r\n    """\r\n    net = jetson.inference.detectNet("ssd-mobilenet-v2", threshold=0.5)\r\n    camera = jetson.utils.gstCamera(1280, 720, "/dev/video0")\r\n    display = jetson.utils.glDisplay()\r\n\r\n    while display.IsOpen():\r\n        img, width, height = camera.CaptureRGBA()\r\n        detections = net.Detect(img, width, height)\r\n        display.RenderOnce(img, width, height)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"for-high-end-gpu-users",children:"For High-End GPU Users"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Optimized for high-end GPUs\r\nimport torch\r\nimport torchvision\r\n\r\ndef gpu_optimized_processing():\r\n    """\r\n    Leverage high-end GPU capabilities for complex computations\r\n    """\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n    model = torchvision.models.resnet50(pretrained=True).to(device)\r\n\r\n    # Perform complex computations using GPU acceleration\r\n    # Optimized for [USER_GPU] hardware specifications\n'})}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Physical AI"})," bridges the gap between digital AI and physical systems, enabling real-world interaction and manipulation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Humanoid robotics"})," presents unique challenges requiring sophisticated integration of perception, control, and cognitive systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ROS2"})," provides the essential middleware for communication between different components of the robotic system."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simulation"})," is crucial for testing and validation before deployment on real hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hardware optimization"})," is essential for achieving real-time performance in robotic applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VLA models"})," represent the future of Physical AI by integrating vision, language, and action in unified architectures."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-basic-ros2-node",children:"Exercise 1: Basic ROS2 Node"}),"\n",(0,r.jsx)(n.p,{children:"Create a ROS2 node that publishes joint states for a simple humanoid model. The node should publish messages at 50Hz with realistic joint angles for standing posture."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-perception-pipeline",children:"Exercise 2: Perception Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"Implement a basic computer vision pipeline that detects colored objects in an image and publishes their positions. Use OpenCV and ROS2 message types."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-balance-controller",children:"Exercise 3: Balance Controller"}),"\n",(0,r.jsx)(n.p,{children:"Implement a simple balance controller that maintains the center of mass within the support polygon. Test with simulated IMU data."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-4-gazebo-simulation",children:"Exercise 4: Gazebo Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Create a simple Gazebo world with a humanoid robot model. Add a basic controller that makes the robot stand upright."}),"\n",(0,r.jsx)(n.h3,{id:"exercise-5-hardware-optimization",children:"Exercise 5: Hardware Optimization"}),"\n",(0,r.jsx)(n.p,{children:"Optimize a simple neural network inference pipeline for your specific hardware (GPU/Jetson). Compare performance between different optimization strategies."}),"\n",(0,r.jsx)(n.h2,{id:"mcqs-quiz",children:"MCQs Quiz"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What does Physical AI primarily focus on?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A) Digital-only applications"}),"\n",(0,r.jsx)(n.li,{children:"B) Integration of intelligence into physical agents and environments"}),"\n",(0,r.jsx)(n.li,{children:"C) Purely software-based solutions"}),"\n",(0,r.jsx)(n.li,{children:"D) Cloud-based computing"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Which of the following is NOT a key component of humanoid robots?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A) Locomotion System"}),"\n",(0,r.jsx)(n.li,{children:"B) Manipulation System"}),"\n",(0,r.jsx)(n.li,{children:"C) Sensory System"}),"\n",(0,r.jsx)(n.li,{children:"D) Purely Digital Interface"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Answer: D"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What does ZMP stand for in robotics?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A) Zero Moment Point"}),"\n",(0,r.jsx)(n.li,{children:"B) Zero Motion Parameter"}),"\n",(0,r.jsx)(n.li,{children:"C) Zed Motor Position"}),"\n",(0,r.jsx)(n.li,{children:"D) Zone Motion Protocol"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Answer: A"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Which middleware is commonly used in humanoid robotics?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A) ROS2"}),"\n",(0,r.jsx)(n.li,{children:"B) MQTT"}),"\n",(0,r.jsx)(n.li,{children:"C) HTTP"}),"\n",(0,r.jsx)(n.li,{children:"D) TCP/IP"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Answer: A"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What do VLA models combine?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A) Vision, Language, and Action"}),"\n",(0,r.jsx)(n.li,{children:"B) Velocity, Location, and Acceleration"}),"\n",(0,r.jsx)(n.li,{children:"C) Voltage, Load, and Amperage"}),"\n",(0,r.jsx)(n.li,{children:"D) Virtual, Linear, and Adaptive"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Answer: A"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Robotics: Control, Sensing, Vision, and Intelligence" by Fu, Gonzalez, and Lee'}),"\n",(0,r.jsx)(n.li,{children:'"Probabilistic Robotics" by Thrun, Burgard, and Fox'}),"\n",(0,r.jsx)(n.li,{children:'"Humanoid Robotics: A Reference" by Veloso'}),"\n",(0,r.jsx)(n.li,{children:'"Introduction to Autonomous Manipulation" by Albu-Sch\xe4ffer and Hirzinger'}),"\n",(0,r.jsx)(n.li,{children:'"Modern Robotics: Mechanics, Planning, and Control" by Lynch and Park'}),"\n",(0,r.jsxs)(n.li,{children:["NVIDIA Isaac Documentation: ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/",children:"https://docs.nvidia.com/isaac/"})]}),"\n",(0,r.jsxs)(n.li,{children:["ROS2 Documentation: ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Gazebo Simulation: ",(0,r.jsx)(n.a,{href:"http://gazebosim.org/",children:"http://gazebosim.org/"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Generated with reusable Claude Subagents & Spec-Kit Plus"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function t(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);