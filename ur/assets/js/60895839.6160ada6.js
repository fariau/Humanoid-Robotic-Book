"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[947],{2509:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>o,toc:()=>c});var i=s(4848),r=s(8453);const t={sidebar_position:9,title:"Perception Systems",description:"Computer vision, sensor processing, and environmental understanding",slug:"/perception"},a="Perception Systems",o={id:"perception/index",title:"Perception Systems",description:"Computer vision, sensor processing, and environmental understanding",source:"@site/docs/perception/index.mdx",sourceDirName:"perception",slug:"/perception",permalink:"/Humanoid-Robotic-Book/ur/docs/perception",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/perception/index.mdx",tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9,title:"Perception Systems",description:"Computer vision, sensor processing, and environmental understanding",slug:"/perception"},sidebar:"tutorialSidebar",previous:{title:"Motion Planning & Control",permalink:"/Humanoid-Robotic-Book/ur/docs/motion-planning"},next:{title:"Learning & Adaptation",permalink:"/Humanoid-Robotic-Book/ur/docs/learning-adaptation/"}},l={},c=[{value:"Introduction to Perception Systems",id:"introduction-to-perception-systems",level:2},{value:"Core Components of Perception Systems",id:"core-components-of-perception-systems",level:3},{value:"Camera-Based Perception",id:"camera-based-perception",level:2},{value:"Image Processing Fundamentals",id:"image-processing-fundamentals",level:3},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:3},{value:"Object Detection with Deep Learning",id:"object-detection-with-deep-learning",level:3},{value:"Depth Perception",id:"depth-perception",level:2},{value:"Stereo Vision",id:"stereo-vision",level:3},{value:"LIDAR Processing",id:"lidar-processing",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Kalman Filter for Sensor Fusion",id:"kalman-filter-for-sensor-fusion",level:3},{value:"Particle Filter for Non-linear Systems",id:"particle-filter-for-non-linear-systems",level:3},{value:"SLAM (Simultaneous Localization and Mapping)",id:"slam-simultaneous-localization-and-mapping",level:2},{value:"Visual SLAM",id:"visual-slam",level:3},{value:"Hardware-Specific Optimizations",id:"hardware-specific-optimizations",level:2},{value:"For NVIDIA Jetson Users",id:"for-nvidia-jetson-users",level:3},{value:"For High-End GPU Users",id:"for-high-end-gpu-users",level:3},{value:"3D Perception and Reconstruction",id:"3d-perception-and-reconstruction",level:2},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Perception Quality Assessment",id:"perception-quality-assessment",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Exercise 1: Feature Detection",id:"exercise-1-feature-detection",level:3},{value:"Exercise 2: Sensor Fusion",id:"exercise-2-sensor-fusion",level:3},{value:"Exercise 3: SLAM Implementation",id:"exercise-3-slam-implementation",level:3},{value:"Exercise 4: 3D Reconstruction",id:"exercise-4-3d-reconstruction",level:3},{value:"Exercise 5: Hardware Optimization",id:"exercise-5-hardware-optimization",level:3},{value:"MCQs Quiz",id:"mcqs-quiz",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:["\n",(0,i.jsxs)("div",{className:"button-container",style:{marginBottom:"20px"},children:[(0,i.jsx)("button",{className:"personalize-button",onClick:()=>{alert("Personalization feature would activate based on your hardware profile (GPU: [user GPU], Jetson: [user Jetson status], Robot: [user robot type])")},style:{backgroundColor:"#4a6fa5",color:"white",border:"none",padding:"10px 15px",borderRadius:"5px",marginRight:"10px",cursor:"pointer"},children:(0,i.jsx)(n.p,{children:"Personalize to my hardware"})}),(0,i.jsx)("button",{className:"urdu-toggle-button",onClick:()=>{alert("Content would toggle between English and Urdu")},style:{backgroundColor:"#2e7d32",color:"white",border:"none",padding:"10px 15px",borderRadius:"5px",cursor:"pointer"},children:(0,i.jsx)(n.p,{children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u067e\u0691\u06be\u06cc\u06ba / Show in Urdu"})})]}),"\n",(0,i.jsx)(n.h1,{id:"perception-systems",children:"Perception Systems"}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-perception-systems",children:"Introduction to Perception Systems"}),"\n",(0,i.jsx)(n.p,{children:"Perception systems form the sensory foundation of autonomous robots, enabling them to interpret and understand their environment through various sensors. These systems process raw sensor data to extract meaningful information about objects, obstacles, surfaces, and other environmental features necessary for navigation, manipulation, and interaction."}),"\n",(0,i.jsx)(n.h3,{id:"core-components-of-perception-systems",children:"Core Components of Perception Systems"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual Perception"}),": Processing camera images for object detection, recognition, and scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Perception"}),": Using stereo cameras, LIDAR, or structured light for 3D scene reconstruction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spatial Mapping"}),": Creating representations of the environment (occupancy grids, point clouds, meshes)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Recognition"}),": Identifying and classifying objects in the environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors for robust perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Estimation"}),": Determining robot pose and motion using sensor data"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Perception System] --\x3e B[Visual Processing]\n    A --\x3e C[Depth Processing]\n    A --\x3e D[Sensor Fusion]\n    A --\x3e E[State Estimation]\n\n    B --\x3e B1[Image Processing]\n    B --\x3e B2[Feature Detection]\n    B --\x3e B3[Object Recognition]\n    B --\x3e B4[Scene Understanding]\n\n    C --\x3e C1[LIDAR Processing]\n    C --\x3e C2[Stereo Vision]\n    C --\x3e C3[Depth Estimation]\n    C --\x3e C4[3D Reconstruction]\n\n    D --\x3e D1[Multi-sensor Integration]\n    D --\x3e D2[Data Association]\n    D --\x3e D3[Uncertainty Management]\n    D --\x3e D4[Consistent Representation]\n\n    E --\x3e E1[Visual Odometry]\n    E --\x3e E2[SLAM]\n    E --\x3e E3[IMU Integration]\n    E --\x3e E4[Particle Filtering]\n\n    F[Environmental Model] --\x3e A\n    F --\x3e F1[Occupancy Grids]\n    F --\x3e F2[Point Clouds]\n    F --\x3e F3[Semantic Maps]\n    F --\x3e F4[Topological Maps]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"camera-based-perception",children:"Camera-Based Perception"}),"\n",(0,i.jsx)(n.h3,{id:"image-processing-fundamentals",children:"Image Processing Fundamentals"}),"\n",(0,i.jsx)(n.p,{children:"Camera-based perception starts with fundamental image processing techniques:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom std_msgs.msg import Header\nfrom visualization_msgs.msg import Marker, MarkerArray\n\nclass CameraPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'camera_perception_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscriber for camera images\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n\n        # Create publisher for processed images\n        self.processed_image_pub = self.create_publisher(\n            Image, \'/processed_image\', 10)\n\n        # Create publisher for visualization markers\n        self.marker_pub = self.create_publisher(\n            MarkerArray, \'/perception_markers\', 10)\n\n        self.get_logger().info(\'Camera perception node initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Apply various image processing techniques\n            processed_image = self.process_image(cv_image)\n\n            # Detect features in the image\n            features = self.detect_features(processed_image)\n\n            # Publish processed image\n            processed_msg = self.cv_bridge.cv2_to_imgmsg(processed_image, "bgr8")\n            processed_msg.header = msg.header\n            self.processed_image_pub.publish(processed_msg)\n\n            # Publish visualization markers\n            markers = self.create_visualization_markers(features, msg.header)\n            self.marker_pub.publish(markers)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def process_image(self, image):\n        """Apply basic image processing techniques"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur to reduce noise\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Apply adaptive thresholding\n        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n\n        # Convert back to BGR for visualization\n        processed = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n\n        return processed\n\n    def detect_features(self, image):\n        """Detect features in the image"""\n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Detect corners using Shi-Tomasi corner detector\n        corners = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=100,\n            qualityLevel=0.01,\n            minDistance=10,\n            blockSize=3\n        )\n\n        features = []\n        if corners is not None:\n            for corner in corners:\n                x, y = corner.ravel()\n                features.append((int(x), int(y)))\n\n        return features\n\n    def create_visualization_markers(self, features, header):\n        """Create visualization markers for detected features"""\n        marker_array = MarkerArray()\n\n        for i, (x, y) in enumerate(features):\n            marker = Marker()\n            marker.header = header\n            marker.ns = "features"\n            marker.id = i\n            marker.type = Marker.CYLINDER\n            marker.action = Marker.ADD\n\n            # Position\n            marker.pose.position.x = x / 100.0  # Scale down for visualization\n            marker.pose.position.y = y / 100.0\n            marker.pose.position.z = 0.0\n            marker.pose.orientation.w = 1.0\n\n            # Scale\n            marker.scale.x = 0.02\n            marker.scale.y = 0.02\n            marker.scale.z = 0.01\n\n            # Color (red)\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n            marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n\n        return marker_array\n'})}),"\n",(0,i.jsx)(n.h3,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,i.jsx)(n.p,{children:"Feature detection is crucial for many perception tasks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class FeatureDetectionNode(Node):\n    def __init__(self):\n        super().__init__(\'feature_detection_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscriber and publisher\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.features_pub = self.create_publisher(\n            Image, \'/feature_overlay\', 10)\n\n        # Initialize different feature detectors\n        self.sift = cv2.SIFT_create()\n        self.surf = cv2.xfeatures2d.SURF_create() if hasattr(cv2.xfeatures2d, \'SURF_create\') else None\n        self.orb = cv2.ORB_create()\n\n        self.get_logger().info(\'Feature detection node initialized\')\n\n    def image_callback(self, msg):\n        """Process image and detect features"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Detect features using different methods\n            keypoints_sift, descriptors_sift = self.detect_sift_features(cv_image)\n            keypoints_orb, descriptors_orb = self.detect_orb_features(cv_image)\n\n            # Create overlay with features\n            overlay_image = self.create_feature_overlay(cv_image, keypoints_sift, keypoints_orb)\n\n            # Publish overlay\n            overlay_msg = self.cv_bridge.cv2_to_imgmsg(overlay_image, "bgr8")\n            overlay_msg.header = msg.header\n            self.features_pub.publish(overlay_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in feature detection: {e}\')\n\n    def detect_sift_features(self, image):\n        """Detect SIFT features"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = self.sift.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def detect_orb_features(self, image):\n        """Detect ORB features"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = self.orb.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def create_feature_overlay(self, image, sift_keypoints, orb_keypoints):\n        """Create image overlay with detected features"""\n        overlay = image.copy()\n\n        # Draw SIFT keypoints in red\n        if sift_keypoints:\n            overlay = cv2.drawKeypoints(\n                overlay, sift_keypoints, None,\n                color=(0, 0, 255),\n                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n            )\n\n        # Draw ORB keypoints in blue\n        if orb_keypoints:\n            overlay = cv2.drawKeypoints(\n                overlay, orb_keypoints, None,\n                color=(255, 0, 0),\n                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n            )\n\n        return overlay\n'})}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-with-deep-learning",children:"Object Detection with Deep Learning"}),"\n",(0,i.jsx)(n.p,{children:"Modern perception systems often use deep learning for object detection:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image as PILImage\nimport io\n\nclass DeepObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('deep_object_detection_node')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Load pre-trained object detection model\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Define COCO dataset classes\n        self.coco_names = [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        # Create subscriber and publisher\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.detection_pub = self.create_publisher(\n            Image, '/detection_overlay', 10)\n\n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        self.get_logger().info('Deep object detection node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process image and detect objects using deep learning\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Convert OpenCV image to PIL\n            pil_image = PILImage.fromarray(cv_image)\n\n            # Preprocess image\n            input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)\n\n            # Run object detection\n            with torch.no_grad():\n                predictions = self.model(input_tensor)\n\n            # Process predictions\n            processed_image = self.draw_detections(cv_image, predictions, msg.header)\n\n            # Publish result\n            result_msg = self.cv_bridge.cv2_to_imgmsg(processed_image, \"bgr8\")\n            result_msg.header = msg.header\n            self.detection_pub.publish(result_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in deep object detection: {e}')\n\n    def draw_detections(self, image, predictions, header):\n        \"\"\"Draw detection results on image\"\"\"\n        # Get the first (and typically only) image results\n        pred = {key: value[0].cpu() for key, value in predictions.items()}\n\n        boxes = pred['boxes'].numpy()\n        labels = pred['labels'].numpy()\n        scores = pred['scores'].numpy()\n\n        # Filter detections by confidence threshold\n        threshold = 0.5\n        valid_detections = scores > threshold\n\n        overlay = image.copy()\n\n        for i, valid in enumerate(valid_detections):\n            if valid:\n                box = boxes[i]\n                label = int(labels[i])\n                score = scores[i]\n\n                # Draw bounding box\n                x1, y1, x2, y2 = map(int, box)\n                cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n                # Draw label and confidence\n                label_text = f\"{self.coco_names[label]}: {score:.2f}\"\n                cv2.putText(\n                    overlay, label_text, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n                )\n\n        return overlay\n"})}),"\n",(0,i.jsx)(n.h2,{id:"depth-perception",children:"Depth Perception"}),"\n",(0,i.jsx)(n.h3,{id:"stereo-vision",children:"Stereo Vision"}),"\n",(0,i.jsx)(n.p,{children:"Stereo vision systems estimate depth by comparing images from two cameras:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class StereoVisionNode(Node):\n    def __init__(self):\n        super().__init__(\'stereo_vision_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers for left and right cameras\n        self.left_sub = self.create_subscription(\n            Image, \'/stereo_camera/left/image_rect_color\', self.left_image_callback, 10)\n        self.right_sub = self.create_subscription(\n            Image, \'/stereo_camera/right/image_rect_color\', self.right_image_callback, 10)\n\n        # Create publisher for disparity map\n        self.disparity_pub = self.create_publisher(\n            Image, \'/disparity_map\', 10)\n        self.depth_pub = self.create_publisher(\n            Image, \'/depth_image\', 10)\n\n        # Initialize stereo matcher\n        self.stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=96,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Camera parameters (these would come from calibration)\n        self.baseline = 0.12  # Baseline in meters\n        self.focal_length = 320  # Focal length in pixels (example value)\n\n        # Store images until both are available\n        self.left_image = None\n        self.right_image = None\n        self.left_timestamp = None\n        self.right_timestamp = None\n\n        self.get_logger().info(\'Stereo vision node initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        try:\n            self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, "mono8")\n            self.left_timestamp = msg.header.stamp\n\n            # If we have both images, process them\n            if self.right_image is not None:\n                self.process_stereo_pair()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        try:\n            self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, "mono8")\n            self.right_timestamp = msg.header.stamp\n\n            # If we have both images, process them\n            if self.left_image is not None:\n                self.process_stereo_pair()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def process_stereo_pair(self):\n        """Process stereo image pair to generate disparity and depth"""\n        if self.left_image is None or self.right_image is None:\n            return\n\n        try:\n            # Compute disparity map\n            disparity = self.stereo.compute(self.left_image, self.right_image).astype(np.float32) / 16.0\n\n            # Convert disparity to depth\n            depth = self.disparity_to_depth(disparity)\n\n            # Publish disparity map\n            disparity_msg = self.cv_bridge.cv2_to_imgmsg(disparity, "32FC1")\n            disparity_msg.header = self.left_timestamp  # Use the earlier timestamp\n            self.disparity_pub.publish(disparity_msg)\n\n            # Publish depth image\n            depth_msg = self.cv_bridge.cv2_to_imgmsg(depth, "32FC1")\n            depth_msg.header = self.left_timestamp\n            self.depth_pub.publish(depth_msg)\n\n            # Reset images after processing\n            self.left_image = None\n            self.right_image = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing stereo pair: {e}\')\n\n    def disparity_to_depth(self, disparity):\n        """Convert disparity map to depth map"""\n        # Depth = (baseline * focal_length) / disparity\n        # Add small value to avoid division by zero\n        depth = np.zeros_like(disparity)\n        valid_pixels = disparity > 0\n        depth[valid_pixels] = (self.baseline * self.focal_length) / (disparity[valid_pixels] + 1e-6)\n        return depth\n'})}),"\n",(0,i.jsx)(n.h3,{id:"lidar-processing",children:"LIDAR Processing"}),"\n",(0,i.jsx)(n.p,{children:"LIDAR sensors provide accurate depth information:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from sensor_msgs.msg import LaserScan, PointCloud2\nimport sensor_msgs.point_cloud2 as pc2\nfrom geometry_msgs.msg import Point32\n\nclass LIDARPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'lidar_perception_node\')\n\n        # Create subscriber for LIDAR scan\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n\n        # Create subscriber for point cloud (if available)\n        self.pc_sub = self.create_subscription(\n            PointCloud2, \'/point_cloud\', self.pointcloud_callback, 10)\n\n        # Create publisher for processed data\n        self.obstacle_pub = self.create_publisher(\n            PointCloud2, \'/obstacles\', 10)\n        self.free_space_pub = self.create_publisher(\n            PointCloud2, \'/free_space\', 10)\n\n        # LIDAR parameters\n        self.min_range = 0.1\n        self.max_range = 10.0\n        self.obstacle_threshold = 0.5  # Distance threshold for obstacles\n\n        self.get_logger().info(\'LIDAR perception node initialized\')\n\n    def scan_callback(self, msg):\n        """Process LIDAR scan data"""\n        try:\n            # Convert scan to points\n            points = self.scan_to_points(msg)\n\n            # Classify points as obstacles or free space\n            obstacles, free_space = self.classify_points(points)\n\n            # Publish results\n            obstacles_cloud = self.create_pointcloud2(obstacles, msg.header)\n            free_space_cloud = self.create_pointcloud2(free_space, msg.header)\n\n            self.obstacle_pub.publish(obstacles_cloud)\n            self.free_space_pub.publish(free_space_cloud)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing LIDAR scan: {e}\')\n\n    def scan_to_points(self, scan_msg):\n        """Convert LaserScan message to list of 3D points"""\n        points = []\n        angle = scan_msg.angle_min\n\n        for i, range_val in enumerate(scan_msg.ranges):\n            if self.min_range <= range_val <= self.max_range:\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n                z = 0.0  # Assuming 2D scan\n                points.append((x, y, z))\n            angle += scan_msg.angle_increment\n\n        return points\n\n    def classify_points(self, points):\n        """Classify points as obstacles or free space"""\n        obstacles = []\n        free_space = []\n\n        for point in points:\n            x, y, z = point\n            distance = np.sqrt(x**2 + y**2)\n\n            if distance < self.obstacle_threshold:\n                obstacles.append(point)\n            else:\n                free_space.append(point)\n\n        return obstacles, free_space\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud data"""\n        try:\n            # Convert point cloud to numpy array\n            points = np.array(list(pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True)))\n\n            # Perform point cloud processing\n            processed_points = self.process_pointcloud(points)\n\n            # Publish results\n            processed_cloud = self.create_pointcloud2(processed_points, msg.header)\n            self.obstacle_pub.publish(processed_cloud)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing point cloud: {e}\')\n\n    def process_pointcloud(self, points):\n        """Process point cloud data"""\n        # This could include:\n        # - Ground plane removal\n        # - Clustering for object detection\n        # - Surface normal estimation\n        # - Feature extraction\n\n        # For this example, we\'ll just return the points\n        return points\n\n    def create_pointcloud2(self, points, header):\n        """Create PointCloud2 message from list of points"""\n        # Convert to list of Point32\n        point32_list = [Point32(x=p[0], y=p[1], z=p[2]) for p in points]\n\n        # Create PointCloud2 message\n        cloud_msg = pc2.create_cloud_xyz32(header, point32_list)\n        return cloud_msg\n'})}),"\n",(0,i.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,i.jsx)(n.h3,{id:"kalman-filter-for-sensor-fusion",children:"Kalman Filter for Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Kalman filters combine data from multiple sensors optimally:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.linalg import block_diag\n\nclass KalmanFilter:\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        """Initialize Kalman filter\n        dim_x: State dimension\n        dim_z: Measurement dimension\n        dim_u: Control dimension\n        """\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        # State vector [x, y, vx, vy] for 2D tracking\n        self.x = np.zeros((dim_x, 1))\n\n        # State covariance matrix\n        self.P = np.eye(dim_x) * 500\n\n        # Process noise covariance\n        self.Q = np.eye(dim_x)\n\n        # Measurement noise covariance\n        self.R = np.eye(dim_z)\n\n        # State transition matrix (constant velocity model)\n        self.F = np.eye(dim_x)\n\n        # Measurement function\n        self.H = np.zeros((dim_z, dim_x))\n\n        # Control transition matrix\n        if dim_u > 0:\n            self.B = np.zeros((dim_x, dim_u))\n        else:\n            self.B = None\n\n        # Identity matrix for computational efficiency\n        self._I = np.eye(dim_x)\n\n    def predict(self, u=None, B=None, F=None, Q=None):\n        """Predict next state"""\n        if B is None:\n            B = self.B\n        if F is None:\n            F = self.F\n        if Q is None:\n            Q = self.Q\n        elif np.isscalar(Q):\n            Q = np.eye(self.dim_x) * Q\n\n        # x = Fx + Bu\n        if B is not None and u is not None:\n            self.x = np.dot(F, self.x) + np.dot(B, u)\n        else:\n            self.x = np.dot(F, self.x)\n\n        # P = FPF\' + Q\n        self.P = np.dot(np.dot(F, self.P), F.T) + Q\n\n    def update(self, z, R=None, H=None):\n        """Update state with measurement"""\n        if z is None:\n            return\n\n        if R is None:\n            R = self.R\n        elif np.isscalar(R):\n            R = np.eye(self.dim_z) * R\n\n        if H is None:\n            H = self.H\n\n        # Compute residual\n        y = z - np.dot(H, self.x)\n\n        # Compute residual covariance\n        PHT = np.dot(self.P, H.T)\n        S = np.dot(H, PHT) + R\n\n        # Compute Kalman gain\n        K = np.dot(PHT, np.linalg.inv(S))\n\n        # Update state\n        self.x = self.x + np.dot(K, y)\n\n        # Update covariance\n        I_KH = self._I - np.dot(K, H)\n        self.P = np.dot(I_KH, self.P)\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion_node\')\n\n        # Initialize Kalman filter for 2D position and velocity\n        # State: [x, y, vx, vy]\n        self.kf = KalmanFilter(dim_x=4, dim_z=2)\n\n        # Initialize filter with constant velocity model\n        dt = 0.1  # Time step\n        self.kf.F = np.array([\n            [1, 0, dt, 0],\n            [0, 1, 0, dt],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ])\n\n        # Measurement function (we only measure position)\n        self.kf.H = np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0]\n        ])\n\n        # Process noise (tuned for our application)\n        q = 0.1\n        self.kf.Q = block_diag(q**2, q**2, q**2, q**2)\n\n        # Measurement noise\n        self.kf.R = np.eye(2) * 0.5**2\n\n        # Subscribe to different sensors\n        self.camera_sub = self.create_subscription(\n            Point, \'/camera_detection\', self.camera_callback, 10)\n        self.lidar_sub = self.create_subscription(\n            Point, \'/lidar_detection\', self.lidar_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Point, \'/odometry\', self.odom_callback, 10)\n\n        # Publisher for fused state\n        self.state_pub = self.create_publisher(\n            Point, \'/fused_state\', 10)\n\n        # Timer for prediction step\n        self.predict_timer = self.create_timer(0.1, self.predict_step)\n\n        self.get_logger().info(\'Sensor fusion node initialized\')\n\n    def camera_callback(self, msg):\n        """Process camera measurement"""\n        # Camera provides position measurement\n        z = np.array([[msg.x], [msg.y]])\n        self.kf.update(z)\n\n    def lidar_callback(self, msg):\n        """Process LIDAR measurement"""\n        # LIDAR provides position measurement\n        z = np.array([[msg.x], [msg.y]])\n        self.kf.update(z)\n\n    def odom_callback(self, msg):\n        """Process odometry measurement"""\n        # Odometry provides position measurement\n        z = np.array([[msg.x], [msg.y]])\n        self.kf.update(z)\n\n    def predict_step(self):\n        """Prediction step of Kalman filter"""\n        self.kf.predict()\n\n        # Publish current state estimate\n        state_msg = Point()\n        state_msg.x = float(self.kf.x[0, 0])\n        state_msg.y = float(self.kf.x[1, 0])\n        state_msg.z = float(self.kf.x[2, 0])  # x velocity\n        # Use z field for x velocity in this example\n        self.state_pub.publish(state_msg)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"particle-filter-for-non-linear-systems",children:"Particle Filter for Non-linear Systems"}),"\n",(0,i.jsx)(n.p,{children:"For non-linear systems, particle filters can be more appropriate:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ParticleFilter:\n    def __init__(self, num_particles, state_dim, process_noise, measurement_noise):\n        self.num_particles = num_particles\n        self.state_dim = state_dim\n        self.process_noise = process_noise\n        self.measurement_noise = measurement_noise\n\n        # Initialize particles randomly\n        self.particles = np.random.randn(num_particles, state_dim) * 10\n        self.weights = np.ones(num_particles) / num_particles\n\n    def predict(self, control_input=None):\n        """Predict step: propagate particles through motion model"""\n        for i in range(self.num_particles):\n            # Simple motion model (constant velocity with noise)\n            self.particles[i] += np.random.normal(0, self.process_noise, self.state_dim)\n\n    def update(self, measurement):\n        """Update step: adjust weights based on measurement likelihood"""\n        # Calculate likelihood of each particle given measurement\n        for i in range(self.num_particles):\n            # Calculate distance between particle and measurement\n            distance = np.linalg.norm(self.particles[i] - measurement)\n            # Update weight based on likelihood (Gaussian model)\n            likelihood = np.exp(-0.5 * (distance**2) / (self.measurement_noise**2))\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        self.weights += 1.e-300  # Avoid zero weights\n        self.weights /= np.sum(self.weights)\n\n    def resample(self):\n        """Resample particles based on their weights"""\n        # Systematic resampling\n        indices = []\n        step = 1.0 / self.num_particles\n        start = np.random.uniform(0, step)\n\n        i = 0\n        for j in range(self.num_particles):\n            while start > self.weights[i]:\n                start -= self.weights[i]\n                i += 1\n            indices.append(i)\n\n        # Resample particles\n        self.particles = self.particles[indices]\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def estimate(self):\n        """Get state estimate as weighted average of particles"""\n        return np.average(self.particles, weights=self.weights, axis=0)\n\nclass ParticleFilterNode(Node):\n    def __init__(self):\n        super().__init__(\'particle_filter_node\')\n\n        # Initialize particle filter\n        self.pf = ParticleFilter(\n            num_particles=100,\n            state_dim=2,  # 2D position\n            process_noise=0.1,\n            measurement_noise=0.5\n        )\n\n        # Subscribe to sensor measurements\n        self.measurement_sub = self.create_subscription(\n            Point, \'/sensor_measurement\', self.measurement_callback, 10)\n\n        # Publisher for estimated state\n        self.estimate_pub = self.create_publisher(\n            Point, \'/particle_filter_estimate\', 10)\n\n        # Timer for prediction step\n        self.predict_timer = self.create_timer(0.1, self.predict_step)\n\n        self.get_logger().info(\'Particle filter node initialized\')\n\n    def measurement_callback(self, msg):\n        """Process measurement and update filter"""\n        measurement = np.array([msg.x, msg.y])\n        self.pf.update(measurement)\n        self.pf.resample()\n\n    def predict_step(self):\n        """Prediction step"""\n        self.pf.predict()\n\n        # Get and publish estimate\n        estimate = self.pf.estimate()\n        estimate_msg = Point()\n        estimate_msg.x = float(estimate[0])\n        estimate_msg.y = float(estimate[1])\n        estimate_msg.z = 0.0  # Not used in this example\n        self.estimate_pub.publish(estimate_msg)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"slam-simultaneous-localization-and-mapping",children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,i.jsx)(n.h3,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,i.jsx)(n.p,{children:"Visual SLAM systems build maps while localizing:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'visual_slam_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscriber for camera images\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n\n        # Create publisher for map visualization\n        self.map_pub = self.create_publisher(\n            MarkerArray, \'/slam_map\', 10)\n\n        # SLAM parameters\n        self.keyframe_threshold = 10  # Threshold for keyframe selection\n        self.max_features = 1000      # Maximum features to track\n\n        # Initialize ORB detector and descriptor\n        self.orb = cv2.ORB_create(nfeatures=self.max_features)\n\n        # FLANN matcher for feature matching\n        FLANN_INDEX_LSH = 6\n        index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n        search_params = dict(checks=50)\n        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n\n        # SLAM state\n        self.current_frame = None\n        self.keyframes = []\n        self.map_points = []\n        self.current_pose = np.eye(4)  # 4x4 identity matrix\n\n        self.get_logger().info(\'Visual SLAM node initialized\')\n\n    def image_callback(self, msg):\n        """Process image for SLAM"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Extract features from current image\n            keypoints, descriptors = self.extract_features(cv_image)\n\n            if len(self.keyframes) == 0:\n                # First frame - initialize\n                self.keyframes.append({\n                    \'image\': cv_image,\n                    \'keypoints\': keypoints,\n                    \'descriptors\': descriptors,\n                    \'pose\': self.current_pose.copy()\n                })\n            else:\n                # Track features and estimate motion\n                prev_frame = self.keyframes[-1]\n                matches = self.match_features(prev_frame[\'descriptors\'], descriptors)\n\n                if len(matches) > 10:  # Minimum matches for reliable pose estimation\n                    # Estimate relative pose\n                    pose_change = self.estimate_pose(\n                        prev_frame[\'keypoints\'], keypoints, matches\n                    )\n\n                    # Update current pose\n                    self.current_pose = self.current_pose @ pose_change\n\n                    # Add as keyframe if significant motion\n                    if self.should_add_keyframe():\n                        self.keyframes.append({\n                            \'image\': cv_image,\n                            \'keypoints\': keypoints,\n                            \'descriptors\': descriptors,\n                            \'pose\': self.current_pose.copy()\n                        })\n\n                        # Update map points\n                        self.update_map_points(keypoints, matches)\n\n            # Publish map visualization\n            self.publish_map_visualization(msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in SLAM: {e}\')\n\n    def extract_features(self, image):\n        """Extract ORB features from image"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = self.orb.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def match_features(self, desc1, desc2):\n        """Match features between two descriptors"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        try:\n            matches = self.flann.knnMatch(desc1, desc2, k=2)\n            # Apply Lowe\'s ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < 0.7 * n.distance:\n                        good_matches.append(m)\n            return good_matches\n        except:\n            return []\n\n    def estimate_pose(self, kp1, kp2, matches):\n        """Estimate relative pose from matched keypoints"""\n        if len(matches) >= 4:\n            # Get matched points\n            src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n            # Find fundamental matrix\n            F, mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.RANSAC, 4, 0.999)\n\n            # For simplicity, we\'ll return a simple transformation\n            # In a real system, this would involve more complex pose estimation\n            return np.eye(4)  # Placeholder\n        else:\n            return np.eye(4)\n\n    def should_add_keyframe(self):\n        """Determine if current frame should be added as keyframe"""\n        # Simple criterion: add keyframe if we have enough new features\n        # or if enough time has passed\n        return len(self.keyframes) % self.keyframe_threshold == 0\n\n    def update_map_points(self, keypoints, matches):\n        """Update map points based on current observations"""\n        # This would triangulate 3D points from stereo observations\n        # or track points across frames\n        pass\n\n    def publish_map_visualization(self, header):\n        """Publish map visualization markers"""\n        marker_array = MarkerArray()\n\n        # Create markers for keyframe positions\n        for i, kf in enumerate(self.keyframes):\n            marker = Marker()\n            marker.header = header\n            marker.ns = "keyframes"\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n\n            # Position from pose\n            marker.pose.position.x = kf[\'pose\'][0, 3] / 10  # Scale down for visualization\n            marker.pose.position.y = kf[\'pose\'][1, 3] / 10\n            marker.pose.position.z = kf[\'pose\'][2, 3] / 10\n            marker.pose.orientation.w = 1.0\n\n            # Scale\n            marker.scale.x = 0.1\n            marker.scale.y = 0.1\n            marker.scale.z = 0.1\n\n            # Color (blue)\n            marker.color.b = 1.0\n            marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n\n        self.map_pub.publish(marker_array)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"hardware-specific-optimizations",children:"Hardware-Specific Optimizations"}),"\n",(0,i.jsx)(n.h3,{id:"for-nvidia-jetson-users",children:"For NVIDIA Jetson Users"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Jetson-specific perception optimizations\nimport rclpy\nfrom rclpy.node import Node\nimport numpy as np\n\nclass JetsonPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'jetson_perception_node\')\n\n        # Optimize perception for Jetson\'s ARM architecture and GPU\n        self.setup_jetson_optimizations()\n\n        self.get_logger().info(\'Jetson-optimized perception node initialized\')\n\n    def setup_jetson_optimizations(self):\n        """Configure perception for Jetson hardware"""\n        # Use optimized libraries for Jetson\n        # Adjust algorithm parameters for Jetson\'s capabilities\n        # Enable hardware acceleration where possible\n        # Optimized for [USER_GPU] hardware\n        pass\n\n    def jetson_image_processing(self, image):\n        """\n        Process image using Jetson-optimized methods\n        Optimized for [USER_GPU] hardware\n        """\n        # Use Jetson\'s hardware accelerators for image processing\n        # This would leverage Jetson\'s ISP, GPU, and other processing units\n        pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"for-high-end-gpu-users",children:"For High-End GPU Users"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# High-end GPU perception optimizations\nimport rclpy\nfrom rclpy.node import Node\nimport torch\nimport numpy as np\n\nclass GPUOptimizedPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'gpu_optimized_perception_node\')\n\n        # Initialize GPU context for perception\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        # Configure perception for high-end GPU\n        self.setup_gpu_optimizations()\n\n        self.get_logger().info(\'GPU-optimized perception node initialized\')\n\n    def setup_gpu_optimizations(self):\n        """Configure perception for high-end GPU hardware"""\n        # Set environment variables for GPU optimization\n        # Enable advanced perception features for powerful GPUs\n        # Use multi-GPU processing if available\n        # Optimized for [USER_GPU] hardware specifications\n        pass\n\n    def gpu_accelerated_perception(self, sensor_data):\n        """\n        Perform GPU-accelerated perception processing\n        Optimized for [USER_GPU] hardware specifications\n        """\n        # Use GPU for intensive perception computations\n        # Leverage tensor cores for efficient parallel computation\n        # Implement batch processing for efficiency\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3d-perception-and-reconstruction",children:"3D Perception and Reconstruction"}),"\n",(0,i.jsx)(n.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from sensor_msgs.msg import PointCloud2\nimport sensor_msgs.point_cloud2 as pc2\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Header\n\nclass PointCloudPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'pointcloud_perception_node\')\n\n        # Subscribe to point cloud\n        self.pc_sub = self.create_subscription(\n            PointCloud2, \'/points\', self.pointcloud_callback, 10)\n\n        # Publishers for processed data\n        self.ground_pub = self.create_publisher(\n            PointCloud2, \'/ground_points\', 10)\n        self.obstacles_pub = self.create_publisher(\n            PointCloud2, \'/obstacle_points\', 10)\n\n        # RANSAC parameters for ground plane detection\n        self.ransac_max_iterations = 100\n        self.ransac_distance_threshold = 0.1\n\n        self.get_logger().info(\'Point cloud perception node initialized\')\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud data"""\n        try:\n            # Convert PointCloud2 to numpy array\n            points = np.array(list(pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True)))\n\n            if len(points) > 0:\n                # Separate ground and obstacles using RANSAC\n                ground_points, obstacle_points = self.separate_ground_obstacles(points)\n\n                # Publish results\n                if len(ground_points) > 0:\n                    ground_cloud = self.create_pointcloud2(ground_points, msg.header)\n                    self.ground_pub.publish(ground_cloud)\n\n                if len(obstacle_points) > 0:\n                    obstacle_cloud = self.create_pointcloud2(obstacle_points, msg.header)\n                    self.obstacles_pub.publish(obstacle_cloud)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing point cloud: {e}\')\n\n    def separate_ground_obstacles(self, points):\n        """Separate ground and obstacle points using RANSAC"""\n        if len(points) < 3:\n            return np.array([]), points\n\n        best_model = None\n        best_inliers = []\n        best_error = float(\'inf\')\n\n        # RANSAC for plane fitting (ground plane)\n        for _ in range(self.ransac_max_iterations):\n            # Randomly select 3 points\n            indices = np.random.choice(len(points), 3, replace=False)\n            sample_points = points[indices]\n\n            # Fit plane to these points\n            model = self.fit_plane(sample_points)\n\n            if model is not None:\n                # Calculate distances to plane\n                distances = np.abs(np.dot(points, model[:3]) + model[3])\n                inliers = points[distances < self.ransac_distance_threshold]\n\n                if len(inliers) > len(best_inliers):\n                    best_model = model\n                    best_inliers = inliers\n                    best_error = np.mean(distances[distances < self.ransac_distance_threshold])\n\n        # If we found a good ground plane\n        if best_model is not None and len(best_inliers) > len(points) * 0.1:  # At least 10% of points\n            # Calculate distances to the best plane\n            all_distances = np.abs(np.dot(points, best_model[:3]) + best_model[3])\n            ground_mask = all_distances < self.ransac_distance_threshold\n\n            ground_points = points[ground_mask]\n            obstacle_points = points[~ground_mask]\n        else:\n            # If no good ground plane found, assume no ground separation\n            ground_points = np.array([])\n            obstacle_points = points\n\n        return ground_points, obstacle_points\n\n    def fit_plane(self, points):\n        """Fit a plane to 3D points using SVD"""\n        if len(points) < 3:\n            return None\n\n        # Calculate centroid\n        centroid = np.mean(points, axis=0)\n\n        # Center points\n        centered_points = points - centroid\n\n        # Calculate covariance matrix\n        cov_matrix = np.cov(centered_points.T)\n\n        # Calculate SVD\n        u, s, vh = np.linalg.svd(cov_matrix)\n\n        # Normal vector is the eigenvector with smallest eigenvalue\n        normal = vh[2, :]\n\n        # Calculate d parameter: ax + by + cz + d = 0\n        d = -np.dot(normal, centroid)\n\n        return np.array([normal[0], normal[1], normal[2], d])\n\n    def create_pointcloud2(self, points, header):\n        """Create PointCloud2 message from numpy array"""\n        # Convert numpy array to list of Point32\n        point32_list = [Point32(x=p[0], y=p[1], z=p[2]) for p in points]\n\n        # Create PointCloud2 message\n        cloud_msg = pc2.create_cloud_xyz32(header, point32_list)\n        return cloud_msg\n'})}),"\n",(0,i.jsx)(n.h2,{id:"perception-quality-assessment",children:"Perception Quality Assessment"}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class PerceptionQualityNode(Node):\n    def __init__(self):\n        super().__init__('perception_quality_node')\n\n        # Publishers for quality metrics\n        self.quality_pub = self.create_publisher(\n            Float64MultiArray, '/perception_quality', 10)\n\n        # Quality assessment parameters\n        self.confidence_threshold = 0.5\n        self.min_detection_count = 5\n\n        # Store metrics over time\n        self.detection_confidences = []\n        self.detection_rates = []\n\n        # Timer for quality assessment\n        self.quality_timer = self.create_timer(1.0, self.assess_quality)\n\n        self.get_logger().info('Perception quality assessment node initialized')\n\n    def assess_quality(self):\n        \"\"\"Assess perception system quality\"\"\"\n        if len(self.detection_confidences) == 0:\n            return\n\n        # Calculate quality metrics\n        avg_confidence = np.mean(self.detection_confidences)\n        confidence_std = np.std(self.detection_confidences)\n        detection_rate = len([c for c in self.detection_confidences if c > self.confidence_threshold]) / len(self.detection_confidences)\n\n        # Prepare quality report\n        quality_metrics = [\n            float(avg_confidence),\n            float(confidence_std),\n            float(detection_rate),\n            float(len(self.detection_confidences))\n        ]\n\n        # Publish quality metrics\n        quality_msg = Float64MultiArray()\n        quality_msg.data = quality_metrics\n        self.quality_pub.publish(quality_msg)\n\n        # Log quality assessment\n        self.get_logger().info(\n            f'Perception Quality - Avg Confidence: {avg_confidence:.3f}, '\n            f'Detection Rate: {detection_rate:.3f}, '\n            f'Count: {len(self.detection_confidences)}'\n        )\n\n        # Clear metrics for next assessment\n        self.detection_confidences.clear()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multi-sensor Integration"}),": Effective perception combines data from multiple sensors for robustness."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Perception systems must operate within strict timing constraints for robot control."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Uncertainty Management"}),": All perception results have associated uncertainties that must be handled properly."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hardware Optimization"}),": Different optimization strategies are needed for different hardware platforms."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping is essential for autonomous navigation in unknown environments."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Quality Assessment"}),": Perception systems need continuous quality monitoring and assessment."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"3D Understanding"}),": Modern robots require 3D scene understanding for manipulation and navigation."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-feature-detection",children:"Exercise 1: Feature Detection"}),"\n",(0,i.jsx)(n.p,{children:"Implement a feature detection system that can detect and match features between consecutive camera frames."}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-sensor-fusion",children:"Exercise 2: Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Create a sensor fusion system that combines camera and LIDAR data to improve object detection accuracy."}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-slam-implementation",children:"Exercise 3: SLAM Implementation"}),"\n",(0,i.jsx)(n.p,{children:"Implement a basic visual SLAM system that can build a map while tracking the robot's position."}),"\n",(0,i.jsx)(n.h3,{id:"exercise-4-3d-reconstruction",children:"Exercise 4: 3D Reconstruction"}),"\n",(0,i.jsx)(n.p,{children:"Create a system that reconstructs 3D scenes from stereo camera images or LIDAR data."}),"\n",(0,i.jsx)(n.h3,{id:"exercise-5-hardware-optimization",children:"Exercise 5: Hardware Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Optimize a perception algorithm for your specific hardware configuration (GPU/Jetson) and measure performance improvements."}),"\n",(0,i.jsx)(n.h2,{id:"mcqs-quiz",children:"MCQs Quiz"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What does SLAM stand for in robotics?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Simultaneous Localization and Mapping"}),"\n",(0,i.jsx)(n.li,{children:"B) Sensor Localization and Mapping"}),"\n",(0,i.jsx)(n.li,{children:"C) Stereo Localization and Mapping"}),"\n",(0,i.jsx)(n.li,{children:"D) Systematic Localization and Mapping"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Answer: A"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Which sensor provides the most accurate depth information for close-range perception?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Camera"}),"\n",(0,i.jsx)(n.li,{children:"B) LIDAR"}),"\n",(0,i.jsx)(n.li,{children:"C) Ultrasonic sensor"}),"\n",(0,i.jsx)(n.li,{children:"D) Infrared sensor"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What is the primary purpose of sensor fusion in perception systems?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) To reduce cost only"}),"\n",(0,i.jsx)(n.li,{children:"B) To combine data from multiple sensors for robust perception"}),"\n",(0,i.jsx)(n.li,{children:"C) To increase sensor range"}),"\n",(0,i.jsx)(n.li,{children:"D) To eliminate the need for calibration"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Which feature detector is known for being scale-invariant?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Harris corner detector"}),"\n",(0,i.jsx)(n.li,{children:"B) SIFT (Scale-Invariant Feature Transform)"}),"\n",(0,i.jsx)(n.li,{children:"C) Canny edge detector"}),"\n",(0,i.jsx)(n.li,{children:"D) Sobel operator"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"What is a key advantage of using a particle filter over a Kalman filter?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A) Lower computational cost"}),"\n",(0,i.jsx)(n.li,{children:"B) Better performance with non-linear systems and non-Gaussian noise"}),"\n",(0,i.jsx)(n.li,{children:"C) Guaranteed convergence"}),"\n",(0,i.jsx)(n.li,{children:"D) Simpler implementation"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"Computer Vision: Algorithms and Applications" by Richard Szeliski'}),"\n",(0,i.jsx)(n.li,{children:'"Probabilistic Robotics" by Thrun, Burgard, and Fox'}),"\n",(0,i.jsx)(n.li,{children:'"Multiple View Geometry in Computer Vision" by Hartley and Zisserman'}),"\n",(0,i.jsx)(n.li,{children:'"Learning OpenCV" by Bradski and Kaehler'}),"\n",(0,i.jsxs)(n.li,{children:["Point Cloud Library (PCL): ",(0,i.jsx)(n.a,{href:"http://pointclouds.org/",children:"http://pointclouds.org/"})]}),"\n",(0,i.jsxs)(n.li,{children:["ROS Perception: ",(0,i.jsx)(n.a,{href:"http://wiki.ros.org/perception",children:"http://wiki.ros.org/perception"})]}),"\n",(0,i.jsxs)(n.li,{children:["OpenCV Documentation: ",(0,i.jsx)(n.a,{href:"https://docs.opencv.org/",children:"https://docs.opencv.org/"})]}),"\n",(0,i.jsx)(n.li,{children:'"Visual SLAM: Why Bundle Adjust?" - Recent advances in SLAM research'}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Generated with reusable Claude Subagents & Spec-Kit Plus"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);