"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[985],{7104:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var i=r(4848),t=r(8453);const a={sidebar_position:10,title:"Learning & Adaptation",description:"Exploring machine learning and adaptation techniques in humanoid robotics, including reinforcement learning, imitation learning, and adaptive control systems.",keywords:["machine learning in robotics","reinforcement learning","imitation learning","adaptive control","robot learning algorithms"]},o="Chapter 10: Learning & Adaptation",s={id:"learning-adaptation/index",title:"Learning & Adaptation",description:"Exploring machine learning and adaptation techniques in humanoid robotics, including reinforcement learning, imitation learning, and adaptive control systems.",source:"@site/docs/learning-adaptation/index.mdx",sourceDirName:"learning-adaptation",slug:"/learning-adaptation/",permalink:"/Humanoid-Robotic-Book/ur/docs/learning-adaptation/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/learning-adaptation/index.mdx",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,title:"Learning & Adaptation",description:"Exploring machine learning and adaptation techniques in humanoid robotics, including reinforcement learning, imitation learning, and adaptive control systems.",keywords:["machine learning in robotics","reinforcement learning","imitation learning","adaptive control","robot learning algorithms"]},sidebar:"tutorialSidebar",previous:{title:"Perception Systems",permalink:"/Humanoid-Robotic-Book/ur/docs/perception"},next:{title:"Safety & Ethics in Robotics",permalink:"/Humanoid-Robotic-Book/ur/docs/safety-ethics/"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Paradigms in Robotics",id:"learning-paradigms-in-robotics",level:2},{value:"Reinforcement Learning in Robotics",id:"reinforcement-learning-in-robotics",level:2},{value:"Core Components of RL",id:"core-components-of-rl",level:3},{value:"Deep Reinforcement Learning",id:"deep-reinforcement-learning",level:3},{value:"Policy Gradient Methods",id:"policy-gradient-methods",level:3},{value:"Imitation Learning",id:"imitation-learning",level:2},{value:"Behavior Cloning",id:"behavior-cloning",level:3},{value:"DAgger Algorithm",id:"dagger-algorithm",level:3},{value:"Adaptive Control Systems",id:"adaptive-control-systems",level:2},{value:"Model Reference Adaptive Control (MRAC)",id:"model-reference-adaptive-control-mrac",level:3},{value:"Self-Organizing Maps for Adaptive Behavior",id:"self-organizing-maps-for-adaptive-behavior",level:3},{value:"Transfer Learning in Robotics",id:"transfer-learning-in-robotics",level:2},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Safety in Learning Systems",id:"safety-in-learning-systems",level:2},{value:"Safe Exploration Strategies",id:"safe-exploration-strategies",level:3},{value:"Hardware-Specific Implementations",id:"hardware-specific-implementations",level:2},{value:"GPU-Accelerated Learning",id:"gpu-accelerated-learning",level:3},{value:"Jetson-Based Learning",id:"jetson-based-learning",level:3},{value:"Real Robot Integration",id:"real-robot-integration",level:3},{value:"Urdu Content: \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u06a9\u06d2 \u0646\u0638\u0627\u0645",id:"urdu-content-\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u0627\u0648\u0631-\u0645\u0637\u0627\u0628\u0642\u062a-\u06a9\u06d2-\u0646\u0638\u0627\u0645",level:2},{value:"\u062a\u0639\u0627\u0631\u0641",id:"\u062a\u0639\u0627\u0631\u0641",level:2},{value:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0637\u0631\u06cc\u0642\u06d2",id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0637\u0631\u06cc\u0642\u06d2",level:2},{value:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af",id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0645\u0636\u0628\u0648\u0637-\u0644\u0631\u0646\u0646\u06af",level:2},{value:"RL \u06a9\u06d2 \u0628\u0646\u06cc\u0627\u062f\u06cc \u062c\u0632\u0648",id:"rl-\u06a9\u06d2-\u0628\u0646\u06cc\u0627\u062f\u06cc-\u062c\u0632\u0648",level:3},{value:"\u06af\u06c1\u0631\u0627 \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af",id:"\u06af\u06c1\u0631\u0627-\u0645\u0636\u0628\u0648\u0637-\u0644\u0631\u0646\u0646\u06af",level:3},{value:"\u067e\u0627\u0644\u06cc\u0633\u06cc \u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679 \u0645\u06cc\u062a\u06be\u0688\u0633",id:"\u067e\u0627\u0644\u06cc\u0633\u06cc-\u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679-\u0645\u06cc\u062a\u06be\u0688\u0633",level:3},{value:"\u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af",id:"\u0646\u0642\u0644-\u0644\u06cc\u0646\u0646\u06af",level:2},{value:"\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af",id:"\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631-\u06a9\u0644\u0648\u0646\u0646\u06af",level:3},{value:"DAgger \u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645",id:"dagger-\u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645",level:3},{value:"\u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0633\u0633\u0679\u0645\u0632",id:"\u0645\u0648\u0627\u0641\u0642-\u06a9\u0646\u0679\u0631\u0648\u0644-\u0633\u0633\u0679\u0645\u0632",level:2},{value:"\u0645\u0627\u0688\u0644 \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 (MRAC)",id:"\u0645\u0627\u0688\u0644-\u0631\u06cc\u0641\u0631\u0646\u0633-\u0645\u0648\u0627\u0641\u0642-\u06a9\u0646\u0679\u0631\u0648\u0644-mrac",level:3},{value:"\u0645\u0648\u0627\u0641\u0642 \u0637\u0631\u0632 \u0639\u0645\u0644 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0633\u06cc\u0644\u0641 \u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af \u0645\u06cc\u067e\u0633",id:"\u0645\u0648\u0627\u0641\u0642-\u0637\u0631\u0632-\u0639\u0645\u0644-\u06a9\u06d2-\u0644\u06cc\u06d2-\u0633\u06cc\u0644\u0641-\u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af-\u0645\u06cc\u067e\u0633",level:3},{value:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0679\u0631\u0627\u0646\u0633\u0641\u0631 \u0644\u0631\u0646\u0646\u06af",id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0679\u0631\u0627\u0646\u0633\u0641\u0631-\u0644\u0631\u0646\u0646\u06af",level:2},{value:"\u0688\u0648\u0645\u06cc\u0646 \u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646",id:"\u0688\u0648\u0645\u06cc\u0646-\u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646",level:3},{value:"\u0633\u06cc\u0641\u0679\u06cc \u0644\u0631\u0646\u0646\u06af \u0633\u0633\u0679\u0645\u0632 \u0645\u06cc\u06ba",id:"\u0633\u06cc\u0641\u0679\u06cc-\u0644\u0631\u0646\u0646\u06af-\u0633\u0633\u0679\u0645\u0632-\u0645\u06cc\u06ba",level:2},{value:"\u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646 \u0627\u0633\u0679\u0631\u06cc\u0679\u06cc\u062c\u0632",id:"\u0645\u062d\u0641\u0648\u0638-\u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646-\u0627\u0633\u0679\u0631\u06cc\u0679\u06cc\u062c\u0632",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Quiz: Learning &amp; Adaptation",id:"quiz-learning--adaptation",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components},{Details:r}=e;return r||function(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-10-learning--adaptation",children:"Chapter 10: Learning & Adaptation"}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Welcome to Chapter 10 of the Physical AI & Humanoid Robotics textbook. In this chapter, we explore the fascinating world of learning and adaptation in humanoid robotics. We'll examine how robots can acquire new skills, adapt to changing environments, and improve their performance through various machine learning techniques."}),"\n",(0,i.jsx)(e.p,{children:"Machine learning has revolutionized robotics by enabling robots to learn from experience, adapt to new situations, and develop sophisticated behaviors without explicit programming for every scenario. This chapter covers the fundamental approaches to learning in robotics, including reinforcement learning, imitation learning, and adaptive control systems."}),"\n",(0,i.jsx)(e.p,{children:"Understanding these concepts is crucial for developing intelligent humanoid robots that can operate effectively in dynamic, real-world environments. Whether it's learning to walk more efficiently, adapting manipulation strategies, or improving interaction with humans, learning and adaptation form the foundation of truly intelligent robotic systems."}),"\n",(0,i.jsx)(e.h2,{id:"learning-paradigms-in-robotics",children:"Learning Paradigms in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Machine learning in robotics encompasses several paradigms, each suited to different types of problems and learning objectives. The primary approaches include:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Supervised Learning"}),": Using labeled datasets to train models that can predict outcomes based on input data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning through trial and error by receiving rewards or penalties for actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Imitation Learning"}),": Learning by observing and mimicking expert demonstrations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Unsupervised Learning"}),": Discovering patterns and structures in unlabeled data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Self-Supervised Learning"}),": Creating supervision from the data itself"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Each paradigm offers unique advantages and is often combined to achieve robust learning capabilities in humanoid robots."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[Learning Paradigms in Robotics] --\x3e B[Supervised Learning]\r\n    A --\x3e C[Reinforcement Learning]\r\n    A --\x3e D[Imitation Learning]\r\n    A --\x3e E[Unsupervised Learning]\r\n    A --\x3e F[Self-Supervised Learning]\r\n\r\n    B --\x3e B1[Pattern Recognition]\r\n    B --\x3e B2[Object Detection]\r\n\r\n    C --\x3e C1[Policy Optimization]\r\n    C --\x3e C2[Value Estimation]\r\n\r\n    D --\x3e D1[Behavior Cloning]\r\n    D --\x3e D2[Dagger Algorithm]\r\n\r\n    E --\x3e E1[Clustering]\r\n    E --\x3e E2[Dimensionality Reduction]\r\n\r\n    F --\x3e F1[Predictive Learning]\r\n    F --\x3e F2[Contrastive Learning]\n"})}),"\n",(0,i.jsx)(e.h2,{id:"reinforcement-learning-in-robotics",children:"Reinforcement Learning in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Reinforcement Learning (RL) is particularly well-suited for robotics applications where agents must learn to interact with physical environments through sequential decision-making. In RL, an agent learns to take actions in an environment to maximize cumulative reward."}),"\n",(0,i.jsx)(e.h3,{id:"core-components-of-rl",children:"Core Components of RL"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"State Space"}),": The set of all possible states the robot can be in"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Space"}),": The set of all possible actions the robot can take"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reward Function"}),": A function that assigns a scalar reward to each state-action pair"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Policy"}),": A mapping from states to actions that defines the agent's behavior"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Value Function"}),": Estimates the expected cumulative reward from a given state"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"deep-reinforcement-learning",children:"Deep Reinforcement Learning"}),"\n",(0,i.jsx)(e.p,{children:"Deep RL combines deep neural networks with reinforcement learning, enabling robots to learn complex behaviors from high-dimensional sensory inputs like camera images and joint encoders."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Deep Q-Network (DQN) for robotic manipulation\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\nclass RobotDQN(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(RobotDQN, self).__init__()\r\n\r\n        # Convolutional layers for processing visual input\r\n        self.conv_layers = nn.Sequential(\r\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # Calculate the output size after convolutions\r\n        conv_out_size = self._get_conv_out_size(state_dim)\r\n\r\n        # Fully connected layers\r\n        self.fc_layers = nn.Sequential(\r\n            nn.Linear(conv_out_size + 6, hidden_dim),  # Include proprioceptive features\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim)\r\n        )\r\n\r\n    def _get_conv_out_size(self, input_shape):\r\n        """Calculate the output size after convolution layers"""\r\n        o = self.conv_layers(torch.zeros(1, *input_shape))\r\n        return int(np.prod(o.size()))\r\n\r\n    def forward(self, vision_input, proprio_input):\r\n        conv_features = self.conv_layers(vision_input)\r\n        conv_features = conv_features.view(conv_features.size(0), -1)\r\n\r\n        # Concatenate vision and proprioceptive features\r\n        combined = torch.cat([conv_features, proprio_input], dim=1)\r\n        q_values = self.fc_layers(combined)\r\n        return q_values\r\n\r\n# Example usage in a robotic manipulation task\r\ndef train_robot_dqn():\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n\r\n    # Initialize network\r\n    state_dim = (3, 84, 84)  # Image dimensions\r\n    action_dim = 18  # Joint space actions for manipulator\r\n    dqn = RobotDQN(state_dim, action_dim).to(device)\r\n\r\n    optimizer = torch.optim.Adam(dqn.parameters(), lr=1e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    # Training loop would include:\r\n    # - Collecting experiences from robot interaction\r\n    # - Storing transitions in replay buffer\r\n    # - Sampling batches for training\r\n    # - Computing target Q-values\r\n    # - Updating network weights\r\n\r\n    return dqn\n'})}),"\n",(0,i.jsx)(e.h3,{id:"policy-gradient-methods",children:"Policy Gradient Methods"}),"\n",(0,i.jsx)(e.p,{children:"Policy gradient methods directly optimize the policy function, making them suitable for continuous action spaces common in robotic control."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example: Proximal Policy Optimization (PPO) for locomotion\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass ActorCritic(nn.Module):\r\n    def __init__(self, state_dim, action_dim):\r\n        super(ActorCritic, self).__init__()\r\n\r\n        # Shared feature extractor\r\n        self.feature_extractor = nn.Sequential(\r\n            nn.Linear(state_dim, 256),\r\n            nn.Tanh(),\r\n            nn.Linear(256, 256),\r\n            nn.Tanh()\r\n        )\r\n\r\n        # Actor network (policy)\r\n        self.actor_mean = nn.Linear(256, action_dim)\r\n        self.actor_logstd = nn.Parameter(torch.zeros(action_dim))\r\n\r\n        # Critic network (value function)\r\n        self.critic = nn.Linear(256, 1)\r\n\r\n    def forward(self, state):\r\n        features = self.feature_extractor(state)\r\n\r\n        # Actor: compute action distribution\r\n        mean = torch.tanh(self.actor_mean(features))\r\n        std = torch.exp(self.actor_logstd)\r\n\r\n        # Critic: compute value\r\n        value = self.critic(features)\r\n\r\n        return mean, std, value\r\n\r\nclass PPOTrainer:\r\n    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\r\n        self.actor_critic = ActorCritic(state_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\r\n        self.clip_epsilon = clip_epsilon\r\n\r\n    def update(self, states, actions, old_log_probs, returns, advantages):\r\n        # Forward pass\r\n        new_mean, new_std, values = self.actor_critic(states)\r\n\r\n        # Compute new log probabilities\r\n        dist = torch.distributions.Normal(new_mean, new_std)\r\n        new_log_probs = dist.log_prob(actions).sum(dim=-1)\r\n\r\n        # Compute ratio\r\n        ratio = torch.exp(new_log_probs - old_log_probs)\r\n\r\n        # PPO surrogate loss\r\n        surr1 = ratio * advantages\r\n        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\r\n        actor_loss = -torch.min(surr1, surr2).mean()\r\n\r\n        # Value loss\r\n        value_loss = nn.MSELoss()(values.squeeze(), returns)\r\n\r\n        # Total loss\r\n        total_loss = actor_loss + 0.5 * value_loss\r\n\r\n        # Update parameters\r\n        self.optimizer.zero_grad()\r\n        total_loss.backward()\r\n        torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\r\n        self.optimizer.step()\r\n\r\n        return actor_loss.item(), value_loss.item()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,i.jsx)(e.p,{children:"Imitation learning enables robots to learn complex behaviors by observing and replicating expert demonstrations. This approach is particularly valuable when defining appropriate reward functions for reinforcement learning is challenging."}),"\n",(0,i.jsx)(e.h3,{id:"behavior-cloning",children:"Behavior Cloning"}),"\n",(0,i.jsx)(e.p,{children:"Behavior cloning treats imitation learning as a supervised learning problem, where the robot learns to map observations to actions based on expert demonstrations."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Behavior Cloning for robotic manipulation\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass BehaviorCloningNet(nn.Module):\r\n    def __init__(self, obs_dim, action_dim):\r\n        super(BehaviorCloningNet, self).__init__()\r\n\r\n        self.network = nn.Sequential(\r\n            nn.Linear(obs_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, action_dim)\r\n        )\r\n\r\n    def forward(self, obs):\r\n        return self.network(obs)\r\n\r\nclass BehaviorCloning:\r\n    def __init__(self, obs_dim, action_dim, lr=1e-3):\r\n        self.network = BehaviorCloningNet(obs_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\r\n        self.criterion = nn.MSELoss()\r\n\r\n    def train_step(self, obs_batch, action_batch):\r\n        self.optimizer.zero_grad()\r\n        pred_actions = self.network(obs_batch)\r\n        loss = self.criterion(pred_actions, action_batch)\r\n        loss.backward()\r\n        self.optimizer.step()\r\n        return loss.item()\r\n\r\n# Example usage\r\ndef collect_demonstrations(robot_env, expert_policy, num_demos=1000):\r\n    """\r\n    Collect expert demonstrations for behavior cloning\r\n    """\r\n    demonstrations = []\r\n\r\n    for demo_idx in range(num_demos):\r\n        obs_list = []\r\n        action_list = []\r\n\r\n        obs = robot_env.reset()\r\n        done = False\r\n\r\n        while not done:\r\n            # Expert provides action\r\n            expert_action = expert_policy(obs)\r\n\r\n            # Store observation-action pair\r\n            obs_list.append(obs.copy())\r\n            action_list.append(expert_action.copy())\r\n\r\n            # Execute action\r\n            obs, reward, done, info = robot_env.step(expert_action)\r\n\r\n        demonstrations.append({\r\n            \'observations\': np.array(obs_list),\r\n            \'actions\': np.array(action_list)\r\n        })\r\n\r\n    return demonstrations\n'})}),"\n",(0,i.jsx)(e.h3,{id:"dagger-algorithm",children:"DAgger Algorithm"}),"\n",(0,i.jsx)(e.p,{children:"DAgger (Dataset Aggregation) addresses the covariate shift problem in behavior cloning by iteratively collecting data from the expert on states visited by the learned policy."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: DAgger algorithm implementation\r\nimport numpy as np\r\n\r\nclass DAgger:\r\n    def __init__(self, bc_model, expert_policy):\r\n        self.bc_model = bc_model\r\n        self.expert_policy = expert_policy\r\n        self.all_obs = []\r\n        self.all_actions = []\r\n\r\n    def train_iteration(self, robot_env, policy, num_rollouts=10):\r\n        """\r\n        Perform one iteration of DAgger\r\n        """\r\n        new_obs_list = []\r\n        new_action_list = []\r\n\r\n        for rollout in range(num_rollouts):\r\n            obs = robot_env.reset()\r\n            done = False\r\n\r\n            while not done:\r\n                # Query expert for action on current policy\'s state\r\n                expert_action = self.expert_policy(obs)\r\n\r\n                # Collect state-action pair\r\n                new_obs_list.append(obs.copy())\r\n                new_action_list.append(expert_action.copy())\r\n\r\n                # Execute learned policy action to visit new state\r\n                with torch.no_grad():\r\n                    policy_action = policy.network(torch.FloatTensor(obs)).numpy()\r\n\r\n                obs, reward, done, info = robot_env.step(policy_action)\r\n\r\n        # Aggregate new data with old data\r\n        self.all_obs.extend(new_obs_list)\r\n        self.all_actions.extend(new_action_list)\r\n\r\n        # Train behavior cloning model on aggregated dataset\r\n        if len(self.all_obs) > 0:\r\n            obs_tensor = torch.FloatTensor(self.all_obs)\r\n            action_tensor = torch.FloatTensor(self.all_actions)\r\n\r\n            # Train for multiple epochs\r\n            for epoch in range(10):\r\n                indices = np.random.permutation(len(self.all_obs))\r\n                for i in range(0, len(indices), 32):  # Batch size 32\r\n                    batch_indices = indices[i:i+32]\r\n                    batch_obs = obs_tensor[batch_indices]\r\n                    batch_actions = action_tensor[batch_indices]\r\n\r\n                    self.bc_model.train_step(batch_obs, batch_actions)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"adaptive-control-systems",children:"Adaptive Control Systems"}),"\n",(0,i.jsx)(e.p,{children:"Adaptive control systems enable robots to adjust their control parameters in real-time based on changing environmental conditions or system dynamics. This is crucial for humanoid robots operating in dynamic environments."}),"\n",(0,i.jsx)(e.h3,{id:"model-reference-adaptive-control-mrac",children:"Model Reference Adaptive Control (MRAC)"}),"\n",(0,i.jsx)(e.p,{children:"MRAC adjusts controller parameters to make the plant behave like a desired reference model."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example: Model Reference Adaptive Control for robotic joint\r\nimport numpy as np\r\n\r\nclass MRACController:\r\n    def __init__(self, reference_model_params, initial_controller_params):\r\n        # Reference model parameters\r\n        self.ref_A = reference_model_params['A']\r\n        self.ref_B = reference_model_params['B']\r\n        self.ref_C = reference_model_params['C']\r\n\r\n        # Controller parameters (initially unknown, adapted online)\r\n        self.theta_m = initial_controller_params['theta_m']  # Model parameters\r\n        self.theta_r = initial_controller_params['theta_r']  # Reference parameters\r\n\r\n        # Adaptation gains\r\n        self.gamma_m = 0.1  # Model adaptation gain\r\n        self.gamma_r = 0.1  # Reference adaptation gain\r\n\r\n        # State variables\r\n        self.state = np.zeros(len(self.ref_A))\r\n        self.ref_state = np.zeros(len(self.ref_A))\r\n\r\n    def update(self, y, r, dt):\r\n        \"\"\"\r\n        Update controller parameters based on tracking error\r\n        y: actual output\r\n        r: reference input\r\n        dt: time step\r\n        \"\"\"\r\n        # Compute tracking error\r\n        e = y - r\r\n\r\n        # Update reference model state\r\n        self.ref_state += dt * (self.ref_A @ self.ref_state + self.ref_B * r)\r\n\r\n        # Update controller state\r\n        u = -self.theta_m @ self.state + self.theta_r * r\r\n        self.state += dt * (self.ref_A @ self.state + self.ref_B * u)\r\n\r\n        # Adapt parameters based on error\r\n        self.theta_m += -self.gamma_m * e * self.state\r\n        self.theta_r += self.gamma_r * e * r\r\n\r\n        return u\r\n\r\n# Example: Adaptive control for robotic manipulator joint\r\nclass AdaptiveJointController:\r\n    def __init__(self, joint_id, initial_mass=1.0, initial_damping=0.1):\r\n        self.joint_id = joint_id\r\n        self.mass = initial_mass\r\n        self.damping = initial_damping\r\n\r\n        # Adaptive parameters\r\n        self.param_history = {'mass': [], 'damping': []}\r\n\r\n    def update_model(self, torque, acceleration, velocity, position, dt):\r\n        \"\"\"\r\n        Adapt model parameters based on observed dynamics\r\n        \"\"\"\r\n        # Physics-based parameter estimation\r\n        # tau = m*q_ddot + b*q_dot + k*q (simplified)\r\n        estimated_mass = torque / (acceleration + 1e-6)  # Add small value to avoid division by zero\r\n        estimated_damping = torque / (velocity + 1e-6)\r\n\r\n        # Update parameters with low-pass filtering\r\n        alpha = 0.01  # Adaptation rate\r\n        self.mass = (1 - alpha) * self.mass + alpha * estimated_mass\r\n        self.damping = (1 - alpha) * self.damping + alpha * estimated_damping\r\n\r\n        # Store history for analysis\r\n        self.param_history['mass'].append(self.mass)\r\n        self.param_history['damping'].append(self.damping)\r\n\r\n    def compute_control(self, desired_pos, desired_vel, current_pos, current_vel, dt):\r\n        \"\"\"\r\n        Compute adaptive control signal\r\n        \"\"\"\r\n        # PD control with adaptive parameters\r\n        pos_error = desired_pos - current_pos\r\n        vel_error = desired_vel - current_vel\r\n\r\n        # Compute control effort\r\n        proportional_term = self.mass * pos_error\r\n        derivative_term = self.damping * vel_error\r\n\r\n        control_signal = proportional_term + derivative_term\r\n\r\n        return control_signal\n"})}),"\n",(0,i.jsx)(e.h3,{id:"self-organizing-maps-for-adaptive-behavior",children:"Self-Organizing Maps for Adaptive Behavior"}),"\n",(0,i.jsx)(e.p,{children:"Self-Organizing Maps (SOMs) can be used for adaptive behavior learning and clustering of similar movement patterns."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Self-Organizing Map for movement pattern learning\r\nimport numpy as np\r\n\r\nclass SOMRobotLearning:\r\n    def __init__(self, grid_width, grid_height, input_dim, learning_rate=0.1):\r\n        self.grid_width = grid_width\r\n        self.grid_height = grid_height\r\n        self.input_dim = input_dim\r\n        self.learning_rate = learning_rate\r\n\r\n        # Initialize weight vectors randomly\r\n        self.weights = np.random.randn(grid_width, grid_height, input_dim)\r\n\r\n        # Neighborhood function parameters\r\n        self.sigma = max(grid_width, grid_height) / 2.0  # Initial neighborhood radius\r\n        self.lambda_decay = 1000  # Decay constant\r\n\r\n    def find_bmu(self, input_vector):\r\n        """\r\n        Find Best Matching Unit (BMU) for input vector\r\n        """\r\n        distances = np.linalg.norm(self.weights - input_vector, axis=2)\r\n        bmu_x, bmu_y = np.unravel_index(np.argmin(distances), distances.shape)\r\n        return bmu_x, bmu_y\r\n\r\n    def update_weights(self, input_vector, bmu_x, bmu_y, iteration):\r\n        """\r\n        Update weights based on neighborhood of BMU\r\n        """\r\n        # Decay parameters over time\r\n        current_lr = self.learning_rate * np.exp(-iteration / self.lambda_decay)\r\n        current_sigma = self.sigma * np.exp(-iteration / self.lambda_decay)\r\n\r\n        # Create coordinate grid\r\n        xx, yy = np.meshgrid(range(self.grid_width), range(self.grid_height))\r\n\r\n        # Calculate distance from BMU in grid space\r\n        grid_distances = (xx - bmu_x)**2 + (yy - bmu_y)**2\r\n\r\n        # Calculate neighborhood influence\r\n        neighborhood = np.exp(-grid_distances / (2 * current_sigma**2))\r\n\r\n        # Update weights\r\n        self.weights += current_lr * neighborhood[:, :, np.newaxis] * \\\r\n                       (input_vector - self.weights)\r\n\r\n    def learn_pattern(self, input_sequence, iterations=1000):\r\n        """\r\n        Learn a sequence of input patterns\r\n        """\r\n        for i in range(iterations):\r\n            idx = np.random.randint(0, len(input_sequence))\r\n            input_vector = input_sequence[idx]\r\n\r\n            bmu_x, bmu_y = self.find_bmu(input_vector)\r\n            self.update_weights(input_vector, bmu_x, bmu_y, i)\r\n\r\n    def recall_pattern(self, input_vector):\r\n        """\r\n        Recall learned pattern for input vector\r\n        """\r\n        bmu_x, bmu_y = self.find_bmu(input_vector)\r\n        return self.weights[bmu_x, bmu_y]\r\n\r\n# Example usage for learning walking patterns\r\ndef learn_walking_patterns():\r\n    # Define walking phase features (e.g., joint angles, ground reaction forces)\r\n    som = SOMRobotLearning(grid_width=10, grid_height=10, input_dim=12)\r\n\r\n    # Simulate walking pattern sequence\r\n    walking_patterns = []\r\n    for phase in range(100):  # 100 walking phases\r\n        # Generate synthetic walking features\r\n        features = np.random.randn(12)  # 12 joint angles + 4 GRF values\r\n        walking_patterns.append(features)\r\n\r\n    # Learn walking patterns\r\n    som.learn_pattern(walking_patterns, iterations=5000)\r\n\r\n    return som\n'})}),"\n",(0,i.jsx)(e.h2,{id:"transfer-learning-in-robotics",children:"Transfer Learning in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Transfer learning allows robots to leverage knowledge acquired in one task or environment to improve learning in related tasks or environments. This is particularly valuable for reducing training time and improving sample efficiency."}),"\n",(0,i.jsx)(e.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,i.jsx)(e.p,{children:"Domain adaptation techniques help transfer learned policies across different environments or robot configurations."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example: Domain adaptation for different robot morphologies\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass DomainAdaptationNet(nn.Module):\r\n    def __init__(self, shared_dim=256, source_dim=10, target_dim=12):\r\n        super(DomainAdaptationNet, self).__init__()\r\n\r\n        # Shared feature extractor\r\n        self.shared_encoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, shared_dim),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # Domain-specific decoders\r\n        self.source_decoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, source_dim)\r\n        )\r\n\r\n        self.target_decoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, target_dim)\r\n        )\r\n\r\n        # Domain classifier for adversarial training\r\n        self.domain_classifier = nn.Sequential(\r\n            nn.Linear(shared_dim, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 2)\r\n        )\r\n\r\n    def encode(self, x):\r\n        return self.shared_encoder(x)\r\n\r\n    def decode_source(self, z):\r\n        return self.source_decoder(z)\r\n\r\n    def decode_target(self, z):\r\n        return self.target_decoder(z)\r\n\r\n    def classify_domain(self, z):\r\n        return self.domain_classifier(z)\r\n\r\nclass DomainAdaptationTrainer:\r\n    def __init__(self, model, lambda_adv=0.1):\r\n        self.model = model\r\n        self.lambda_adv = lambda_adv\r\n        self.recon_criterion = nn.MSELoss()\r\n        self.domain_criterion = nn.CrossEntropyLoss()\r\n\r\n    def train_step(self, source_data, target_data):\r\n        # Encode both domains\r\n        source_encoded = self.model.encode(source_data)\r\n        target_encoded = self.model.encode(target_data)\r\n\r\n        # Reconstruct original data\r\n        source_recon = self.model.decode_source(source_encoded)\r\n        target_recon = self.model.decode_target(target_encoded)\r\n\r\n        # Reconstruction losses\r\n        source_recon_loss = self.recon_criterion(source_recon, source_data)\r\n        target_recon_loss = self.recon_criterion(target_recon, target_data)\r\n\r\n        # Domain classification\r\n        source_domains = torch.zeros(source_encoded.size(0)).long()\r\n        target_domains = torch.ones(target_encoded.size(0)).long()\r\n\r\n        all_encoded = torch.cat([source_encoded, target_encoded], dim=0)\r\n        all_domains = torch.cat([source_domains, target_domains], dim=0)\r\n\r\n        domain_preds = self.model.classify_domain(all_encoded)\r\n        domain_loss = self.domain_criterion(domain_preds, all_domains)\r\n\r\n        # Adversarial loss (reverse gradients)\r\n        # In practice, you'd use gradient reversal layer\r\n        total_loss = source_recon_loss + target_recon_loss - self.lambda_adv * domain_loss\r\n\r\n        return total_loss\n"})}),"\n",(0,i.jsx)(e.h2,{id:"safety-in-learning-systems",children:"Safety in Learning Systems"}),"\n",(0,i.jsx)(e.p,{children:"Safety considerations are paramount when deploying learning algorithms on physical robots. Safe learning ensures that robots do not damage themselves or their environment during the learning process."}),"\n",(0,i.jsx)(e.h3,{id:"safe-exploration-strategies",children:"Safe Exploration Strategies"}),"\n",(0,i.jsx)(e.p,{children:"Safe exploration balances the need to explore the environment to learn with the need to avoid dangerous states."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Safe exploration with constraint satisfaction\r\nimport numpy as np\r\nfrom scipy.optimize import minimize\r\n\r\nclass SafeExploration:\r\n    def __init__(self, safety_constraints, exploration_rate=0.1):\r\n        self.safety_constraints = safety_constraints\r\n        self.exploration_rate = exploration_rate\r\n        self.constraint_history = []\r\n\r\n    def is_safe_action(self, state, action):\r\n        """\r\n        Check if action is safe in given state\r\n        """\r\n        # Apply action and check constraints\r\n        next_state = self.simulate_transition(state, action)\r\n\r\n        for constraint in self.safety_constraints:\r\n            if not constraint(next_state):\r\n                return False\r\n        return True\r\n\r\n    def safe_exploration_policy(self, state, q_values):\r\n        """\r\n        Modify Q-values to ensure safe exploration\r\n        """\r\n        safe_actions = []\r\n        for action in range(len(q_values)):\r\n            if self.is_safe_action(state, action):\r\n                safe_actions.append(action)\r\n\r\n        if not safe_actions:\r\n            # No safe actions available, return random safe action or null\r\n            return None\r\n\r\n        # Apply epsilon-greedy on safe actions only\r\n        if np.random.rand() < self.exploration_rate:\r\n            # Explore: choose random safe action\r\n            chosen_action = np.random.choice(safe_actions)\r\n        else:\r\n            # Exploit: choose best safe action\r\n            safe_q_values = q_values[safe_actions]\r\n            best_safe_idx = np.argmax(safe_q_values)\r\n            chosen_action = safe_actions[best_safe_idx]\r\n\r\n        return chosen_action\r\n\r\n    def simulate_transition(self, state, action):\r\n        """\r\n        Simulate state transition (simplified model)\r\n        """\r\n        # Placeholder for physics simulation\r\n        next_state = state + 0.1 * action  # Simplified dynamics\r\n        return next_state\r\n\r\n# Example safety constraints\r\ndef joint_limit_constraint(state):\r\n    """Ensure joint positions are within limits"""\r\n    joint_limits = [-2.0, 2.0]  # Example limits\r\n    for joint_pos in state[:6]:  # First 6 joints\r\n        if joint_pos < joint_limits[0] or joint_pos > joint_limits[1]:\r\n            return False\r\n    return True\r\n\r\ndef collision_constraint(state):\r\n    """Check for potential collisions"""\r\n    # Simplified collision check\r\n    obstacle_positions = [1.5, -1.0]  # Example obstacle positions\r\n    robot_pos = state[0]  # Robot position\r\n\r\n    for obs_pos in obstacle_positions:\r\n        if abs(robot_pos - obs_pos) < 0.5:  # Collision threshold\r\n            return False\r\n    return True\r\n\r\n# Example usage\r\nsafety_system = SafeExploration([joint_limit_constraint, collision_constraint])\n'})}),"\n",(0,i.jsx)(e.h2,{id:"hardware-specific-implementations",children:"Hardware-Specific Implementations"}),"\n",(0,i.jsx)(e.p,{children:"Different hardware configurations require tailored learning and adaptation approaches. Let's implement specific examples for different hardware types:"}),"\n",(0,i.jsx)(e.h3,{id:"gpu-accelerated-learning",children:"GPU-Accelerated Learning"}),"\n",(0,i.jsx)(e.p,{children:"For robots with powerful GPUs, we can implement more computationally intensive learning algorithms:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-jsx",children:"// Example: GPU-accelerated learning component (React/Preact)\r\nimport React, { useState, useEffect } from 'react';\r\n\r\nconst GPULearningComponent = ({ robotSpecs }) => {\r\n  const [isLearning, setIsLearning] = useState(false);\r\n  const [performanceMetrics, setPerformanceMetrics] = useState({\r\n    fps: 0,\r\n    memoryUsage: 0,\r\n    learningRate: 0\r\n  });\r\n\r\n  useEffect(() => {\r\n    if (isLearning) {\r\n      // Initialize GPU-accelerated learning\r\n      initializeGPULearning();\r\n    }\r\n  }, [isLearning]);\r\n\r\n  const initializeGPULearning = () => {\r\n    // Check GPU capabilities\r\n    const gpuInfo = getGPUInfo();\r\n\r\n    if (gpuInfo.supportsCompute) {\r\n      // Configure learning parameters based on GPU\r\n      const params = configureLearningParams(gpuInfo);\r\n\r\n      // Start learning process\r\n      startLearningProcess(params);\r\n    }\r\n  };\r\n\r\n  const getGPUInfo = () => {\r\n    // In a real implementation, this would query WebGL/Compute capabilities\r\n    return {\r\n      supportsCompute: true,\r\n      memorySize: robotSpecs.gpuMemory || 8192, // MB\r\n      computeCapability: robotSpecs.gpuComputeCapability || '7.5'\r\n    };\r\n  };\r\n\r\n  const configureLearningParams = (gpuInfo) => {\r\n    // Adjust parameters based on GPU capabilities\r\n    const batchSize = Math.min(64, Math.floor(gpuInfo.memorySize / 1024));\r\n    const learningRate = gpuInfo.computeCapability >= 7.0 ? 0.001 : 0.0005;\r\n\r\n    return {\r\n      batchSize,\r\n      learningRate,\r\n      useMixedPrecision: gpuInfo.computeCapability >= 7.0\r\n    };\r\n  };\r\n\r\n  const startLearningProcess = (params) => {\r\n    // Simulate learning process\r\n    const interval = setInterval(() => {\r\n      setPerformanceMetrics(prev => ({\r\n        ...prev,\r\n        fps: Math.random() * 30 + 20, // 20-50 FPS\r\n        memoryUsage: Math.random() * 80 + 10, // 10-90%\r\n        learningRate: params.learningRate\r\n      }));\r\n    }, 1000);\r\n\r\n    // Cleanup\r\n    return () => clearInterval(interval);\r\n  };\r\n\r\n  return (\r\n    <div className=\"gpu-learning-container\">\r\n      <h3>GPU-Accelerated Learning</h3>\r\n      <button\r\n        onClick={() => setIsLearning(!isLearning)}\r\n        className={`btn ${isLearning ? 'btn-danger' : 'btn-success'}`}\r\n      >\r\n        {isLearning ? 'Stop Learning' : 'Start Learning'}\r\n      </button>\r\n\r\n      {isLearning && (\r\n        <div className=\"metrics\">\r\n          <p>FPS: {performanceMetrics.fps.toFixed(2)}</p>\r\n          <p>Memory Usage: {performanceMetrics.memoryUsage.toFixed(1)}%</p>\r\n          <p>Learning Rate: {performanceMetrics.learningRate}</p>\r\n        </div>\r\n      )}\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default GPULearningComponent;\n"})}),"\n",(0,i.jsx)(e.h3,{id:"jetson-based-learning",children:"Jetson-Based Learning"}),"\n",(0,i.jsx)(e.p,{children:"For NVIDIA Jetson platforms, we can implement optimized learning algorithms:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Jetson-specific learning optimizations\r\nimport jetson.inference\r\nimport jetson.utils\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass JetsonLearningOptimizer:\r\n    def __init__(self, jetson_model):\r\n        self.jetson_model = jetson_model\r\n        self.is_jetson = self.detect_jetson_platform()\r\n\r\n        if self.is_jetson:\r\n            self.configure_jetson_optimizations()\r\n\r\n    def detect_jetson_platform(self):\r\n        """Detect if running on NVIDIA Jetson"""\r\n        try:\r\n            with open(\'/proc/device-tree/model\', \'r\') as f:\r\n                model = f.read().strip(\'\\x00\')\r\n                return \'jetson\' in model.lower()\r\n        except:\r\n            return False\r\n\r\n    def configure_jetson_optimizations(self):\r\n        """Configure optimizations specific to Jetson platform"""\r\n        import subprocess\r\n\r\n        # Enable Jetson-specific optimizations\r\n        self.tensorrt_enabled = True\r\n        self.gpu_memory_fraction = 0.8\r\n\r\n        # Set power mode for optimal performance\r\n        try:\r\n            subprocess.run([\'nvpmodel\', \'-m\', \'0\'], check=True)  # MAX performance\r\n            print("Jetson configured for maximum performance mode")\r\n        except:\r\n            print("Could not configure Jetson power mode")\r\n\r\n    def optimize_model_for_jetson(self, model):\r\n        """Optimize PyTorch model for Jetson deployment"""\r\n        if not self.is_jetson:\r\n            return model\r\n\r\n        # Convert to TensorRT for inference acceleration\r\n        if self.tensorrt_enabled:\r\n            import torch_tensorrt\r\n\r\n            # Compile model with TensorRT\r\n            compiled_model = torch_tensorrt.compile(\r\n                model,\r\n                inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\r\n                enabled_precisions={torch.float, torch.half},\r\n                workspace_size=1<<25\r\n            )\r\n            return compiled_model\r\n\r\n        return model\r\n\r\n    def adaptive_learning_rate(self, episode_num):\r\n        """Adjust learning rate based on Jetson capabilities"""\r\n        if not self.is_jetson:\r\n            return 0.001\r\n\r\n        # Reduce learning rate to account for computational constraints\r\n        base_lr = 0.0005\r\n        decay_factor = 0.995\r\n\r\n        return base_lr * (decay_factor ** episode_num)\r\n\r\n# Example: Jetson-specific reinforcement learning\r\nclass JetsonRLAgent:\r\n    def __init__(self, state_dim, action_dim):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        self.optimizer = JetsonLearningOptimizer(self)\r\n\r\n        # Initialize network\r\n        self.network = self.build_network()\r\n        self.network = self.optimizer.optimize_model_for_jetson(self.network)\r\n\r\n        # Configure learning parameters for Jetson\r\n        self.learning_rate = self.optimizer.adaptive_learning_rate(0)\r\n        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\r\n\r\n    def build_network(self):\r\n        """Build lightweight network suitable for Jetson"""\r\n        return nn.Sequential(\r\n            nn.Linear(self.state_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),  # Light regularization\r\n            nn.Linear(128, 128),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(128, self.action_dim)\r\n        )\r\n\r\n    def train_step(self, states, actions, rewards):\r\n        """Perform training step optimized for Jetson"""\r\n        if not self.optimizer.is_jetson:\r\n            # Standard training\r\n            pass\r\n        else:\r\n            # Jetson-optimized training\r\n            with torch.cuda.amp.autocast():  # Mixed precision\r\n                predictions = self.network(states)\r\n                loss = nn.MSELoss()(predictions, actions)\r\n\r\n            # Scale loss for mixed precision\r\n            scaler = torch.cuda.amp.GradScaler()\r\n            scaler.scale(loss).backward()\r\n            scaler.step(self.optimizer)\r\n            scaler.update()\r\n\r\n            # Clear gradients periodically\r\n            if torch.cuda.memory_allocated() > 0.8 * torch.cuda.get_device_properties(0).total_memory:\r\n                torch.cuda.empty_cache()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"real-robot-integration",children:"Real Robot Integration"}),"\n",(0,i.jsx)(e.p,{children:"For robots with real hardware, we need to consider real-time constraints and safety:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Real robot learning with safety constraints\r\nimport rospy\r\nfrom sensor_msgs.msg import JointState\r\nfrom std_msgs.msg import Float64MultiArray\r\nimport threading\r\nimport time\r\n\r\nclass RealRobotLearner:\r\n    def __init__(self, robot_type="unitree_go2"):\r\n        self.robot_type = robot_type\r\n        self.joint_states = None\r\n        self.is_learning = False\r\n        self.safety_monitor = SafetyMonitor()\r\n\r\n        # Initialize ROS subscribers/publishers\r\n        self.joint_sub = rospy.Subscriber(\'/joint_states\', JointState, self.joint_callback)\r\n        self.command_pub = rospy.Publisher(\'/joint_commands\', Float64MultiArray, queue_size=10)\r\n\r\n        # Learning parameters\r\n        self.learning_thread = None\r\n        self.data_buffer = []\r\n        self.max_buffer_size = 1000\r\n\r\n    def joint_callback(self, msg):\r\n        """Callback for joint state updates"""\r\n        self.joint_states = msg\r\n        self.update_data_buffer(msg)\r\n\r\n    def update_data_buffer(self, joint_msg):\r\n        """Update learning data buffer"""\r\n        if len(self.data_buffer) >= self.max_buffer_size:\r\n            self.data_buffer.pop(0)  # Remove oldest entry\r\n\r\n        # Extract relevant state information\r\n        state_info = {\r\n            \'positions\': list(joint_msg.position),\r\n            \'velocities\': list(joint_msg.velocity),\r\n            \'effort\': list(joint_msg.effort),\r\n            \'timestamp\': rospy.Time.now().to_sec()\r\n        }\r\n\r\n        self.data_buffer.append(state_info)\r\n\r\n    def start_learning(self):\r\n        """Start learning process with safety monitoring"""\r\n        if not self.safety_monitor.is_safe_to_learn():\r\n            print("Safety check failed - cannot start learning")\r\n            return False\r\n\r\n        self.is_learning = True\r\n        self.learning_thread = threading.Thread(target=self.learning_loop)\r\n        self.learning_thread.start()\r\n        return True\r\n\r\n    def learning_loop(self):\r\n        """Main learning loop for real robot"""\r\n        while self.is_learning and not rospy.is_shutdown():\r\n            # Safety check\r\n            if not self.safety_monitor.is_safe_to_operate():\r\n                print("Safety violation - stopping learning")\r\n                self.stop_learning()\r\n                break\r\n\r\n            # Collect current state\r\n            if self.joint_states is not None:\r\n                current_state = self.extract_state_features(self.joint_states)\r\n\r\n                # Determine next action using learning algorithm\r\n                action = self.select_action(current_state)\r\n\r\n                # Publish action with safety limits\r\n                self.publish_safe_action(action)\r\n\r\n            time.sleep(0.01)  # 100Hz control loop\r\n\r\n    def extract_state_features(self, joint_msg):\r\n        """Extract relevant features from joint states"""\r\n        # Normalize joint positions to [-1, 1] range\r\n        position_range = 3.14  # Assuming \xb1\u03c0 range\r\n        normalized_positions = [pos / position_range for pos in joint_msg.position]\r\n\r\n        # Normalize velocities\r\n        velocity_range = 10.0  # rad/s\r\n        normalized_velocities = [vel / velocity_range for vel in joint_msg.velocity]\r\n\r\n        # Combine features\r\n        features = normalized_positions + normalized_velocities\r\n\r\n        return np.array(features)\r\n\r\n    def select_action(self, state):\r\n        """Select action based on current state and learning algorithm"""\r\n        # Placeholder for actual learning algorithm\r\n        # In practice, this would use a trained neural network or policy\r\n        action = np.random.normal(0, 0.1, size=len(state))  # Random exploration\r\n\r\n        # Ensure action is within safe limits\r\n        action = np.clip(action, -0.5, 0.5)  # Limit to \xb10.5 rad\r\n        return action\r\n\r\n    def publish_safe_action(self, action):\r\n        """Publish action with safety checks"""\r\n        # Apply safety limits\r\n        safe_action = self.safety_monitor.apply_safety_limits(action)\r\n\r\n        # Publish to robot\r\n        cmd_msg = Float64MultiArray()\r\n        cmd_msg.data = safe_action.tolist()\r\n        self.command_pub.publish(cmd_msg)\r\n\r\n    def stop_learning(self):\r\n        """Stop learning process safely"""\r\n        self.is_learning = False\r\n        if self.learning_thread:\r\n            self.learning_thread.join(timeout=1.0)\r\n\r\n        # Send zero commands to stop robot\r\n        zero_cmd = Float64MultiArray()\r\n        zero_cmd.data = [0.0] * 12  # Assuming 12 joints\r\n        self.command_pub.publish(zero_cmd)\r\n\r\nclass SafetyMonitor:\r\n    def __init__(self):\r\n        self.joint_limits = {\r\n            \'min\': [-2.0] * 12,  # Example limits\r\n            \'max\': [2.0] * 12\r\n        }\r\n        self.velocity_limits = [5.0] * 12  # rad/s\r\n        self.effort_limits = [100.0] * 12  # N*m\r\n\r\n    def is_safe_to_learn(self):\r\n        """Check if it\'s safe to start learning"""\r\n        # Check if robot is in safe initial position\r\n        # Check if emergency stop is not engaged\r\n        # Check if environment is safe\r\n        return True  # Placeholder\r\n\r\n    def is_safe_to_operate(self):\r\n        """Check if it\'s safe to continue operation"""\r\n        # Check joint limits\r\n        # Check for collisions\r\n        # Check for hardware faults\r\n        return True  # Placeholder\r\n\r\n    def apply_safety_limits(self, action):\r\n        """Apply safety limits to action"""\r\n        limited_action = np.clip(action,\r\n                                np.array(self.joint_limits[\'min\']),\r\n                                np.array(self.joint_limits[\'max\']))\r\n        return limited_action\r\n\r\n# Example usage\r\ndef main():\r\n    rospy.init_node(\'real_robot_learner\')\r\n\r\n    learner = RealRobotLearner(robot_type="unitree_go2")\r\n\r\n    # Start learning\r\n    if learner.start_learning():\r\n        print("Learning started successfully")\r\n\r\n        # Run for 10 minutes or until stopped\r\n        start_time = time.time()\r\n        while time.time() - start_time < 600 and not rospy.is_shutdown():\r\n            time.sleep(1)\r\n\r\n        learner.stop_learning()\r\n        print("Learning stopped")\r\n\r\n    rospy.spin()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"urdu-content-\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u0627\u0648\u0631-\u0645\u0637\u0627\u0628\u0642\u062a-\u06a9\u06d2-\u0646\u0638\u0627\u0645",children:"Urdu Content: \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u06a9\u06d2 \u0646\u0638\u0627\u0645"}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsx)("summary",{children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u067e\u0691\u06be\u06cc\u06ba / Show in Urdu"}),(0,i.jsx)(e.h1,{id:"\u0628\u0627\u0628-10-\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u0627\u0648\u0631-\u0645\u0637\u0627\u0628\u0642\u062a-\u06a9\u06d2-\u0646\u0638\u0627\u0645",children:"\u0628\u0627\u0628 10: \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u06a9\u06d2 \u0646\u0638\u0627\u0645"}),(0,i.jsx)(e.h2,{id:"\u062a\u0639\u0627\u0631\u0641",children:"\u062a\u0639\u0627\u0631\u0641"}),(0,i.jsx)(e.p,{children:'"\u0633\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u06a9\u06d2 \u0646\u0638\u0627\u0645" \u06a9\u0627 \u0628\u0627\u0628 10 \u0622\u067e \u06a9\u0648 \u0641\u0632\u06cc\u06a9\u0644 \u0627\u06cc \u0622\u0626\u06cc \u0627\u0648\u0631 \u06c1\u06cc\u0648\u0645\u0646\u0648\u0627\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u06a9\u06d2 \u0679\u06cc\u06a9\u0633\u0679 \u0628\u06a9 \u0645\u06cc\u06ba \u062e\u0648\u0634 \u0622\u0645\u062f\u06cc\u062f \u06a9\u06c1\u062a\u0627 \u06c1\u06d2\u06d4 \u0627\u0633 \u0628\u0627\u0628 \u0645\u06cc\u06ba\u060c \u06c1\u0645 \u06c1\u06cc\u0648\u0645\u0646\u0648\u0627\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u06a9\u06cc \u062f\u0644\u0686\u0633\u067e \u062f\u0646\u06cc\u0627 \u06a9\u0627 \u062c\u0627\u0626\u0632\u06c1 \u0644\u06cc\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06c1\u0645 \u0627\u0633 \u0628\u0627\u062a \u06a9\u0627 \u062c\u0627\u0626\u0632\u06c1 \u0644\u06cc\u06ba \u06af\u06d2 \u06a9\u06c1 \u0631\u0648\u0628\u0648\u0679 \u0646\u0626\u06d2 \u06c1\u0646\u0631 \u06a9\u06cc\u0633\u06d2 \u062d\u0627\u0635\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c \u062a\u0628\u062f\u06cc\u0644 \u06c1\u0648\u062a\u06d2 \u0645\u0627\u062d\u0648\u0644 \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u067e\u0646\u06d2 \u0622\u067e \u06a9\u0648 \u06a9\u06cc\u0633\u06d2 \u0688\u06be\u0627\u0644\u062a\u06d2 \u06c1\u06cc\u06ba\u060c \u0627\u0648\u0631 \u0645\u062e\u062a\u0644\u0641 \u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af \u06a9\u06cc \u062a\u06a9\u0646\u06cc\u06a9\u0648\u06ba \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u0627\u067e\u0646\u0627 \u06a9\u0627\u0645 \u06a9\u06cc\u0633\u06d2 \u0628\u06c1\u062a\u0631 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4'}),(0,i.jsx)(e.p,{children:"\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af \u0646\u06d2 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u06a9\u0648 \u0627\u0646\u0642\u0644\u0627\u0628\u06cc \u0637\u0648\u0631 \u067e\u0631 \u0628\u062f\u0644 \u062f\u06cc\u0627 \u06c1\u06d2 \u06a9\u06cc\u0648\u0646\u06a9\u06c1 \u0627\u0633 \u0646\u06d2 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u062a\u062c\u0631\u0628\u06c1 \u0633\u06d2 \u0633\u06cc\u06a9\u06be\u0646\u06d2\u060c \u0646\u0626\u06cc \u0635\u0648\u0631\u062a\u062d\u0627\u0644 \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u067e\u0646\u06d2 \u0622\u067e \u06a9\u0648 \u0688\u06be\u0627\u0644\u0646\u06d2\u060c \u0627\u0648\u0631 \u06c1\u0631 \u0627\u0633\u0679\u06cc\u0679 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0635\u0631\u0641 \u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0646\u06af \u06a9\u06d2 \u0628\u063a\u06cc\u0631 \u062c\u0679\u06cc\u0644 \u0637\u0631\u0632 \u0639\u0645\u0644 \u062a\u06cc\u0627\u0631 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u06cc\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u0628\u0627\u0628 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0628\u0646\u06cc\u0627\u062f\u06cc \u0637\u0631\u06cc\u0642\u06d2 \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2\u060c \u0628\u0634\u0645\u0648\u0644 \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af\u060c \u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af\u060c \u0627\u0648\u0631 \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0633\u0633\u0679\u0645\u0632\u06d4"}),(0,i.jsx)(e.p,{children:"\u0627\u0646 \u062a\u0635\u0648\u0631\u0627\u062a \u06a9\u0648 \u0633\u0645\u062c\u06be\u0646\u0627 \u06c1\u06cc\u0648\u0645\u0646\u0648\u0627\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u062a\u06cc\u0627\u0631 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0636\u0631\u0648\u0631\u06cc \u06c1\u06d2 \u062c\u0648 \u0645\u062a\u062d\u0631\u06a9\u060c \u062d\u0642\u06cc\u0642\u06cc \u062f\u0646\u06cc\u0627 \u06a9\u06d2 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u0645\u0624\u062b\u0631 \u0637\u0631\u06cc\u0642\u06d2 \u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631 \u0633\u06a9\u06cc\u06ba\u06d4 \u0686\u0627\u06c1\u06d2 \u06cc\u06c1 \u0632\u06cc\u0627\u062f\u06c1 \u06a9\u0627\u0631\u0622\u0645\u062f \u0686\u0644\u0646\u0627 \u0633\u06cc\u06a9\u06be\u0646\u0627 \u06c1\u0648\u060c \u06c1\u0627\u062a\u06be \u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u0646\u06d2 \u06a9\u06cc \u062d\u06a9\u0645\u062a \u0639\u0645\u0644\u06cc\u0648\u06ba \u06a9\u0648 \u0688\u06be\u0627\u0644\u0646\u0627 \u06c1\u0648\u060c \u06cc\u0627 \u0627\u0646\u0633\u0627\u0646\u0648\u06ba \u06a9\u06d2 \u0633\u0627\u062a\u06be \u062a\u0639\u0627\u0645\u0644 \u06a9\u0648 \u0628\u06c1\u062a\u0631 \u0628\u0646\u0627\u0646\u0627 \u06c1\u0648\u060c \u0633\u06cc\u06a9\u06be\u0646\u0627 \u0627\u0648\u0631 \u0645\u0637\u0627\u0628\u0642\u062a \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u06a9\u06d2 \u0633\u0686\u06d2 \u0627\u0646\u0679\u06cc\u0644\u06cc\u062c\u0646\u0679 \u0633\u0633\u0679\u0645\u0632 \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.h2,{id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0637\u0631\u06cc\u0642\u06d2",children:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0637\u0631\u06cc\u0642\u06d2"}),(0,i.jsx)(e.p,{children:"\u0440\u043e\u0431\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af \u06a9\u0626\u06cc \u0637\u0631\u06cc\u0642\u0648\u06ba \u06a9\u0648 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u062c\u0646 \u0645\u06cc\u06ba \u0633\u06d2 \u06c1\u0631 \u0627\u06cc\u06a9 \u0645\u062e\u062a\u0644\u0641 \u0642\u0633\u0645 \u06a9\u06d2 \u0645\u0633\u0627\u0626\u0644 \u0627\u0648\u0631 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0645\u0642\u0627\u0635\u062f \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0646\u0627\u0633\u0628 \u06c1\u06d2\u06d4 \u0627\u06c1\u0645 \u0637\u0631\u06cc\u0642\u06d2 \u062f\u0631\u062c \u0630\u06cc\u0644 \u06c1\u06cc\u06ba:"}),(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af"}),": \u0644\u06cc\u0628\u0644 \u0648\u0627\u0644\u06d2 \u0688\u06cc\u0679\u0627 \u0633\u06cc\u0679 \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06a9\u06d2 \u0645\u0627\u0688\u0644\u0632 \u06a9\u0648 \u062a\u0631\u0628\u06cc\u062a \u062f\u06cc\u0646\u0627 \u062c\u0648 \u0627\u0646 \u067e\u0679 \u0688\u06cc\u0679\u0627 \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u0646\u062a\u0627\u0626\u062c \u06a9\u06cc \u067e\u06cc\u0634\u0646 \u06af\u0648\u0626\u06cc \u06a9\u0631 \u0633\u06a9\u06cc\u06ba"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af"}),": \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0646\u0639\u0627\u0645 \u06cc\u0627 \u0633\u0632\u0627 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u062a\u062c\u0631\u0628\u06c1 \u0627\u0648\u0631 \u063a\u0644\u0637\u06cc \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u0633\u06cc\u06a9\u06be\u0646\u0627"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af"}),": \u0645\u0627\u06c1\u0631 \u06a9\u06cc \u0645\u0638\u0627\u06c1\u0631\u06d2 \u06a9\u0648 \u062f\u06cc\u06a9\u06be \u06a9\u0631 \u0627\u0648\u0631 \u0646\u0642\u0644 \u06a9\u0631 \u06a9\u06d2 \u0633\u06cc\u06a9\u06be\u0646\u0627"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u063a\u06cc\u0631 \u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af"}),": \u0644\u06cc\u0628\u0644 \u0648\u0627\u0644\u06d2 \u0688\u06cc\u0679\u0627 \u06a9\u06d2 \u0628\u063a\u06cc\u0631 \u0646\u0645\u0648\u0646\u0648\u06ba \u0627\u0648\u0631 \u0633\u0627\u062e\u062a\u0648\u06ba \u06a9\u0648 \u062f\u0631\u06cc\u0627\u0641\u062a \u06a9\u0631\u0646\u0627"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u062e\u0648\u062f \u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af"}),": \u0688\u06cc\u0679\u0627 \u06a9\u06d2 \u062e\u0648\u062f \u0633\u06d2 \u0646\u06af\u0631\u0627\u0646\u06cc \u067e\u06cc\u062f\u0627 \u06a9\u0631\u0646\u0627"]}),"\n"]}),(0,i.jsx)(e.p,{children:"\u06c1\u0631 \u0637\u0631\u06cc\u0642\u06c1 \u0627\u067e\u0646\u06d2 \u0645\u0646\u0641\u0631\u062f \u0641\u0648\u0627\u0626\u062f \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 \u0627\u06a9\u062b\u0631 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0645\u0636\u0628\u0648\u0637 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06cc \u0635\u0644\u0627\u062d\u06cc\u062a\u0648\u06ba \u06a9\u0648 \u062d\u0627\u0635\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0644\u0627 \u06a9\u0631 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u06cc\u0627 \u062c\u0627\u062a\u0627 \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:"graph TD\r\n    A[\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0637\u0631\u06cc\u0642\u06d2] --\x3e B[\u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af]\r\n    A --\x3e C[\u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af]\r\n    A --\x3e D[\u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af]\r\n    A --\x3e E[\u063a\u06cc\u0631 \u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af]\r\n    A --\x3e F[\u062e\u0648\u062f \u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af]\r\n\r\n    B --\x3e B1[\u0646\u0645\u0648\u0646\u06c1 \u067e\u06c1\u0686\u0627\u0646]\r\n    B --\x3e B2[\u0622\u0628\u062c\u06cc\u06a9\u0679 \u0688\u06cc\u0679\u06cc\u06a9\u0634\u0646]\r\n\r\n    C --\x3e C1[\u067e\u0627\u0644\u06cc\u0633\u06cc \u06a9\u06cc \u0627\u0635\u0644\u0627\u062d]\r\n    C --\x3e C2[\u0648\u06cc\u0644\u06cc\u0648 \u06a9\u0627 \u062a\u062e\u0645\u06cc\u0646\u06c1]\r\n\r\n    D --\x3e D1[\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af]\r\n    D --\x3e D2[\u0688\u06cc\u06af\u0631 \u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645]\r\n\r\n    E --\x3e E1[\u06a9\u0644\u0633\u0679\u0631\u0646\u06af]\r\n    E --\x3e E2[\u0688\u0627\u0626\u0645\u0646\u0634\u0646\u0644\u0679\u06cc \u0631\u06cc\u0688\u06a9\u0634\u0646]\r\n\r\n    F --\x3e F1[\u067e\u0631\u06cc\u0688\u06a9\u0679\u0648 \u0644\u0631\u0646\u0646\u06af]\r\n    F --\x3e F2[\u06a9\u0646\u0679\u0631\u0627\u0633\u0679\u0648 \u0644\u0631\u0646\u0646\u06af]\n"})}),(0,i.jsx)(e.h2,{id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0645\u0636\u0628\u0648\u0637-\u0644\u0631\u0646\u0646\u06af",children:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af"}),(0,i.jsx)(e.p,{children:"\u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af (RL) \u0628\u0627\u0644\u062e\u0635\u0648\u0635 \u0627\u0633 \u0648\u0642\u062a \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0646\u0627\u0633\u0628 \u06c1\u06d2 \u062c\u0628 \u0627\u06cc\u062c\u0646\u0679\u0633 \u06a9\u0648 \u0641\u0632\u06cc\u06a9\u0644 \u0645\u0627\u062d\u0648\u0644 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0645\u062a\u0633\u0644\u0633\u0644 \u0641\u06cc\u0635\u0644\u06c1 \u0633\u0627\u0632\u06cc \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u0628\u0627\u062a \u0686\u06cc\u062a \u06a9\u0631\u0646\u0627 \u0633\u06cc\u06a9\u06be\u0646\u06cc \u06c1\u0648\u06d4 RL \u0645\u06cc\u06ba\u060c \u0627\u06cc\u06a9 \u0627\u06cc\u062c\u0646\u0679 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u0627\u0642\u062f\u0627\u0645\u0627\u062a \u06a9\u0631 \u06a9\u06d2 \u06a9\u0644 \u0627\u0646\u0639\u0627\u0645 \u06a9\u0648 \u0632\u06cc\u0627\u062f\u06c1 \u0633\u06d2 \u0632\u06cc\u0627\u062f\u06c1 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0633\u06cc\u06a9\u06be\u062a\u0627 \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.h3,{id:"rl-\u06a9\u06d2-\u0628\u0646\u06cc\u0627\u062f\u06cc-\u062c\u0632\u0648",children:"RL \u06a9\u06d2 \u0628\u0646\u06cc\u0627\u062f\u06cc \u062c\u0632\u0648"}),(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0627\u0633\u0679\u06cc\u0679 \u0627\u0633\u067e\u06cc\u0633"}),": \u062a\u0645\u0627\u0645 \u0645\u0645\u06a9\u0646\u06c1 \u0627\u0633\u0679\u06cc\u0679\u0633 \u06a9\u0627 \u0633\u06cc\u0679 \u062c\u0646 \u0645\u06cc\u06ba \u0631\u0648\u0628\u0648\u0679 \u06c1\u0648 \u0633\u06a9\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0627\u06cc\u06a9\u0634\u0646 \u0627\u0633\u067e\u06cc\u0633"}),": \u062a\u0645\u0627\u0645 \u0645\u0645\u06a9\u0646\u06c1 \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06a9\u0627 \u0633\u06cc\u0679 \u062c\u0648 \u0631\u0648\u0628\u0648\u0679 \u06a9\u0631 \u0633\u06a9\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0627\u0646\u0639\u0627\u0645 \u0641\u0646\u06a9\u0634\u0646"}),": \u06c1\u0631 \u0627\u0633\u0679\u06cc\u0679 \u0627\u06cc\u06a9\u0634\u0646 \u062c\u0648\u0691\u06cc \u06a9\u0648 \u0627\u0633\u06a9\u06cc\u0644\u0631 \u0627\u0646\u0639\u0627\u0645 \u062a\u0641\u0648\u06cc\u0636 \u06a9\u0631\u0646\u06d2 \u0648\u0627\u0644\u0627 \u0641\u0646\u06a9\u0634\u0646"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u067e\u0627\u0644\u06cc\u0633\u06cc"}),": \u0627\u0633\u0679\u06cc\u0679\u0633 \u0633\u06d2 \u0627\u06cc\u06a9\u0634\u0646\u0632 \u062a\u06a9 \u0645\u06cc\u067e\u0646\u06af \u062c\u0648 \u0627\u06cc\u062c\u0646\u0679 \u06a9\u06d2 \u0637\u0631\u0632 \u0639\u0645\u0644 \u06a9\u06cc \u0648\u0636\u0627\u062d\u062a \u06a9\u0631\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"\u0648\u06cc\u0644\u06cc\u0648 \u0641\u0646\u06a9\u0634\u0646"}),": \u0627\u06cc\u06a9 \u062f\u06cc \u06af\u0626\u06cc \u0627\u0633\u0679\u06cc\u0679 \u0633\u06d2 \u0645\u062a\u0648\u0642\u0639 \u06a9\u0644 \u0627\u0646\u0639\u0627\u0645 \u06a9\u0627 \u062a\u062e\u0645\u06cc\u0646\u06c1 \u0644\u06af\u0627\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),(0,i.jsx)(e.h3,{id:"\u06af\u06c1\u0631\u0627-\u0645\u0636\u0628\u0648\u0637-\u0644\u0631\u0646\u0646\u06af",children:"\u06af\u06c1\u0631\u0627 \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af"}),(0,i.jsx)(e.p,{children:"\u06af\u06c1\u0631\u0627 RL \u06af\u06c1\u0631\u06d2 \u0646\u06cc\u0648\u0631\u0644 \u0646\u06cc\u0679 \u0648\u0631\u06a9\u0633 \u06a9\u0648 \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af \u06a9\u06d2 \u0633\u0627\u062a\u06be \u062c\u0648\u0691\u062a\u0627 \u06c1\u06d2\u060c \u062c\u0633 \u0633\u06d2 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06cc\u0645\u0631\u06c1 \u06a9\u06cc \u062a\u0635\u0627\u0648\u06cc\u0631 \u0627\u0648\u0631 \u062c\u0648\u0627\u0626\u0646\u0679 \u0627\u0646\u06a9\u0648\u0688\u0631\u0632 \u062c\u06cc\u0633\u06d2 \u0632\u06cc\u0627\u062f\u06c1 \u0628\u0639\u062f\u06cc \u062d\u0648\u0627\u0633\u06cc \u0627\u0646 \u067e\u0679\u0633 \u0633\u06d2 \u062c\u0679\u06cc\u0644 \u0637\u0631\u0632 \u0639\u0645\u0644 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u06c1\u0648 \u062c\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644: \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u0646\u06cc\u067e\u0648\u0644\u06cc\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0688\u06cc\u0641 Q-\u0646\u06cc\u0679 \u0648\u0631\u06a9 (DQN)\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\nclass RobotDQN(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(RobotDQN, self).__init__()\r\n\r\n        # \u0648\u0698\u0646 \u0627\u0646 \u067e\u0679 \u06a9\u0648 \u067e\u0631\u0648\u0633\u06cc\u0633 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u0646\u0648\u0644\u0648\u0634\u0646\u0644 \u0644\u06cc\u0626\u0631\u0632\r\n        self.conv_layers = nn.Sequential(\r\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # \u06a9\u0646\u0648\u0644\u0648\u0634\u0646 \u06a9\u06d2 \u0628\u0639\u062f \u0622\u0624\u0679 \u067e\u0679 \u0633\u0627\u0626\u0632 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        conv_out_size = self._get_conv_out_size(state_dim)\r\n\r\n        # \u0645\u06a9\u0645\u0644 \u0637\u0648\u0631 \u067e\u0631 \u0645\u0646\u0633\u0644\u06a9 \u0644\u06cc\u0626\u0631\u0632\r\n        self.fc_layers = nn.Sequential(\r\n            nn.Linear(conv_out_size + 6, hidden_dim),  # \u067e\u0631\u0648\u067e\u0631\u06cc\u0648\u0633\u06cc\u0641\u0679\u0648 \u062e\u0635\u0648\u0635\u06cc\u0627\u062a \u0634\u0627\u0645\u0644 \u06a9\u0631\u06cc\u06ba\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim)\r\n        )\r\n\r\n    def _get_conv_out_size(self, input_shape):\r\n        """\u06a9\u0646\u0648\u0644\u0648\u0634\u0646 \u0644\u06cc\u0626\u0631\u0632 \u06a9\u06d2 \u0628\u0639\u062f \u0622\u0624\u0679 \u067e\u0679 \u0633\u0627\u0626\u0632 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba"""\r\n        o = self.conv_layers(torch.zeros(1, *input_shape))\r\n        return int(np.prod(o.size()))\r\n\r\n    def forward(self, vision_input, proprio_input):\r\n        conv_features = self.conv_layers(vision_input)\r\n        conv_features = conv_features.view(conv_features.size(0), -1)\r\n\r\n        # \u0648\u0698\u0646 \u0627\u0648\u0631 \u067e\u0631\u0648\u067e\u0631\u06cc\u0648\u0633\u06cc\u0641\u0679\u0648 \u062e\u0635\u0648\u0635\u06cc\u0627\u062a \u06a9\u0648 \u062c\u0648\u0691\u06cc\u06ba\r\n        combined = torch.cat([conv_features, proprio_input], dim=1)\r\n        q_values = self.fc_layers(combined)\r\n        return q_values\r\n\r\n# \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u0646\u06cc\u067e\u0648\u0644\u06cc\u0634\u0646 \u0679\u0627\u0633\u06a9 \u0645\u06cc\u06ba \u0645\u062b\u0627\u0644 \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644\r\ndef train_robot_dqn():\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n\r\n    # \u0646\u06cc\u0679 \u0648\u0631\u06a9 \u06a9\u0648 \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba\r\n    state_dim = (3, 84, 84)  # \u062a\u0635\u0648\u06cc\u0631 \u06a9\u06d2 \u0627\u0628\u0639\u0627\u062f\r\n    action_dim = 18  # \u0645\u06cc\u0646\u06cc\u067e\u0648\u0644\u06cc\u0679\u0631 \u06a9\u06d2 \u062c\u0648\u0627\u0626\u0646\u0679 \u0627\u0633\u067e\u06cc\u0633 \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n    dqn = RobotDQN(state_dim, action_dim).to(device)\r\n\r\n    optimizer = torch.optim.Adam(dqn.parameters(), lr=1e-4)\r\n    criterion = nn.MSELoss()\r\n\r\n    # \u062a\u0631\u0628\u06cc\u062a \u0644\u0648\u067e \u0645\u06cc\u06ba \u0634\u0627\u0645\u0644 \u06c1\u0648\u06af\u0627:\r\n    # - \u0631\u0648\u0628\u0648\u0679 \u06a9\u06cc \u0628\u0627\u062a \u0686\u06cc\u062a \u0633\u06d2 \u062a\u062c\u0631\u0628\u0627\u062a \u062c\u0645\u0639 \u06a9\u0631\u0646\u0627\r\n    # - \u0631\u06cc\u067e\u0644\u06d2 \u0628\u0641\u0631 \u0645\u06cc\u06ba \u0679\u0631\u0627\u0646\u0632\u06cc\u0634\u0646\u0632 \u0627\u0633\u0679\u0648\u0631 \u06a9\u0631\u0646\u0627\r\n    # - \u062a\u0631\u0628\u06cc\u062a \u06a9\u06d2 \u0644\u06cc\u06d2 \u0628\u06cc\u0686\u0633 \u06a9\u0627 \u0646\u0645\u0648\u0646\u06c1 \u0644\u06cc\u0646\u0627\r\n    # - \u06c1\u062f\u0641 Q-\u0648\u06cc\u0644\u06cc\u0648\u0632 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0646\u0627\r\n    # - \u0646\u06cc\u0679 \u0648\u0631\u06a9 \u0648\u0632\u0646 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u0646\u0627\r\n\r\n    return dqn\n'})}),(0,i.jsx)(e.h3,{id:"\u067e\u0627\u0644\u06cc\u0633\u06cc-\u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679-\u0645\u06cc\u062a\u06be\u0688\u0633",children:"\u067e\u0627\u0644\u06cc\u0633\u06cc \u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679 \u0645\u06cc\u062a\u06be\u0688\u0633"}),(0,i.jsx)(e.p,{children:"\u067e\u0627\u0644\u06cc\u0633\u06cc \u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679 \u0645\u06cc\u062a\u06be\u0688\u0633 \u0628\u0631\u0627\u06c1 \u0631\u0627\u0633\u062a \u067e\u0627\u0644\u06cc\u0633\u06cc \u0641\u0646\u06a9\u0634\u0646 \u06a9\u06cc \u0627\u0635\u0644\u0627\u062d \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c \u062c\u0648 \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0645\u06cc\u06ba \u0645\u0633\u0644\u0633\u0644 \u0627\u06cc\u06a9\u0634\u0646 \u0627\u0633\u067e\u06cc\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0646\u0627\u0633\u0628 \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644: \u0644\u0627\u06a9\u0648\u0645\u0648\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u067e\u0631\u0648\u06a9\u0633\u06cc\u0645\u0644 \u067e\u0627\u0644\u06cc\u0633\u06cc \u0622\u067e\u0679\u06cc\u0645\u0627\u0626\u0632\u0646\u06af (PPO)\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass ActorCritic(nn.Module):\r\n    def __init__(self, state_dim, action_dim):\r\n        super(ActorCritic, self).__init__()\r\n\r\n        # \u0645\u0634\u062a\u0631\u06a9\u06c1 \u0641\u06cc\u0686\u0631 \u0627\u06cc\u06a9\u0633\u0679\u0631\u06cc\u06a9\u0679\u0631\r\n        self.feature_extractor = nn.Sequential(\r\n            nn.Linear(state_dim, 256),\r\n            nn.Tanh(),\r\n            nn.Linear(256, 256),\r\n            nn.Tanh()\r\n        )\r\n\r\n        # \u0627\u06cc\u06a9\u0679\u0631 \u0646\u06cc\u0679 \u0648\u0631\u06a9 (\u067e\u0627\u0644\u06cc\u0633\u06cc)\r\n        self.actor_mean = nn.Linear(256, action_dim)\r\n        self.actor_logstd = nn.Parameter(torch.zeros(action_dim))\r\n\r\n        # \u06a9\u0631\u06cc\u0679\u06a9 \u0646\u06cc\u0679 \u0648\u0631\u06a9 (\u0648\u06cc\u0644\u06cc\u0648 \u0641\u0646\u06a9\u0634\u0646)\r\n        self.critic = nn.Linear(256, 1)\r\n\r\n    def forward(self, state):\r\n        features = self.feature_extractor(state)\r\n\r\n        # \u0627\u06cc\u06a9\u0679\u0631: \u0627\u06cc\u06a9\u0634\u0646 \u062a\u0642\u0633\u06cc\u0645 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        mean = torch.tanh(self.actor_mean(features))\r\n        std = torch.exp(self.actor_logstd)\r\n\r\n        # \u06a9\u0631\u06cc\u0679\u06a9: \u0648\u06cc\u0644\u06cc\u0648 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        value = self.critic(features)\r\n\r\n        return mean, std, value\r\n\r\nclass PPOTrainer:\r\n    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\r\n        self.actor_critic = ActorCritic(state_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\r\n        self.clip_epsilon = clip_epsilon\r\n\r\n    def update(self, states, actions, old_log_probs, returns, advantages):\r\n        # \u0641\u0627\u0631\u0648\u0631\u0688 \u067e\u0627\u0633\r\n        new_mean, new_std, values = self.actor_critic(states)\r\n\r\n        # \u0646\u0626\u06d2 \u0644\u0627\u06af \u0627\u0645\u06a9\u0627\u0646\u0627\u062a \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        dist = torch.distributions.Normal(new_mean, new_std)\r\n        new_log_probs = dist.log_prob(actions).sum(dim=-1)\r\n\r\n        # \u062a\u0646\u0627\u0633\u0628 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        ratio = torch.exp(new_log_probs - old_log_probs)\r\n\r\n        # PPO \u0633\u0631\u0631\u0648\u06af\u06cc\u0679 \u0646\u0642\u0635\u0627\u0646\r\n        surr1 = ratio * advantages\r\n        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\r\n        actor_loss = -torch.min(surr1, surr2).mean()\r\n\r\n        # \u0648\u06cc\u0644\u06cc\u0648 \u0646\u0642\u0635\u0627\u0646\r\n        value_loss = nn.MSELoss()(values.squeeze(), returns)\r\n\r\n        # \u06a9\u0644 \u0646\u0642\u0635\u0627\u0646\r\n        total_loss = actor_loss + 0.5 * value_loss\r\n\r\n        # \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        self.optimizer.zero_grad()\r\n        total_loss.backward()\r\n        torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\r\n        self.optimizer.step()\r\n\r\n        return actor_loss.item(), value_loss.item()\n"})}),(0,i.jsx)(e.h2,{id:"\u0646\u0642\u0644-\u0644\u06cc\u0646\u0646\u06af",children:"\u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af"}),(0,i.jsx)(e.p,{children:"\u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u0645\u0627\u06c1\u0631 \u0645\u0638\u0627\u06c1\u0631\u06d2 \u062f\u06cc\u06a9\u06be \u06a9\u0631 \u0627\u0648\u0631 \u0646\u0642\u0644 \u06a9\u0631 \u06a9\u06d2 \u062c\u0679\u06cc\u0644 \u0637\u0631\u0632 \u0639\u0645\u0644 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06cc \u0627\u062c\u0627\u0632\u062a \u062f\u06cc\u062a\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u0627\u0633 \u0648\u0642\u062a \u062e\u0627\u0635 \u0637\u0648\u0631 \u067e\u0631 \u0642\u06cc\u0645\u062a\u06cc \u06c1\u06d2 \u062c\u0628 \u0645\u0636\u0628\u0648\u0637 \u0644\u0631\u0646\u0646\u06af \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0646\u0627\u0633\u0628 \u0627\u0646\u0639\u0627\u0645 \u0641\u0646\u06a9\u0634\u0646\u0632 \u06a9\u06cc \u0648\u0636\u0627\u062d\u062a \u06a9\u0631\u0646\u0627 \u0686\u06cc\u0644\u0646\u062c\u0646\u06af \u06c1\u0648\u06d4"}),(0,i.jsx)(e.h3,{id:"\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631-\u06a9\u0644\u0648\u0646\u0646\u06af",children:"\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af"}),(0,i.jsx)(e.p,{children:"\u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af \u0646\u0642\u0644 \u0644\u06cc\u0646\u0646\u06af \u06a9\u0648 \u0627\u06cc\u06a9 \u0646\u06af\u0631\u0627\u0646\u06cc \u0634\u062f\u06c1 \u0644\u0631\u0646\u0646\u06af \u06a9\u06d2 \u0645\u0633\u0626\u0644\u06c1 \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 \u0633\u0645\u062c\u06be\u062a\u0627 \u06c1\u06d2\u060c \u062c\u06c1\u0627\u06ba \u0631\u0648\u0628\u0648\u0679 \u0645\u0627\u06c1\u0631 \u0645\u0638\u0627\u06c1\u0631\u06d2 \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u0645\u0634\u0627\u06c1\u062f\u0627\u062a \u06a9\u0648 \u0627\u06cc\u06a9\u0634\u0646\u0632 \u0645\u06cc\u06ba \u0645\u06cc\u067e \u06a9\u0631\u0646\u0627 \u0633\u06cc\u06a9\u06be\u062a\u0627 \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644: \u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u0646\u06cc\u067e\u0648\u0644\u06cc\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass BehaviorCloningNet(nn.Module):\r\n    def __init__(self, obs_dim, action_dim):\r\n        super(BehaviorCloningNet, self).__init__()\r\n\r\n        self.network = nn.Sequential(\r\n            nn.Linear(obs_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, action_dim)\r\n        )\r\n\r\n    def forward(self, obs):\r\n        return self.network(obs)\r\n\r\nclass BehaviorCloning:\r\n    def __init__(self, obs_dim, action_dim, lr=1e-3):\r\n        self.network = BehaviorCloningNet(obs_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\r\n        self.criterion = nn.MSELoss()\r\n\r\n    def train_step(self, obs_batch, action_batch):\r\n        self.optimizer.zero_grad()\r\n        pred_actions = self.network(obs_batch)\r\n        loss = self.criterion(pred_actions, action_batch)\r\n        loss.backward()\r\n        self.optimizer.step()\r\n        return loss.item()\r\n\r\n# \u0645\u062b\u0627\u0644 \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644\r\ndef collect_demonstrations(robot_env, expert_policy, num_demos=1000):\r\n    """\r\n    \u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0627\u06c1\u0631 \u0645\u0638\u0627\u06c1\u0631\u06d2 \u062c\u0645\u0639 \u06a9\u0631\u06cc\u06ba\r\n    """\r\n    demonstrations = []\r\n\r\n    for demo_idx in range(num_demos):\r\n        obs_list = []\r\n        action_list = []\r\n\r\n        obs = robot_env.reset()\r\n        done = False\r\n\r\n        while not done:\r\n            # \u0645\u0627\u06c1\u0631 \u0627\u06cc\u06a9\u0634\u0646 \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\r\n            expert_action = expert_policy(obs)\r\n\r\n            # \u0645\u0634\u0627\u06c1\u062f\u06c1 \u0627\u06cc\u06a9\u0634\u0646 \u062c\u0648\u0691\u06cc \u0627\u0633\u0679\u0648\u0631 \u06a9\u0631\u06cc\u06ba\r\n            obs_list.append(obs.copy())\r\n            action_list.append(expert_action.copy())\r\n\r\n            # \u0627\u06cc\u06a9\u0634\u0646 \u0627\u0646\u062c\u0627\u0645 \u062f\u06cc\u06ba\r\n            obs, reward, done, info = robot_env.step(expert_action)\r\n\r\n        demonstrations.append({\r\n            \'observations\': np.array(obs_list),\r\n            \'actions\': np.array(action_list)\r\n        })\r\n\r\n    return demonstrations\n'})}),(0,i.jsx)(e.h3,{id:"dagger-\u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645",children:"DAgger \u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645"}),(0,i.jsx)(e.p,{children:"DAgger (\u0688\u06cc\u0679\u0627 \u0633\u06cc\u0679 \u0627\u06cc\u06af\u0631\u06cc\u06af\u06cc\u0634\u0646) \u0628\u0631\u06c1\u06cc\u0648\u06cc\u0631 \u06a9\u0644\u0648\u0646\u0646\u06af \u0645\u06cc\u06ba \u06a9\u0648\u0648\u0627\u0631\u06cc\u0626\u0679 \u0634\u0641\u0679 \u06a9\u06d2 \u0645\u0633\u0626\u0644\u06c1 \u06a9\u0648 \u062d\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u062c\u0648 \u0633\u06cc\u06a9\u06be\u06cc \u06af\u0626\u06cc \u067e\u0627\u0644\u06cc\u0633\u06cc \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u0645\u0644\u0627\u062d\u0638\u06c1 \u06a9\u0631\u062f\u06c1 \u0627\u0633\u0679\u06cc\u0679\u0633 \u067e\u0631 \u0645\u0627\u06c1\u0631 \u0633\u06d2 \u0688\u06cc\u0679\u0627 \u062c\u0645\u0639 \u06a9\u0631 \u06a9\u06d2."}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644: DAgger \u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645 \u06a9\u0627 \u0646\u0641\u0627\u0630\r\nimport numpy as np\r\n\r\nclass DAgger:\r\n    def __init__(self, bc_model, expert_policy):\r\n        self.bc_model = bc_model\r\n        self.expert_policy = expert_policy\r\n        self.all_obs = []\r\n        self.all_actions = []\r\n\r\n    def train_iteration(self, robot_env, policy, num_rollouts=10):\r\n        """\r\n        DAgger \u06a9\u0627 \u0627\u06cc\u06a9 \u0627\u06cc\u0679\u0631\u06cc\u0634\u0646 \u06a9\u0631\u06cc\u06ba\r\n        """\r\n        new_obs_list = []\r\n        new_action_list = []\r\n\r\n        for rollout in range(num_rollouts):\r\n            obs = robot_env.reset()\r\n            done = False\r\n\r\n            while not done:\r\n                # \u0645\u0627\u06c1\u0631 \u06a9\u0648 \u0633\u06cc\u06a9\u06be\u06cc \u06af\u0626\u06cc \u067e\u0627\u0644\u06cc\u0633\u06cc \u06a9\u06cc \u0627\u0633\u0679\u06cc\u0679 \u067e\u0631 \u0627\u06cc\u06a9\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u06c1\u06cc\u06ba\r\n                expert_action = self.expert_policy(obs)\r\n\r\n                # \u0627\u0633\u0679\u06cc\u0679 \u0627\u06cc\u06a9\u0634\u0646 \u062c\u0648\u0691\u06cc \u062c\u0645\u0639 \u06a9\u0631\u06cc\u06ba\r\n                new_obs_list.append(obs.copy())\r\n                new_action_list.append(expert_action.copy())\r\n\r\n                # \u0633\u06cc\u06a9\u06be\u06cc \u06af\u0626\u06cc \u067e\u0627\u0644\u06cc\u0633\u06cc \u0627\u06cc\u06a9\u0634\u0646 \u0627\u0646\u062c\u0627\u0645 \u062f\u06cc\u06ba \u062a\u0627\u06a9\u06c1 \u0646\u0626\u06cc \u0627\u0633\u0679\u06cc\u0679 \u0645\u0644\u0627\u062d\u0638\u06c1 \u06a9\u06cc \u062c\u0627 \u0633\u06a9\u06d2\r\n                with torch.no_grad():\r\n                    policy_action = policy.network(torch.FloatTensor(obs)).numpy()\r\n\r\n                obs, reward, done, info = robot_env.step(policy_action)\r\n\r\n        # \u0646\u06cc\u0627 \u0688\u06cc\u0679\u0627 \u067e\u0631\u0627\u0646\u06d2 \u0688\u06cc\u0679\u0627 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0627\u06cc\u06af\u0631\u06cc\u06af\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        self.all_obs.extend(new_obs_list)\r\n        self.all_actions.extend(new_action_list)\r\n\r\n        # \u0627\u06cc\u06af\u0631\u06cc\u06af\u06cc\u0679\u0688 \u0688\u06cc\u0679\u0627 \u0633\u06cc\u0679 \u067e\u0631 \u0628\u0631\u062a\u0627\u0624 \u06a9\u0644\u0648\u0646\u0646\u06af \u0645\u0627\u0688\u0644 \u062a\u0631\u0628\u06cc\u062a \u062f\u06cc\u06ba\r\n        if len(self.all_obs) > 0:\r\n            obs_tensor = torch.FloatTensor(self.all_obs)\r\n            action_tensor = torch.FloatTensor(self.all_actions)\r\n\r\n            # \u0645\u062a\u0639\u062f\u062f \u0627\u06cc\u067e\u0648\u06a9\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0631\u0628\u06cc\u062a \u062f\u06cc\u06ba\r\n            for epoch in range(10):\r\n                indices = np.random.permutation(len(self.all_obs))\r\n                for i in range(0, len(indices), 32):  # \u0628\u06cc\u0686 \u0633\u0627\u0626\u0632 32\r\n                    batch_indices = indices[i:i+32]\r\n                    batch_obs = obs_tensor[batch_indices]\r\n                    batch_actions = action_tensor[batch_indices]\r\n\r\n                    self.bc_model.train_step(batch_obs, batch_actions)\n'})}),(0,i.jsx)(e.h2,{id:"\u0645\u0648\u0627\u0641\u0642-\u06a9\u0646\u0679\u0631\u0648\u0644-\u0633\u0633\u0679\u0645\u0632",children:"\u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0633\u0633\u0679\u0645\u0632"}),(0,i.jsx)(e.p,{children:"\u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0633\u0633\u0679\u0645\u0632 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u0627\u0635\u0644 \u0648\u0642\u062a \u0645\u06cc\u06ba \u0627\u067e\u0646\u06d2 \u06a9\u0646\u0679\u0631\u0648\u0644 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u06a9\u0648 \u062a\u0628\u062f\u06cc\u0644 \u06c1\u0648\u062a\u06d2 \u0645\u0627\u062d\u0648\u0644 \u06cc\u0627 \u0633\u0633\u0679\u0645 \u0688\u06cc\u0646\u0627\u0645\u06a9\u0633 \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0688\u06be\u0627\u0644\u0646\u06d2 \u06a9\u06cc \u0627\u062c\u0627\u0632\u062a \u062f\u06cc\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06cc\u06c1 \u06c1\u06cc\u0648\u0645\u0646\u0648\u0627\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06c1\u0645 \u06c1\u06d2 \u062c\u0648 \u0645\u062a\u062d\u0631\u06a9 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u06a9\u0627\u0645 \u06a9\u0631 \u0631\u06c1\u06d2 \u06c1\u0648\u06ba\u06d4"}),(0,i.jsx)(e.h3,{id:"\u0645\u0627\u0688\u0644-\u0631\u06cc\u0641\u0631\u0646\u0633-\u0645\u0648\u0627\u0641\u0642-\u06a9\u0646\u0679\u0631\u0648\u0644-mrac",children:"\u0645\u0627\u0688\u0644 \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 (MRAC)"}),(0,i.jsx)(e.p,{children:"MRAC \u06a9\u0646\u0679\u0631\u0648\u0644\u0631 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u06a9\u0648 \u0627\u0633 \u0637\u0631\u062d \u0627\u06cc\u0688\u062c\u0633\u0679 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u067e\u0644\u0627\u0646\u0679 \u0645\u0637\u0644\u0648\u0628\u06c1 \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0627\u0688\u0644 \u06a9\u06cc \u0637\u0631\u062d \u0628\u0631\u062a\u0627\u0624 \u06a9\u0631\u06d2\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644: \u0631\u0648\u0628\u0648\u0679\u06a9 \u062c\u0648\u0627\u0626\u0646\u0679 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0627\u0688\u0644 \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644\r\nimport numpy as np\r\n\r\nclass MRACController:\r\n    def __init__(self, reference_model_params, initial_controller_params):\r\n        # \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0627\u0688\u0644 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632\r\n        self.ref_A = reference_model_params['A']\r\n        self.ref_B = reference_model_params['B']\r\n        self.ref_C = reference_model_params['C']\r\n\r\n        # \u06a9\u0646\u0679\u0631\u0648\u0644\u0631 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 (\u0634\u0631\u0648\u0639 \u0645\u06cc\u06ba \u0646\u0627\u0645\u0639\u0644\u0648\u0645\u060c \u0622\u0646 \u0644\u0627\u0626\u0646 \u0627\u06cc\u0688\u062c\u0633\u0679 \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba)\r\n        self.theta_m = initial_controller_params['theta_m']  # \u0645\u0627\u0688\u0644 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632\r\n        self.theta_r = initial_controller_params['theta_r']  # \u0631\u06cc\u0641\u0631\u0646\u0633 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632\r\n\r\n        # \u0627\u06cc\u0688\u0627\u067e\u0679\u06cc\u0634\u0646 \u06af\u06cc\u0646\u0632\r\n        self.gamma_m = 0.1  # \u0645\u0627\u0688\u0644 \u0627\u06cc\u0688\u0627\u067e\u0679\u06cc\u0634\u0646 \u06af\u06cc\u0646\r\n        self.gamma_r = 0.1  # \u0631\u06cc\u0641\u0631\u0646\u0633 \u0627\u06cc\u0688\u0627\u067e\u0679\u06cc\u0634\u0646 \u06af\u06cc\u0646\r\n\r\n        # \u0627\u0633\u0679\u06cc\u0679 \u0645\u062a\u063a\u06cc\u0631\u0627\u062a\r\n        self.state = np.zeros(len(self.ref_A))\r\n        self.ref_state = np.zeros(len(self.ref_A))\r\n\r\n    def update(self, y, r, dt):\r\n        \"\"\"\r\n        \u0679\u0631\u06cc\u06a9\u0646\u06af \u063a\u0644\u0637\u06cc \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u06a9\u0646\u0679\u0631\u0648\u0644\u0631 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        y: \u0627\u0635\u0644 \u0622\u0624\u0679 \u067e\u0679\r\n        r: \u0631\u06cc\u0641\u0631\u0646\u0633 \u0627\u0646 \u067e\u0679\r\n        dt: \u0679\u0627\u0626\u0645 \u0633\u0679\u06cc\u067e\r\n        \"\"\"\r\n        # \u0679\u0631\u06cc\u06a9\u0646\u06af \u063a\u0644\u0637\u06cc \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        e = y - r\r\n\r\n        # \u0631\u06cc\u0641\u0631\u0646\u0633 \u0645\u0627\u0688\u0644 \u0627\u0633\u0679\u06cc\u0679 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        self.ref_state += dt * (self.ref_A @ self.ref_state + self.ref_B * r)\r\n\r\n        # \u06a9\u0646\u0679\u0631\u0648\u0644\u0631 \u0627\u0633\u0679\u06cc\u0679 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        u = -self.theta_m @ self.state + self.theta_r * r\r\n        self.state += dt * (self.ref_A @ self.state + self.ref_B * u)\r\n\r\n        # \u063a\u0644\u0637\u06cc \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0627\u06cc\u0688\u062c\u0633\u0679 \u06a9\u0631\u06cc\u06ba\r\n        self.theta_m += -self.gamma_m * e * self.state\r\n        self.theta_r += self.gamma_r * e * r\r\n\r\n        return u\r\n\r\n# \u0645\u062b\u0627\u0644: \u0631\u0648\u0628\u0648\u0679\u06a9 \u0645\u06cc\u0646\u06cc\u067e\u0648\u0644\u06cc\u0679\u0631 \u062c\u0648\u0627\u0626\u0646\u0679 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644\r\nclass AdaptiveJointController:\r\n    def __init__(self, joint_id, initial_mass=1.0, initial_damping=0.1):\r\n        self.joint_id = joint_id\r\n        self.mass = initial_mass\r\n        self.damping = initial_damping\r\n\r\n        # \u0645\u0648\u0627\u0641\u0642 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632\r\n        self.param_history = {'mass': [], 'damping': []}\r\n\r\n    def update_model(self, torque, acceleration, velocity, position, dt):\r\n        \"\"\"\r\n        \u0645\u0634\u0627\u06c1\u062f\u06c1 \u0688\u06cc\u0646\u0627\u0645\u06a9\u0633 \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u0645\u0627\u0688\u0644 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        \"\"\"\r\n        # \u0641\u0632\u06a9\u0633 \u0628\u06cc\u0633\u0688 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631 \u0627\u0633\u0679\u06cc\u0645\u06cc\u0634\u0646\r\n        # tau = m*q_ddot + b*q_dot + k*q (\u0633\u0627\u062f\u06c1)\r\n        estimated_mass = torque / (acceleration + 1e-6)  # \u062a\u0642\u0633\u06cc\u0645 \u0628\u0631 \u0635\u0641\u0631 \u0633\u06d2 \u0628\u0686\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0686\u06be\u0648\u0679\u06cc \u0642\u06cc\u0645\u062a \u0634\u0627\u0645\u0644 \u06a9\u0631\u06cc\u06ba\r\n        estimated_damping = torque / (velocity + 1e-6)\r\n\r\n        # \u06a9\u0645 \u06af\u0632\u0631 \u06a9\u06d2 \u0641\u0644\u0679\u0631 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        alpha = 0.01  # \u0627\u06cc\u0688\u0627\u067e\u0679\u06cc\u0634\u0646 \u0631\u06cc\u0679\r\n        self.mass = (1 - alpha) * self.mass + alpha * estimated_mass\r\n        self.damping = (1 - alpha) * self.damping + alpha * estimated_damping\r\n\r\n        # \u062a\u062c\u0632\u06cc\u06c1 \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0627\u0631\u06cc\u062e \u0627\u0633\u0679\u0648\u0631 \u06a9\u0631\u06cc\u06ba\r\n        self.param_history['mass'].append(self.mass)\r\n        self.param_history['damping'].append(self.damping)\r\n\r\n    def compute_control(self, desired_pos, desired_vel, current_pos, current_vel, dt):\r\n        \"\"\"\r\n        \u0645\u0648\u0627\u0641\u0642 \u06a9\u0646\u0679\u0631\u0648\u0644 \u0633\u06af\u0646\u0644 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        \"\"\"\r\n        # PD \u06a9\u0646\u0679\u0631\u0648\u0644 \u0645\u0648\u0627\u0641\u0642 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u06a9\u06d2 \u0633\u0627\u062a\u06be\r\n        pos_error = desired_pos - current_pos\r\n        vel_error = desired_vel - current_vel\r\n\r\n        # \u06a9\u0646\u0679\u0631\u0648\u0644 \u0627\u06cc\u0641\u0648\u0631\u0679 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        proportional_term = self.mass * pos_error\r\n        derivative_term = self.damping * vel_error\r\n\r\n        control_signal = proportional_term + derivative_term\r\n\r\n        return control_signal\n"})}),(0,i.jsx)(e.h3,{id:"\u0645\u0648\u0627\u0641\u0642-\u0637\u0631\u0632-\u0639\u0645\u0644-\u06a9\u06d2-\u0644\u06cc\u06d2-\u0633\u06cc\u0644\u0641-\u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af-\u0645\u06cc\u067e\u0633",children:"\u0645\u0648\u0627\u0641\u0642 \u0637\u0631\u0632 \u0639\u0645\u0644 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0633\u06cc\u0644\u0641 \u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af \u0645\u06cc\u067e\u0633"}),(0,i.jsx)(e.p,{children:"\u0633\u06cc\u0644\u0641 \u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af \u0645\u06cc\u067e\u0633 (SOMs) \u0645\u0648\u0627\u0641\u0642 \u0637\u0631\u0632 \u0639\u0645\u0644 \u0644\u0631\u0646\u0646\u06af \u0627\u0648\u0631 \u0645\u0645\u0627\u062b\u0644 \u062d\u0631\u06a9\u062a \u06a9\u06d2 \u0646\u0645\u0648\u0646\u0648\u06ba \u06a9\u06d2 \u06a9\u0644\u0633\u0679\u0631\u0646\u06af \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u06cc\u06d2 \u062c\u0627 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644: \u062d\u0631\u06a9\u062a \u06a9\u06d2 \u0646\u0645\u0648\u0646\u06d2 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0633\u06cc\u0644\u0641 \u0622\u0631\u06af\u0646\u0627\u0626\u0632\u0646\u06af \u0645\u06cc\u067e\r\nimport numpy as np\r\n\r\nclass SOMRobotLearning:\r\n    def __init__(self, grid_width, grid_height, input_dim, learning_rate=0.1):\r\n        self.grid_width = grid_width\r\n        self.grid_height = grid_height\r\n        self.input_dim = input_dim\r\n        self.learning_rate = learning_rate\r\n\r\n        # \u0648\u0632\u0646 \u0648\u06cc\u06a9\u0679\u0631\u0632 \u06a9\u0648 \u0628\u06d2 \u062a\u0631\u062a\u06cc\u0628 \u0637\u0648\u0631 \u067e\u0631 \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba\r\n        self.weights = np.random.randn(grid_width, grid_height, input_dim)\r\n\r\n        # \u06c1\u0645\u0633\u0627\u06cc\u06c1 \u0641\u0646\u06a9\u0634\u0646 \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632\r\n        self.sigma = max(grid_width, grid_height) / 2.0  # \u0627\u0628\u062a\u062f\u0627\u0626\u06cc \u06c1\u0645\u0633\u0627\u06cc\u06c1 \u0631\u062f\u0627\u0633\r\n        self.lambda_decay = 1000  # \u0688\u06cc\u06a9\u06d2 \u0645\u0633\u062a\u0642\u0644\r\n\r\n    def find_bmu(self, input_vector):\r\n        """\r\n        \u0627\u0646 \u067e\u0679 \u0648\u06cc\u06a9\u0679\u0631 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0628\u06cc\u0633\u0679 \u0645\u06cc\u0686\u0646\u06af \u06cc\u0648\u0646\u0679 (BMU) \u062a\u0644\u0627\u0634 \u06a9\u0631\u06cc\u06ba\r\n        """\r\n        distances = np.linalg.norm(self.weights - input_vector, axis=2)\r\n        bmu_x, bmu_y = np.unravel_index(np.argmin(distances), distances.shape)\r\n        return bmu_x, bmu_y\r\n\r\n    def update_weights(self, input_vector, bmu_x, bmu_y, iteration):\r\n        """\r\n        BMU \u06a9\u06d2 \u06c1\u0645\u0633\u0627\u06cc\u06c1 \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 \u0648\u0632\u0646 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        """\r\n        # \u0648\u0642\u062a \u06a9\u06d2 \u0633\u0627\u062a\u06be \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631\u0632 \u0688\u06cc\u06a9 \u06a9\u0631\u06cc\u06ba\r\n        current_lr = self.learning_rate * np.exp(-iteration / self.lambda_decay)\r\n        current_sigma = self.sigma * np.exp(-iteration / self.lambda_decay)\r\n\r\n        # \u06a9\u0648\u0622\u0631\u0688\u06cc\u0646\u06cc\u0679 \u06af\u0631\u0688 \u0628\u0646\u0627\u0626\u06cc\u06ba\r\n        xx, yy = np.meshgrid(range(self.grid_width), range(self.grid_height))\r\n\r\n        # \u06af\u0631\u0688 \u0633\u067e\u06cc\u0633 \u0645\u06cc\u06ba BMU \u0633\u06d2 \u0641\u0627\u0635\u0644\u06c1 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        grid_distances = (xx - bmu_x)**2 + (yy - bmu_y)**2\r\n\r\n        # \u06c1\u0645\u0633\u0627\u06cc\u06c1 \u06a9\u06d2 \u0627\u062b\u0631 \u06a9\u0627 \u062d\u0633\u0627\u0628 \u0644\u06af\u0627\u0626\u06cc\u06ba\r\n        neighborhood = np.exp(-grid_distances / (2 * current_sigma**2))\r\n\r\n        # \u0648\u0632\u0646 \u0627\u067e \u0688\u06cc\u0679 \u06a9\u0631\u06cc\u06ba\r\n        self.weights += current_lr * neighborhood[:, :, np.newaxis] * \\\r\n                       (input_vector - self.weights)\r\n\r\n    def learn_pattern(self, input_sequence, iterations=1000):\r\n        """\r\n        \u0627\u0646 \u067e\u0679 \u067e\u06cc\u0679\u0631\u0646\u0632 \u06a9\u06cc \u062a\u0631\u062a\u06cc\u0628 \u0633\u06cc\u06a9\u06be\u06cc\u06ba\r\n        """\r\n        for i in range(iterations):\r\n            idx = np.random.randint(0, len(input_sequence))\r\n            input_vector = input_sequence[idx]\r\n\r\n            bmu_x, bmu_y = self.find_bmu(input_vector)\r\n            self.update_weights(input_vector, bmu_x, bmu_y, i)\r\n\r\n    def recall_pattern(self, input_vector):\r\n        """\r\n        \u0627\u0646 \u067e\u0679 \u0648\u06cc\u06a9\u0679\u0631 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0633\u06cc\u06a9\u06be\u0627 \u06c1\u0648\u0627 \u067e\u06cc\u0679\u0631\u0646 \u06cc\u0627\u062f \u06a9\u0631\u06cc\u06ba\r\n        """\r\n        bmu_x, bmu_y = self.find_bmu(input_vector)\r\n        return self.weights[bmu_x, bmu_y]\r\n\r\n# \u0686\u0644\u0646\u06d2 \u06a9\u06d2 \u067e\u06cc\u0679\u0631\u0646 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u062b\u0627\u0644 \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644\r\ndef learn_walking_patterns():\r\n    # \u0686\u0644\u0646\u06d2 \u06a9\u06d2 \u0645\u0631\u062d\u0644\u06d2 \u06a9\u06cc \u062e\u0635\u0648\u0635\u06cc\u0627\u062a \u06a9\u06cc \u0648\u0636\u0627\u062d\u062a \u06a9\u0631\u06cc\u06ba (\u0645\u062b\u0644\u0627\u064b \u062c\u0648\u0627\u0626\u0646\u0679 \u0627\u06cc\u0646\u06af\u0644\u0632\u060c \u06af\u0631\u0627\u0624\u0646\u0688 \u0631\u06cc \u0627\u06cc\u06a9\u0634\u0646 \u0641\u0648\u0631\u0633\u0632)\r\n    som = SOMRobotLearning(grid_width=10, grid_height=10, input_dim=12)\r\n\r\n    # \u0686\u0644\u0646\u06d2 \u06a9\u0627 \u067e\u06cc\u0679\u0631\u0646 \u062a\u0631\u062a\u06cc\u0628 \u06a9\u0627 \u0634\u0628\u06cc\u06c1 \u0633\u0627\u0632\u06cc \u06a9\u0631\u06cc\u06ba\r\n    walking_patterns = []\r\n    for phase in range(100):  # 100 \u0686\u0644\u0646\u06d2 \u06a9\u06d2 \u0645\u0631\u0627\u062d\u0644\r\n        # \u0645\u0635\u0646\u0648\u0639\u06cc \u0686\u0644\u0646\u06d2 \u06a9\u06cc \u062e\u0635\u0648\u0635\u06cc\u0627\u062a \u067e\u06cc\u062f\u0627 \u06a9\u0631\u06cc\u06ba\r\n        features = np.random.randn(12)  # 12 \u062c\u0648\u0627\u0626\u0646\u0679 \u0627\u06cc\u0646\u06af\u0644\u0632 + 4 GRF \u0648\u06cc\u0644\u06cc\u0648\u0632\r\n        walking_patterns.append(features)\r\n\r\n    # \u0686\u0644\u0646\u06d2 \u06a9\u06d2 \u067e\u06cc\u0679\u0631\u0646 \u0633\u06cc\u06a9\u06be\u06cc\u06ba\r\n    som.learn_pattern(walking_patterns, iterations=5000)\r\n\r\n    return som\n'})}),(0,i.jsx)(e.h2,{id:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633-\u0645\u06cc\u06ba-\u0679\u0631\u0627\u0646\u0633\u0641\u0631-\u0644\u0631\u0646\u0646\u06af",children:"\u0631\u0648\u0628\u0648\u0679\u06a9\u0633 \u0645\u06cc\u06ba \u0679\u0631\u0627\u0646\u0633\u0641\u0631 \u0644\u0631\u0646\u0646\u06af"}),(0,i.jsx)(e.p,{children:"\u0679\u0631\u0627\u0646\u0633\u0641\u0631 \u0644\u0631\u0646\u0646\u06af \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u0627\u06cc\u06a9 \u0679\u0627\u0633\u06a9 \u06cc\u0627 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u062d\u0627\u0635\u0644 \u06a9\u0631\u062f\u06c1 \u0639\u0644\u0645 \u06a9\u0648 \u0645\u062a\u0639\u0644\u0642\u06c1 \u0679\u0627\u0633\u06a9 \u06cc\u0627 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u0648 \u0628\u06c1\u062a\u0631 \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06cc \u0627\u062c\u0627\u0632\u062a \u062f\u06cc\u062a\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u062a\u0631\u0628\u06cc\u062a \u06a9\u0627 \u0648\u0642\u062a \u06a9\u0645 \u06a9\u0631\u0646\u06d2 \u0627\u0648\u0631 \u0646\u0645\u0648\u0646\u06c1 \u06a9\u06cc \u06a9\u0627\u0631\u06a9\u0631\u062f\u06af\u06cc \u06a9\u0648 \u0628\u06c1\u062a\u0631 \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u062e\u0627\u0635 \u0637\u0648\u0631 \u067e\u0631 \u0642\u06cc\u0645\u062a\u06cc \u06c1\u06d2\u06d4"}),(0,i.jsx)(e.h3,{id:"\u0688\u0648\u0645\u06cc\u0646-\u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646",children:"\u0688\u0648\u0645\u06cc\u0646 \u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646"}),(0,i.jsx)(e.p,{children:"\u0688\u0648\u0645\u06cc\u0646 \u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646 \u06a9\u06cc \u062a\u06a9\u0646\u06cc\u06a9\u06cc\u06ba \u0645\u062e\u062a\u0644\u0641 \u0645\u0627\u062d\u0648\u0644 \u06cc\u0627 \u0631\u0648\u0628\u0648\u0679 \u06a9\u0646\u0641\u06cc\u06af\u0631\u06cc\u0634\u0646\u0632 \u067e\u0631 \u0633\u06cc\u06a9\u06be\u06d2 \u06af\u0626\u06d2 \u067e\u0627\u0644\u06cc\u0633\u06cc\u0632 \u0645\u0646\u062a\u0642\u0644 \u06a9\u0631\u0646\u06d2 \u0645\u06cc\u06ba \u0645\u062f\u062f \u06a9\u0631\u062a\u06cc \u06c1\u06cc\u06ba\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644: \u0645\u062e\u062a\u0644\u0641 \u0631\u0648\u0628\u0648\u0679 \u0645\u0648\u0631\u0641\u0648\u0644\u0648\u062c\u06cc\u0632 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0688\u0648\u0645\u06cc\u0646 \u0627\u0688\u0627\u067e\u0679\u06cc\u0634\u0646\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass DomainAdaptationNet(nn.Module):\r\n    def __init__(self, shared_dim=256, source_dim=10, target_dim=12):\r\n        super(DomainAdaptationNet, self).__init__()\r\n\r\n        # \u0645\u0634\u062a\u0631\u06a9\u06c1 \u0641\u06cc\u0686\u0631 \u0627\u06cc\u06a9\u0633\u0679\u0631\u06cc\u06a9\u0679\u0631\r\n        self.shared_encoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, shared_dim),\r\n            nn.ReLU()\r\n        )\r\n\r\n        # \u0688\u0648\u0645\u06cc\u0646 \u0645\u062e\u0635\u0648\u0635 \u0688\u06cc\u06a9\u0648\u0688\u0631\u0632\r\n        self.source_decoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, source_dim)\r\n        )\r\n\r\n        self.target_decoder = nn.Sequential(\r\n            nn.Linear(shared_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, target_dim)\r\n        )\r\n\r\n        # \u0627\u06cc\u0688\u0648\u0631\u0633\u0631\u06cc\u0644 \u0679\u0631\u06cc\u0646\u0646\u06af \u06a9\u06d2 \u0644\u06cc\u06d2 \u0688\u0648\u0645\u06cc\u0646 \u06a9\u0644\u0627\u0633\u06cc\u0641\u0627\u0626\u0631\r\n        self.domain_classifier = nn.Sequential(\r\n            nn.Linear(shared_dim, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 2)\r\n        )\r\n\r\n    def encode(self, x):\r\n        return self.shared_encoder(x)\r\n\r\n    def decode_source(self, z):\r\n        return self.source_decoder(z)\r\n\r\n    def decode_target(self, z):\r\n        return self.target_decoder(z)\r\n\r\n    def classify_domain(self, z):\r\n        return self.domain_classifier(z)\r\n\r\nclass DomainAdaptationTrainer:\r\n    def __init__(self, model, lambda_adv=0.1):\r\n        self.model = model\r\n        self.lambda_adv = lambda_adv\r\n        self.recon_criterion = nn.MSELoss()\r\n        self.domain_criterion = nn.CrossEntropyLoss()\r\n\r\n    def train_step(self, source_data, target_data):\r\n        # \u062f\u0648\u0646\u0648\u06ba \u0688\u0648\u0645\u06cc\u0646\u0632 \u0627\u0646\u06a9\u0648\u0688 \u06a9\u0631\u06cc\u06ba\r\n        source_encoded = self.model.encode(source_data)\r\n        target_encoded = self.model.encode(target_data)\r\n\r\n        # \u0627\u0635\u0644 \u0688\u06cc\u0679\u0627 \u0631\u06cc\u06a9\u0646\u0633\u0679\u0631\u06a9\u0679 \u06a9\u0631\u06cc\u06ba\r\n        source_recon = self.model.decode_source(source_encoded)\r\n        target_recon = self.model.decode_target(target_encoded)\r\n\r\n        # \u0631\u06cc\u06a9\u0646\u0633\u0679\u0631\u06a9\u0634\u0646 \u0646\u0642\u0635\u0627\u0646\u0627\u062a\r\n        source_recon_loss = self.recon_criterion(source_recon, source_data)\r\n        target_recon_loss = self.recon_criterion(target_recon, target_data)\r\n\r\n        # \u0688\u0648\u0645\u06cc\u0646 \u06a9\u0644\u0627\u0633\u06cc\u0641\u06a9\u06cc\u0634\u0646\r\n        source_domains = torch.zeros(source_encoded.size(0)).long()\r\n        target_domains = torch.ones(target_encoded.size(0)).long()\r\n\r\n        all_encoded = torch.cat([source_encoded, target_encoded], dim=0)\r\n        all_domains = torch.cat([source_domains, target_domains], dim=0)\r\n\r\n        domain_preds = self.model.classify_domain(all_encoded)\r\n        domain_loss = self.domain_criterion(domain_preds, all_domains)\r\n\r\n        # \u0627\u06cc\u0688\u0648\u0631\u0633\u0631\u06cc\u0644 \u0646\u0642\u0635\u0627\u0646 (\u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679\u0633 \u0631\u06cc\u0648\u0631\u0633 \u06a9\u0631\u06cc\u06ba)\r\n        # \u0639\u0645\u0644 \u0645\u06cc\u06ba\u060c \u0622\u067e \u06af\u0631\u06cc\u0688\u06cc\u0626\u0646\u0679 \u0631\u06cc\u0648\u0631\u0633\u0627\u0644 \u0644\u06cc\u0626\u0631 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba \u06af\u06d2\r\n        total_loss = source_recon_loss + target_recon_loss - self.lambda_adv * domain_loss\r\n\r\n        return total_loss\n"})}),(0,i.jsx)(e.h2,{id:"\u0633\u06cc\u0641\u0679\u06cc-\u0644\u0631\u0646\u0646\u06af-\u0633\u0633\u0679\u0645\u0632-\u0645\u06cc\u06ba",children:"\u0633\u06cc\u0641\u0679\u06cc \u0644\u0631\u0646\u0646\u06af \u0633\u0633\u0679\u0645\u0632 \u0645\u06cc\u06ba"}),(0,i.jsx)(e.p,{children:"\u0633\u06cc\u0641\u0679\u06cc \u06a9\u0627 \u062e\u06cc\u0627\u0644 \u062c\u0633\u0645\u0627\u0646\u06cc \u0631\u0648\u0628\u0648\u0679\u0633 \u067e\u0631 \u0644\u0631\u0646\u0646\u06af \u0627\u0644\u06af\u0648\u0631\u062a\u06be\u0645\u0632 \u06a9\u0648 \u0627\u062a\u0627\u0631\u0646\u06d2 \u0645\u06cc\u06ba \u0627\u0646\u062a\u06c1\u0627\u0626\u06cc \u0627\u06c1\u0645 \u06c1\u06d2\u06d4 \u0645\u062d\u0641\u0648\u0638 \u0644\u0631\u0646\u0646\u06af \u06cc\u06c1 \u06cc\u0642\u06cc\u0646\u06cc \u0628\u0646\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u0631\u0648\u0628\u0648\u0679 \u0644\u0631\u0646\u0646\u06af \u06a9\u06d2 \u0639\u0645\u0644 \u06a9\u06d2 \u062f\u0648\u0631\u0627\u0646 \u0627\u067e\u0646\u06d2 \u0622\u067e \u06a9\u0648 \u06cc\u0627 \u0627\u067e\u0646\u06d2 \u0645\u0627\u062d\u0648\u0644 \u06a9\u0648 \u0646\u0642\u0635\u0627\u0646 \u0646\u06c1\u06cc\u06ba \u067e\u06c1\u0646\u0686\u0627\u062a\u06d2\u06d4"}),(0,i.jsx)(e.h3,{id:"\u0645\u062d\u0641\u0648\u0638-\u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646-\u0627\u0633\u0679\u0631\u06cc\u0679\u06cc\u062c\u0632",children:"\u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646 \u0627\u0633\u0679\u0631\u06cc\u0679\u06cc\u062c\u0632"}),(0,i.jsx)(e.p,{children:"\u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646 \u0645\u0627\u062d\u0648\u0644 \u0645\u06cc\u06ba \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631 \u06a9\u0631\u0646\u06d2 \u06a9\u06cc \u0636\u0631\u0648\u0631\u062a \u06a9\u0648 \u0645\u062d\u0641\u0648\u0638 \u0631\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0645\u062a\u0648\u0627\u0632\u0646 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u062a\u0627\u06a9\u06c1 \u062e\u0637\u0631\u0646\u0627\u06a9 \u0627\u0633\u0679\u06cc\u0679\u0633 \u0633\u06d2 \u0628\u0686\u0627 \u062c\u0627 \u0633\u06a9\u06d2\u06d4"}),(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644: \u0645\u062d\u062f\u0648\u062f\u06cc\u062a \u06a9\u06cc \u062a\u0633\u0644\u06cc \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646\r\nimport numpy as np\r\nfrom scipy.optimize import minimize\r\n\r\nclass SafeExploration:\r\n    def __init__(self, safety_constraints, exploration_rate=0.1):\r\n        self.safety_constraints = safety_constraints\r\n        self.exploration_rate = exploration_rate\r\n        self.constraint_history = []\r\n\r\n    def is_safe_action(self, state, action):\r\n        """\r\n        \u0686\u06cc\u06a9 \u06a9\u0631\u06cc\u06ba \u06a9\u06c1 \u06a9\u06cc\u0627 \u0627\u06cc\u06a9\u0634\u0646 \u062f\u06cc \u06af\u0626\u06cc \u0627\u0633\u0679\u06cc\u0679 \u0645\u06cc\u06ba \u0645\u062d\u0641\u0648\u0638 \u06c1\u06d2\r\n        """\r\n        # \u0627\u06cc\u06a9\u0634\u0646 \u0644\u06af\u0627\u0626\u06cc\u06ba \u0627\u0648\u0631 \u0645\u062d\u062f\u0648\u062f\u06cc\u062a\u06cc\u06ba \u0686\u06cc\u06a9 \u06a9\u0631\u06cc\u06ba\r\n        next_state = self.simulate_transition(state, action)\r\n\r\n        for constraint in self.safety_constraints:\r\n            if not constraint(next_state):\r\n                return False\r\n        return True\r\n\r\n    def safe_exploration_policy(self, state, q_values):\r\n        """\r\n        Q-\u0648\u06cc\u0644\u06cc\u0648\u0632 \u06a9\u0648 \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631\u06cc\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u06cc\u06ba\r\n        """\r\n        safe_actions = []\r\n        for action in range(len(q_values)):\r\n            if self.is_safe_action(state, action):\r\n                safe_actions.append(action)\r\n\r\n        if not safe_actions:\r\n            # \u06a9\u0648\u0626\u06cc \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0634\u0646 \u062f\u0633\u062a\u06cc\u0627\u0628 \u0646\u06c1\u06cc\u06ba\u060c \u0628\u06d2 \u062a\u0631\u062a\u06cc\u0628 \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0634\u0646 \u06cc\u0627 \u0646\u0644 \u0644\u0648\u0679\u0627\u0626\u06cc\u06ba\r\n            return None\r\n\r\n        # \u0635\u0631\u0641 \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0634\u0646\u0632 \u067e\u0631 \u0627\u06cc\u0628\u0633\u06cc\u0644\u0648\u0646 \u06af\u0631\u06cc\u0688\u06cc \u06a9\u0627 \u0627\u0637\u0644\u0627\u0642\r\n        if np.random.rand() < self.exploration_rate:\r\n            # \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0631: \u0628\u06d2 \u062a\u0631\u062a\u06cc\u0628 \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0634\u0646 \u0645\u0646\u062a\u062e\u0628 \u06a9\u0631\u06cc\u06ba\r\n            chosen_action = np.random.choice(safe_actions)\r\n        else:\r\n            # \u0627\u06cc\u06a9\u0633\u067e\u0644\u0648\u0679: \u0628\u06c1\u062a\u0631\u06cc\u0646 \u0645\u062d\u0641\u0648\u0638 \u0627\u06cc\u06a9\u0634\u0646 \u0645\u0646\u062a\u062e\u0628 \u06a9\u0631\u06cc\u06ba\r\n            safe_q_values = q_values[safe_actions]\r\n            best_safe_idx = np.argmax(safe_q_values)\r\n            chosen_action = safe_actions[best_safe_idx]\r\n\r\n        return chosen_action\r\n\r\n    def simulate_transition(self, state, action):\r\n        """\r\n        \u0627\u0633\u0679\u06cc\u0679 \u0679\u0631\u0627\u0646\u0632\u06cc\u0634\u0646 \u06a9\u06cc \u0634\u0628\u06cc\u06c1 \u0633\u0627\u0632\u06cc \u06a9\u0631\u06cc\u06ba (\u0633\u0627\u062f\u06c1 \u0645\u0627\u0688\u0644)\r\n        """\r\n        # \u0641\u0632\u06a9\u0633 \u06a9\u06cc \u0634\u0628\u06cc\u06c1 \u0633\u0627\u0632\u06cc \u06a9\u06d2 \u0644\u06cc\u06d2 \u062c\u06af\u06c1\r\n        next_state = state + 0.1 * action  # \u0633\u0627\u062f\u06c1 \u0688\u06cc\u0646\u0627\u0645\u06a9\u0633\r\n        return next_state\r\n\r\n# \u0645\u062b\u0627\u0644 \u0633\u06cc\u0641\u0679\u06cc \u0645\u062d\u062f\u0648\u062f\u06cc\u062a\u06cc\u06ba\r\ndef joint_limit_constraint(state):\r\n    """\u06cc\u0642\u06cc\u0646\u06cc \u0628\u0646\u0627\u0626\u06cc\u06ba \u06a9\u06c1 \u062c\u0648\u0627\u0626\u0646\u0679 \u067e\u0648\u0632\u06cc\u0634\u0646 \u0645\u062d\u062f\u0648\u062f\u06cc\u062a \u06a9\u06d2 \u0627\u0646\u062f\u0631 \u06c1\u06cc\u06ba"""\r\n    joint_limits = [-2.0, 2.0]  # \u0645\u062b\u0627\u0644 \u0645\u062d\u062f\u0648\u062f\u06cc\u062a\u06cc\u06ba\r\n    for joint_pos in state[:6]:  # \u067e\u06c1\u0644\u06d2 6 \u062c\u0648\u0627\u0626\u0646\u0679\u0633\r\n        if joint_pos < joint_limits[0] or joint_pos > joint_limits[1]:\r\n            return False\r\n    return True\r\n\r\ndef collision_constraint(state):\r\n    """\u0645\u0645\u06a9\u0646\u06c1 \u0679\u06a9\u0631\u0627\u0624 \u06a9\u06cc \u062c\u0627\u0646\u0686 \u06a9\u0631\u06cc\u06ba"""\r\n    # \u0633\u0627\u062f\u06c1 \u0679\u06a9\u0631\u0627\u0624 \u0686\u06cc\u06a9\r\n    obstacle_positions = [1.5, -1.0]  # \u0645\u062b\u0627\u0644 \u0679\u06a9\u0631\u0627\u0624 \u06a9\u06cc \u067e\u0648\u0632\u06cc\u0634\u0646\r\n    robot_pos = state[0]  # \u0631\u0648\u0628\u0648\u0679 \u06a9\u06cc \u067e\u0648\u0632\u06cc\u0634\u0646\r\n\r\n    for obs_pos in obstacle_positions:\r\n        if abs(robot_pos - obs_pos) < 0.5:  # \u0679\u06a9\u0631\u0627\u0624 \u06a9\u06cc \u062d\u062f\r\n            return False\r\n    return True\r\n\r\n# \u0645\u062b\u0627\u0644 \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644\r\nsafety_system = SafeExploration([joint_limit_constraint, collision_constraint])\n'})})]}),"\n",(0,i.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Learning Paradigms"}),": Understanding the different approaches to machine learning in robotics (supervised, reinforcement, imitation, unsupervised) is crucial for selecting the right technique for specific tasks."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Reinforcement Learning"}),": Deep RL enables robots to learn complex behaviors from raw sensory inputs, while policy gradient methods are ideal for continuous control tasks."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Imitation Learning"}),": Behavior cloning and DAgger algorithms allow robots to learn from expert demonstrations, which is particularly valuable when reward functions are difficult to define."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Control"}),": MRAC and other adaptive control systems enable robots to adjust their control parameters in real-time based on changing conditions."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Safety in Learning"}),": Implementing safe exploration strategies and constraint satisfaction is essential for deploying learning algorithms on physical robots."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Transfer Learning"}),": Domain adaptation techniques can accelerate learning by transferring knowledge between similar tasks or robot configurations."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Hardware Considerations"}),": Different hardware platforms (GPU, Jetson, real robots) require tailored learning approaches to optimize performance and safety."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Reinforcement Learning Implementation"}),": Implement a simple DQN agent to control a 2-DOF robotic arm to reach a target position. Compare the performance with a traditional PID controller."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Imitation Learning Challenge"}),": Create a demonstration dataset for a simple pick-and-place task and implement behavior cloning to learn the task. Evaluate the success rate compared to hand-coded controllers."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Control Simulation"}),": Design an adaptive controller for a single robotic joint and simulate its response to varying load conditions. Compare with a fixed-parameter controller."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Safety Constraint Integration"}),": Implement safety constraints in a learning algorithm to prevent a simulated robot from entering dangerous states. Test the algorithm's ability to learn while maintaining safety."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Cross-Platform Learning"}),": Develop a learning algorithm that can transfer knowledge from a simulated robot to a physical robot with different kinematic properties."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"quiz-learning--adaptation",children:"Quiz: Learning & Adaptation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is the primary advantage of using Deep Reinforcement Learning in robotics compared to traditional control methods?\r\na) Lower computational requirements\r\nb) Ability to learn from high-dimensional sensory inputs\r\nc) Faster convergence rates\r\nd) Simpler mathematical formulation"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answer: b)"})," Deep RL enables robots to learn from high-dimensional sensory inputs like camera images and joint encoders, which traditional control methods struggle with."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"In the DAgger algorithm, what problem does iterative data collection address?\r\na) Slow convergence of policy optimization\r\nb) Covariate shift between expert and learned policy states\r\nc) Computational complexity of neural networks\r\nd) Lack of sufficient demonstration data"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answer: b)"})," DAgger addresses the covariate shift problem by collecting expert demonstrations on states visited by the learned policy, not just initial states."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Which type of learning is most appropriate when defining a reward function is challenging?\r\na) Supervised learning\r\nb) Reinforcement learning\r\nc) Imitation learning\r\nd) Unsupervised learning"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answer: c)"})," Imitation learning is ideal when reward functions are difficult to define, as it learns from expert demonstrations instead of relying on reward signals."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"What is the main purpose of Model Reference Adaptive Control (MRAC)?\r\na) To reduce computational complexity\r\nb) To make the plant behave like a desired reference model\r\nc) To eliminate the need for sensors\r\nd) To speed up training processes"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answer: b)"})," MRAC adjusts controller parameters to make the controlled system (plant) behave like a desired reference model."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Why is safe exploration critical in robotics learning systems?\r\na) To reduce computational costs\r\nb) To prevent damage to the robot or environment during learning\r\nc) To speed up the learning process\r\nd) To simplify algorithm design"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Answer: b)"})," Safe exploration is critical because physical robots can cause damage to themselves or their environment during the learning process if safety is not maintained."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Sutton, R. S., & Barto, A. G. (2018). "Reinforcement Learning: An Introduction". MIT Press. This foundational text covers the theory and practice of reinforcement learning, including applications to robotics.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Argall, B. D., Chernova, S., Veloso, M., & Browning, B. (2009). "A survey of robot learning from demonstration". Robotics and Autonomous Systems, 57(5), 469-483. A comprehensive overview of imitation learning techniques in robotics.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Slotine, J. J. E., & Li, W. (1991). "Applied Nonlinear Control". Prentice Hall. Covers adaptive control theory and its application to robotic systems.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Kober, J., Bagnell, J. A., & Peters, J. (2013). "Reinforcement learning in robotics: A survey". The International Journal of Robotics Research, 32(11), 1238-1274. An extensive survey of RL applications in robotics.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Taylor, M. E., & Stone, P. (2009). "Transfer learning for reinforcement learning domains: A survey". Journal of Machine Learning Research, 10(7). This paper explores various transfer learning techniques applicable to robotics.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Cheng, C. A., Rai, A., Losey, D., & O\'Malley, M. K. (2019). "CLARC: A unified framework for concurrent learning and robust control". IEEE Transactions on Robotics, 35(5), 1148-1165. Discusses safe learning and control integration.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Abbeel, P., Quigley, M., & Ng, A. Y. (2006). "Using inaccurate models in reinforcement learning". Proceedings of the 23rd international conference on Machine learning. This paper addresses practical challenges in applying RL to physical systems.'}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:'Chen, K., Bai, H., Hutter, M., & Fua, P. (2021). "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey". arXiv preprint arXiv:2103.12711. A comprehensive survey on sim-to-real transfer techniques for robotic learning.'}),"\n"]}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>s});var i=r(6540);const t={},a=i.createContext(t);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);