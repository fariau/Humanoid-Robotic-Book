"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[156],{2644:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var s=t(4848),o=t(8453);const i={sidebar_position:6,title:"Conversational Robotics",description:"Natural language interaction and dialogue systems for robots",slug:"/conversational-robotics"},r="Conversational Robotics",a={id:"conversational-robotics/index",title:"Conversational Robotics",description:"Natural language interaction and dialogue systems for robots",source:"@site/docs/conversational-robotics/index.mdx",sourceDirName:"conversational-robotics",slug:"/conversational-robotics",permalink:"/Humanoid-Robotic-Book/ur/docs/conversational-robotics",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/conversational-robotics/index.mdx",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6,title:"Conversational Robotics",description:"Natural language interaction and dialogue systems for robots",slug:"/conversational-robotics"},sidebar:"tutorialSidebar",previous:{title:"Vision Language Action (VLA) Models",permalink:"/Humanoid-Robotic-Book/ur/docs/vla"},next:{title:"Hardware Integration",permalink:"/Humanoid-Robotic-Book/ur/docs/hardware-integration"}},l={},c=[{value:"Introduction to Conversational Robotics",id:"introduction-to-conversational-robotics",level:2},{value:"Key Components of Conversational Robotics",id:"key-components-of-conversational-robotics",level:3},{value:"Speech Recognition and Natural Language Understanding",id:"speech-recognition-and-natural-language-understanding",level:2},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"Natural Language Understanding (NLU)",id:"natural-language-understanding-nlu",level:3},{value:"Dialogue Management",id:"dialogue-management",level:2},{value:"State Management",id:"state-management",level:3},{value:"Context Awareness",id:"context-awareness",level:3},{value:"Natural Language Generation",id:"natural-language-generation",level:2},{value:"Response Generation",id:"response-generation",level:3},{value:"Text-to-Speech Integration",id:"text-to-speech-integration",level:2},{value:"TTS Implementation",id:"tts-implementation",level:3},{value:"Hardware-Specific Optimizations",id:"hardware-specific-optimizations",level:2},{value:"For NVIDIA Jetson Users",id:"for-nvidia-jetson-users",level:3},{value:"For High-End GPU Users",id:"for-high-end-gpu-users",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Conversational Robotics Launch File",id:"conversational-robotics-launch-file",level:3},{value:"Multimodal Integration",id:"multimodal-integration",level:2},{value:"Combining Vision and Language",id:"combining-vision-and-language",level:3},{value:"Conversational AI Frameworks",id:"conversational-ai-frameworks",level:2},{value:"Integration with Popular Frameworks",id:"integration-with-popular-frameworks",level:3},{value:"Privacy and Security Considerations",id:"privacy-and-security-considerations",level:2},{value:"Secure Conversational Robotics",id:"secure-conversational-robotics",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Exercise 1: Basic Conversational System",id:"exercise-1-basic-conversational-system",level:3},{value:"Exercise 2: Context Awareness",id:"exercise-2-context-awareness",level:3},{value:"Exercise 3: Dialogue State Management",id:"exercise-3-dialogue-state-management",level:3},{value:"Exercise 4: Privacy Implementation",id:"exercise-4-privacy-implementation",level:3},{value:"Exercise 5: Hardware Optimization",id:"exercise-5-hardware-optimization",level:3},{value:"MCQs Quiz",id:"mcqs-quiz",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:["\n",(0,s.jsxs)("div",{className:"button-container",style:{marginBottom:"20px"},children:[(0,s.jsx)("button",{className:"personalize-button",onClick:()=>{alert("Personalization feature would activate based on your hardware profile (GPU: [user GPU], Jetson: [user Jetson status], Robot: [user robot type])")},style:{backgroundColor:"#4a6fa5",color:"white",border:"none",padding:"10px 15px",borderRadius:"5px",marginRight:"10px",cursor:"pointer"},children:(0,s.jsx)(n.p,{children:"Personalize to my hardware"})}),(0,s.jsx)("button",{className:"urdu-toggle-button",onClick:()=>{alert("Content would toggle between English and Urdu")},style:{backgroundColor:"#2e7d32",color:"white",border:"none",padding:"10px 15px",borderRadius:"5px",cursor:"pointer"},children:(0,s.jsx)(n.p,{children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u067e\u0691\u06be\u06cc\u06ba / Show in Urdu"})})]}),"\n",(0,s.jsx)(n.h1,{id:"conversational-robotics",children:"Conversational Robotics"}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-conversational-robotics",children:"Introduction to Conversational Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Conversational robotics represents the intersection of natural language processing, human-robot interaction, and artificial intelligence. It focuses on enabling robots to engage in meaningful, context-aware conversations with humans, going beyond simple command-response interactions to support complex, multi-turn dialogues that can adapt to the user's needs, preferences, and context."}),"\n",(0,s.jsx)(n.h3,{id:"key-components-of-conversational-robotics",children:"Key Components of Conversational Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converting speech to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interpreting user intent and extracting entities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dialogue Management"}),": Maintaining conversation context and state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Generation (NLG)"}),": Creating appropriate responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text-to-Speech (TTS)"}),": Converting text responses to audible speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Integration"}),": Incorporating visual and contextual information"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Conversational Robotics System] --\x3e B[Speech Input]\n    A --\x3e C[Visual Context]\n    A --\x3e D[Environmental Sensors]\n\n    B --\x3e E[ASR - Speech to Text]\n    E --\x3e F[NLU - Intent Recognition]\n    F --\x3e G[Dialogue Manager]\n\n    C --\x3e G\n    D --\x3e G\n\n    G --\x3e H[NLG - Response Generation]\n    H --\x3e I[TTS - Text to Speech]\n    I --\x3e J[Speech Output]\n\n    G --\x3e K[Action Execution]\n    K --\x3e L[Robot Movement/Actions]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"speech-recognition-and-natural-language-understanding",children:"Speech Recognition and Natural Language Understanding"}),"\n",(0,s.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,s.jsx)(n.p,{children:"ASR systems convert spoken language into text, forming the foundation of conversational robotics. Modern ASR systems leverage deep learning models trained on large datasets to achieve high accuracy."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example ASR implementation using speech recognition\nimport speech_recognition as sr\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass ASRNode(Node):\n    def __init__(self):\n        super().__init__('asr_node')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Create publisher for recognized text\n        self.text_pub = self.create_publisher(String, '/recognized_text', 10)\n\n        # Timer for continuous listening\n        self.listen_timer = self.create_timer(1.0, self.listen_for_speech)\n\n        self.get_logger().info('ASR node initialized')\n\n    def listen_for_speech(self):\n        \"\"\"Listen for speech and convert to text\"\"\"\n        try:\n            with self.microphone as source:\n                self.get_logger().info('Listening...')\n                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)\n\n            # Recognize speech using Google Web Speech API\n            text = self.recognizer.recognize_google(audio)\n            self.get_logger().info(f'Recognized: {text}')\n\n            # Publish recognized text\n            text_msg = String()\n            text_msg.data = text\n            self.text_pub.publish(text_msg)\n\n        except sr.WaitTimeoutError:\n            self.get_logger().info('Timeout: No speech detected')\n        except sr.UnknownValueError:\n            self.get_logger().info('Could not understand audio')\n        except sr.RequestError as e:\n            self.get_logger().error(f'Could not request results from speech service; {e}')\n        except Exception as e:\n            self.get_logger().error(f'Error in speech recognition: {e}')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"natural-language-understanding-nlu",children:"Natural Language Understanding (NLU)"}),"\n",(0,s.jsx)(n.p,{children:"NLU systems interpret user intent and extract relevant information from the recognized text:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Header\nimport re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass Intent:\n    name: str\n    confidence: float\n    entities: dict\n\nclass NLUProcessor(Node):\n    def __init__(self):\n        super().__init__('nlu_processor')\n\n        # Subscribe to recognized text\n        self.text_sub = self.create_subscription(\n            String, '/recognized_text', self.text_callback, 10)\n\n        # Create publisher for parsed intents\n        self.intent_pub = self.create_publisher(\n            String, '/parsed_intent', 10)\n\n        # Define intent patterns\n        self.intent_patterns = {\n            'navigation': [\n                r'move to (.+)',\n                r'go to (.+)',\n                r'go (.+)',\n                r'navigate to (.+)',\n                r'bring me to (.+)'\n            ],\n            'object_interaction': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'get (.+)',\n                r'pick (.+)',\n                r'take (.+)'\n            ],\n            'information_request': [\n                r'what is (.+)',\n                r'tell me about (.+)',\n                r'explain (.+)',\n                r'describe (.+)'\n            ],\n            'status_request': [\n                r'how are you',\n                r'what can you do',\n                r'what are you',\n                r'who are you'\n            ]\n        }\n\n        self.get_logger().info('NLU processor initialized')\n\n    def text_callback(self, msg):\n        \"\"\"Process recognized text and extract intent\"\"\"\n        text = msg.data.lower()\n        intent = self.parse_intent(text)\n\n        if intent:\n            self.get_logger().info(f'Parsed intent: {intent.name} with confidence {intent.confidence}')\n\n            # Publish intent as JSON string\n            import json\n            intent_json = {\n                'intent': intent.name,\n                'confidence': intent.confidence,\n                'entities': intent.entities,\n                'original_text': msg.data\n            }\n            intent_msg = String()\n            intent_msg.data = json.dumps(intent_json)\n            self.intent_pub.publish(intent_msg)\n\n    def parse_intent(self, text: str) -> Optional[Intent]:\n        \"\"\"Parse text to identify intent and extract entities\"\"\"\n        best_match = None\n        best_confidence = 0.0\n\n        for intent_name, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    # Calculate confidence based on pattern match\n                    confidence = 0.9 if match else 0.7\n\n                    if confidence > best_confidence:\n                        best_confidence = confidence\n                        entities = {}\n\n                        # Extract captured groups as entities\n                        if match.groups():\n                            entities['object'] = match.group(1).strip()\n\n                        best_match = Intent(\n                            name=intent_name,\n                            confidence=confidence,\n                            entities=entities\n                        )\n\n        return best_match if best_confidence > 0.5 else None\n"})}),"\n",(0,s.jsx)(n.h2,{id:"dialogue-management",children:"Dialogue Management"}),"\n",(0,s.jsx)(n.h3,{id:"state-management",children:"State Management"}),"\n",(0,s.jsx)(n.p,{children:"Effective dialogue management requires maintaining conversation context and state:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Header\nimport json\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\n\n@dataclass\nclass DialogueState:\n    context: Dict[str, Any]\n    last_intent: Optional[str]\n    last_entities: Dict[str, Any]\n    conversation_history: list\n    user_profile: Dict[str, Any]\n\nclass DialogueManager(Node):\n    def __init__(self):\n        super().__init__('dialogue_manager')\n\n        # Subscribe to parsed intents\n        self.intent_sub = self.create_subscription(\n            String, '/parsed_intent', self.intent_callback, 10)\n\n        # Subscribe to user profile updates\n        self.profile_sub = self.create_subscription(\n            String, '/user_profile', self.profile_callback, 10)\n\n        # Create publisher for responses\n        self.response_pub = self.create_publisher(\n            String, '/robot_response', 10)\n\n        # Initialize dialogue state\n        self.dialogue_state = DialogueState(\n            context={},\n            last_intent=None,\n            last_entities={},\n            conversation_history=[],\n            user_profile={}\n        )\n\n        self.get_logger().info('Dialogue manager initialized')\n\n    def intent_callback(self, msg):\n        \"\"\"Process incoming intent and generate response\"\"\"\n        try:\n            intent_data = json.loads(msg.data)\n\n            # Update dialogue state\n            self.dialogue_state.last_intent = intent_data['intent']\n            self.dialogue_state.last_entities = intent_data['entities']\n            self.dialogue_state.conversation_history.append({\n                'timestamp': datetime.now().isoformat(),\n                'type': 'user_input',\n                'data': intent_data\n            })\n\n            # Generate response based on intent and context\n            response = self.generate_response(intent_data)\n\n            # Update conversation history with response\n            self.dialogue_state.conversation_history.append({\n                'timestamp': datetime.now().isoformat(),\n                'type': 'robot_response',\n                'data': response\n            })\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.response_pub.publish(response_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing intent: {e}')\n\n    def generate_response(self, intent_data: dict) -> str:\n        \"\"\"Generate appropriate response based on intent and context\"\"\"\n        intent = intent_data['intent']\n        entities = intent_data['entities']\n\n        if intent == 'navigation':\n            location = entities.get('object', 'unknown location')\n            return self.handle_navigation_request(location)\n\n        elif intent == 'object_interaction':\n            object_name = entities.get('object', 'unknown object')\n            return self.handle_object_interaction_request(object_name)\n\n        elif intent == 'information_request':\n            topic = entities.get('object', 'unknown topic')\n            return self.handle_information_request(topic)\n\n        elif intent == 'status_request':\n            return self.handle_status_request()\n\n        else:\n            return \"I'm not sure how to help with that. Can you rephrase your request?\"\n\n    def handle_navigation_request(self, location: str) -> str:\n        \"\"\"Handle navigation requests\"\"\"\n        # Check if location is in known locations\n        known_locations = self.dialogue_state.context.get('known_locations', [])\n\n        if location in known_locations:\n            return f\"Okay, I'll navigate to {location}. Please follow me.\"\n        else:\n            return f\"I don't know where {location} is. Could you show me or provide more details?\"\n\n    def handle_object_interaction_request(self, object_name: str) -> str:\n        \"\"\"Handle object interaction requests\"\"\"\n        # Check if object is visible or in known objects\n        known_objects = self.dialogue_state.context.get('known_objects', [])\n\n        if object_name in known_objects:\n            return f\"I see the {object_name}. I'll pick it up for you.\"\n        else:\n            return f\"I don't see a {object_name} nearby. Can you point it out or describe its location?\"\n\n    def handle_information_request(self, topic: str) -> str:\n        \"\"\"Handle information requests\"\"\"\n        # Return information based on robot's knowledge\n        knowledge_base = self.dialogue_state.context.get('knowledge_base', {})\n\n        if topic in knowledge_base:\n            return knowledge_base[topic]\n        else:\n            return f\"I don't have information about {topic}. I can learn more if you'd like to teach me.\"\n\n    def handle_status_request(self) -> str:\n        \"\"\"Handle status requests\"\"\"\n        user_name = self.dialogue_state.user_profile.get('name', 'there')\n        capabilities = self.dialogue_state.context.get('capabilities', [])\n\n        response = f\"Hello {user_name}! I'm a conversational robot. I can help with navigation, object interaction, and answering questions. My capabilities include: {', '.join(capabilities)}. How can I assist you today?\"\n        return response\n\n    def profile_callback(self, msg):\n        \"\"\"Update user profile\"\"\"\n        try:\n            profile_data = json.loads(msg.data)\n            self.dialogue_state.user_profile.update(profile_data)\n        except Exception as e:\n            self.get_logger().error(f'Error updating profile: {e}')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"context-awareness",children:"Context Awareness"}),"\n",(0,s.jsx)(n.p,{children:"Context-aware dialogue systems consider environmental and situational factors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport json\n\nclass ContextAwareDialogue(Node):\n    def __init__(self):\n        super().__init__(\'context_aware_dialogue\')\n\n        # Subscribe to environmental sensors\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10)\n        self.pose_sub = self.create_subscription(\n            PoseStamped, \'/robot_pose\', self.pose_callback, 10)\n\n        # Subscribe to dialogue manager\n        self.dialogue_sub = self.create_subscription(\n            String, \'/robot_response\', self.dialogue_callback, 10)\n\n        # Publisher for context-enhanced responses\n        self.enhanced_response_pub = self.create_publisher(\n            String, \'/enhanced_response\', 10)\n\n        # Store environmental context\n        self.current_pose = None\n        self.visible_objects = []\n        self.obstacle_distances = []\n\n        self.get_logger().info(\'Context-aware dialogue system initialized\')\n\n    def image_callback(self, msg):\n        """Process camera image to identify visible objects"""\n        # In a real implementation, this would use computer vision\n        # to identify objects in the camera view\n        # For this example, we\'ll simulate object detection\n        self.visible_objects = ["red cup", "blue book", "green plant"]\n\n    def laser_callback(self, msg):\n        """Process LIDAR data to identify obstacles"""\n        # Process LIDAR scan to identify nearby obstacles\n        self.obstacle_distances = list(msg.ranges[:50])  # First 50 readings\n\n    def pose_callback(self, msg):\n        """Update robot\'s current pose"""\n        self.current_pose = msg.pose\n\n    def dialogue_callback(self, msg):\n        """Enhance response with environmental context"""\n        try:\n            response = msg.data\n\n            # Add context to response if relevant\n            enhanced_response = self.add_context_to_response(response)\n\n            # Publish enhanced response\n            enhanced_msg = String()\n            enhanced_msg.data = enhanced_response\n            self.enhanced_response_pub.publish(enhanced_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error enhancing response: {e}\')\n\n    def add_context_to_response(self, response: str) -> str:\n        """Add environmental context to the response"""\n        context_additions = []\n\n        # Add visible objects if relevant\n        if "what do you see" in response.lower() or "what is nearby" in response.lower():\n            if self.visible_objects:\n                context_additions.append(f"I can see: {\', \'.join(self.visible_objects)}")\n\n        # Add location information if relevant\n        if "where are you" in response.lower() or "location" in response.lower():\n            if self.current_pose:\n                context_additions.append(f"I\'m currently at position ({self.current_pose.position.x:.2f}, {self.current_pose.position.y:.2f})")\n\n        # Add obstacle information if relevant\n        if "move" in response.lower() or "navigate" in response.lower():\n            if self.obstacle_distances:\n                min_distance = min([d for d in self.obstacle_distances if d > 0])\n                if min_distance < 1.0:  # Less than 1 meter\n                    context_additions.append(f"Warning: Obstacle detected {min_distance:.2f}m ahead")\n\n        # Combine original response with context\n        if context_additions:\n            return f"{response} {\'. \'.join(context_additions)}."\n\n        return response\n'})}),"\n",(0,s.jsx)(n.h2,{id:"natural-language-generation",children:"Natural Language Generation"}),"\n",(0,s.jsx)(n.h3,{id:"response-generation",children:"Response Generation"}),"\n",(0,s.jsx)(n.p,{children:"NLG systems create appropriate responses based on the dialogue state and intent:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport random\nimport json\n\nclass NaturalLanguageGenerator(Node):\n    def __init__(self):\n        super().__init__(\'nlg_node\')\n\n        # Subscribe to dialogue manager output\n        self.dialogue_sub = self.create_subscription(\n            String, \'/enhanced_response\', self.response_callback, 10)\n\n        # Create publisher for final responses\n        self.final_response_pub = self.create_publisher(\n            String, \'/final_response\', 10)\n\n        # Define response templates\n        self.response_templates = {\n            \'navigation_success\': [\n                "I\'m on my way to {location}.",\n                "Navigating to {location} now.",\n                "Heading to {location} as requested."\n            ],\n            \'navigation_failure\': [\n                "I\'m having trouble reaching {location}.",\n                "I can\'t navigate to {location} right now.",\n                "Unable to reach {location}. Please try another location."\n            ],\n            \'object_success\': [\n                "I\'ve picked up the {object}.",\n                "Got the {object} for you.",\n                "Successfully retrieved the {object}."\n            ],\n            \'object_failure\': [\n                "I couldn\'t find the {object}.",\n                "Unable to locate the {object}.",\n                "I don\'t see the {object} nearby."\n            ],\n            \'greeting\': [\n                "Hello! How can I help you?",\n                "Hi there! What can I do for you?",\n                "Greetings! How may I assist you?"\n            ]\n        }\n\n        self.get_logger().info(\'Natural Language Generator initialized\')\n\n    def response_callback(self, msg):\n        """Process and enhance response"""\n        try:\n            # If the message is JSON (from dialogue manager), parse it\n            try:\n                response_data = json.loads(msg.data)\n                response_text = response_data.get(\'response\', msg.data)\n            except json.JSONDecodeError:\n                response_text = msg.data\n\n            # Generate enhanced response\n            enhanced_response = self.generate_enhanced_response(response_text)\n\n            # Publish final response\n            final_msg = String()\n            final_msg.data = enhanced_response\n            self.final_response_pub.publish(final_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in NLG: {e}\')\n\n    def generate_enhanced_response(self, base_response: str) -> str:\n        """Enhance response with personality and natural language"""\n        # Add personality markers\n        polite_markers = ["Please", "Thank you", "You\'re welcome", "Sure", "Of course"]\n        uncertainty_markers = ["I think", "It seems", "Possibly", "Maybe", "Perhaps"]\n\n        # Randomly add politeness\n        if random.random() < 0.3:  # 30% chance\n            return f"Sure, {base_response.lower()}"\n\n        return base_response\n'})}),"\n",(0,s.jsx)(n.h2,{id:"text-to-speech-integration",children:"Text-to-Speech Integration"}),"\n",(0,s.jsx)(n.h3,{id:"tts-implementation",children:"TTS Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Converting text responses to speech for human-robot interaction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport subprocess\nimport os\n\nclass TextToSpeechNode(Node):\n    def __init__(self):\n        super().__init__('tts_node')\n\n        # Subscribe to final responses\n        self.response_sub = self.create_subscription(\n            String, '/final_response', self.response_callback, 10)\n\n        self.get_logger().info('Text-to-Speech node initialized')\n\n    def response_callback(self, msg):\n        \"\"\"Convert text response to speech\"\"\"\n        try:\n            text = msg.data\n            self.speak_text(text)\n        except Exception as e:\n            self.get_logger().error(f'Error in TTS: {e}')\n\n    def speak_text(self, text: str):\n        \"\"\"Speak the given text using system TTS\"\"\"\n        try:\n            # Use espeak as an example TTS engine\n            # In a real system, you might use more advanced TTS like Festival, MaryTTS, or cloud services\n            subprocess.run(['espeak', text], check=True)\n        except subprocess.CalledProcessError:\n            # Fallback: print to console if TTS fails\n            self.get_logger().info(f'[TTS Fallback] Robot says: {text}')\n        except FileNotFoundError:\n            # espeak not installed, use alternative\n            self.get_logger().info(f'[TTS Fallback] Robot says: {text}')\n            # Could also use: espeak-ng, festival, or cloud TTS services\n"})}),"\n",(0,s.jsx)(n.h2,{id:"hardware-specific-optimizations",children:"Hardware-Specific Optimizations"}),"\n",(0,s.jsx)(n.h3,{id:"for-nvidia-jetson-users",children:"For NVIDIA Jetson Users"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Jetson-specific conversational robotics optimizations\nimport rclpy\nfrom rclpy.node import Node\nimport subprocess\nimport os\nimport numpy as np\n\nclass JetsonConversationalNode(Node):\n    def __init__(self):\n        super().__init__('jetson_conversational_node')\n\n        # Optimize for Jetson's ARM architecture and GPU\n        self.setup_jetson_optimizations()\n\n        self.get_logger().info('Jetson-optimized conversational robotics node initialized')\n\n    def setup_jetson_optimizations(self):\n        \"\"\"Configure conversational system for Jetson hardware\"\"\"\n        # Set environment variables for Jetson optimization\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n        os.environ['NVIDIA_VISIBLE_DEVICES'] = 'all'\n\n        # Use lightweight models optimized for Jetson\n        # Enable hardware acceleration for audio processing\n        # Optimized for [USER_GPU] hardware\n        pass\n\n    def jetson_speech_processing(self, audio_data):\n        \"\"\"\n        Process speech using Jetson's capabilities\n        Optimized for [USER_GPU] hardware\n        \"\"\"\n        # Use Jetson's hardware accelerators for audio processing\n        # This would leverage Jetson's integrated GPU and audio processing units\n        pass\n"})}),"\n",(0,s.jsx)(n.h3,{id:"for-high-end-gpu-users",children:"For High-End GPU Users"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# High-end GPU conversational robotics optimizations\nimport rclpy\nfrom rclpy.node import Node\nimport torch\nimport os\n\nclass GPUOptimizedConversationalNode(Node):\n    def __init__(self):\n        super().__init__('gpu_optimized_conversational_node')\n\n        # Initialize GPU context for conversational AI\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Configure conversational system for high-end GPU\n        self.setup_gpu_optimizations()\n\n        self.get_logger().info('GPU-optimized conversational robotics node initialized')\n\n    def setup_gpu_optimizations(self):\n        \"\"\"Configure conversational system for high-end GPU hardware\"\"\"\n        # Set environment variables for GPU optimization\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n        os.environ['NVIDIA_VISIBLE_DEVICES'] = 'all'\n\n        # Enable advanced conversational AI features for powerful GPUs\n        # Use multi-GPU processing if available\n        # Optimized for [USER_GPU] hardware specifications\n        pass\n\n    def gpu_intensive_nlp(self, text):\n        \"\"\"\n        Perform GPU-intensive NLP processing\n        Optimized for [USER_GPU] hardware specifications\n        \"\"\"\n        # Use GPU-accelerated NLP models\n        # Leverage tensor cores for efficient processing\n        # Implement batch processing for efficiency\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"conversational-robotics-launch-file",children:"Conversational Robotics Launch File"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# launch/conversational_robot.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true'\n    )\n\n    # ASR node\n    asr_node = Node(\n        package='conversational_robot',\n        executable='asr_node',\n        name='asr_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # NLU processor\n    nlu_node = Node(\n        package='conversational_robot',\n        executable='nlu_processor',\n        name='nlu_processor',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # Dialogue manager\n    dialogue_node = Node(\n        package='conversational_robot',\n        executable='dialogue_manager',\n        name='dialogue_manager',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # Natural language generator\n    nlg_node = Node(\n        package='conversational_robot',\n        executable='nlg_node',\n        name='nlg_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    # Text-to-speech node\n    tts_node = Node(\n        package='conversational_robot',\n        executable='tts_node',\n        name='tts_node',\n        parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        use_sim_time,\n        asr_node,\n        nlu_node,\n        dialogue_node,\n        nlg_node,\n        tts_node,\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-integration",children:"Multimodal Integration"}),"\n",(0,s.jsx)(n.h3,{id:"combining-vision-and-language",children:"Combining Vision and Language"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass MultimodalConversationalNode(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_conversational_node\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Subscribe to camera and text input\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.text_sub = self.create_subscription(\n            String, \'/user_input\', self.text_callback, 10)\n\n        # Publisher for multimodal responses\n        self.multimodal_response_pub = self.create_publisher(\n            String, \'/multimodal_response\', 10)\n\n        # Store latest image\n        self.latest_image = None\n\n        self.get_logger().info(\'Multimodal conversational node initialized\')\n\n    def image_callback(self, msg):\n        """Process camera image"""\n        try:\n            # Convert ROS Image to OpenCV\n            self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def text_callback(self, msg):\n        """Process text with visual context"""\n        try:\n            text = msg.data\n\n            # If we have a recent image, combine vision and language\n            if self.latest_image is not None:\n                response = self.process_multimodal_request(text, self.latest_image)\n            else:\n                response = self.process_text_only_request(text)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.multimodal_response_pub.publish(response_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing multimodal request: {e}\')\n\n    def process_multimodal_request(self, text, image):\n        """Process request combining text and visual information"""\n        # This would integrate vision and language processing\n        # For example, if user says "What color is that object?"\n        # the system would analyze the image to identify objects and their colors\n        if "color" in text.lower() and "object" in text.lower():\n            # Analyze image to identify colors of objects\n            # This is a simplified example\n            return "I can see several objects with different colors in the image."\n\n        return f"You said: \'{text}\'. I can also see visual information."\n\n    def process_text_only_request(self, text):\n        """Process request with text only"""\n        return f"You said: \'{text}\'. I don\'t have visual information right now."\n'})}),"\n",(0,s.jsx)(n.h2,{id:"conversational-ai-frameworks",children:"Conversational AI Frameworks"}),"\n",(0,s.jsx)(n.h3,{id:"integration-with-popular-frameworks",children:"Integration with Popular Frameworks"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Integration with Rasa (popular conversational AI framework)\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport requests\nimport json\n\nclass RasaIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('rasa_integration_node')\n\n        # Subscribe to processed text\n        self.text_sub = self.create_subscription(\n            String, '/processed_text', self.text_callback, 10)\n\n        # Publisher for Rasa responses\n        self.rasa_response_pub = self.create_publisher(\n            String, '/rasa_response', 10)\n\n        # Rasa server configuration\n        self.rasa_server_url = \"http://localhost:5005/webhooks/rest/webhook\"\n\n        self.get_logger().info('Rasa integration node initialized')\n\n    def text_callback(self, msg):\n        \"\"\"Send text to Rasa and receive response\"\"\"\n        try:\n            text = msg.data\n\n            # Prepare payload for Rasa\n            payload = {\n                \"sender\": \"robot\",\n                \"message\": text\n            }\n\n            # Send to Rasa\n            response = requests.post(\n                self.rasa_server_url,\n                json=payload,\n                headers={'Content-Type': 'application/json'}\n            )\n\n            if response.status_code == 200:\n                rasa_response = response.json()\n                if rasa_response:\n                    # Extract response text\n                    bot_response = rasa_response[0].get('text', 'I did not understand that.')\n\n                    # Publish response\n                    response_msg = String()\n                    response_msg.data = bot_response\n                    self.rasa_response_pub.publish(response_msg)\n            else:\n                self.get_logger().error(f'Rasa request failed with status {response.status_code}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error communicating with Rasa: {e}')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"privacy-and-security-considerations",children:"Privacy and Security Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"secure-conversational-robotics",children:"Secure Conversational Robotics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport hashlib\nimport json\nfrom cryptography.fernet import Fernet\n\nclass SecureConversationalNode(Node):\n    def __init__(self):\n        super().__init__('secure_conversational_node')\n\n        # Subscribe to user input\n        self.input_sub = self.create_subscription(\n            String, '/user_input', self.secure_input_callback, 10)\n\n        # Initialize encryption\n        self.cipher_suite = Fernet(Fernet.generate_key())\n\n        self.get_logger().info('Secure conversational node initialized')\n\n    def secure_input_callback(self, msg):\n        \"\"\"Process input with privacy considerations\"\"\"\n        try:\n            text = msg.data\n\n            # Anonymize sensitive information\n            anonymized_text = self.anonymize_sensitive_info(text)\n\n            # Log securely (don't store raw text)\n            self.log_securely(anonymized_text)\n\n            # Process the anonymized text\n            self.process_anonymized_input(anonymized_text)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in secure processing: {e}')\n\n    def anonymize_sensitive_info(self, text: str) -> str:\n        \"\"\"Remove or anonymize sensitive information from text\"\"\"\n        import re\n\n        # This is a basic example - real implementation would be more sophisticated\n        # Remove potential email addresses\n        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n\n        # Remove potential phone numbers\n        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n\n        # In a real system, you'd also handle names, addresses, etc.\n\n        return text\n\n    def log_securely(self, text: str):\n        \"\"\"Log information securely without storing sensitive data\"\"\"\n        # Create hash of the text for reference without storing the actual content\n        text_hash = hashlib.sha256(text.encode()).hexdigest()\n        self.get_logger().info(f'Processed text with hash: {text_hash[:8]}...')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Integration"}),": Conversational robotics combines speech, vision, and contextual information for richer interactions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pipeline Components"}),": ASR, NLU, Dialogue Management, NLG, and TTS form the core processing pipeline."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context Awareness"}),": Effective systems consider environmental and situational context for more natural interactions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hardware Optimization"}),": Different strategies are needed for different hardware platforms (Jetson vs. high-end GPUs)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Privacy Considerations"}),": Conversational systems must handle sensitive user information appropriately."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": Systems must balance accuracy with real-time response requirements."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Integration"}),": Conversational robotics benefits from integration with established frameworks and tools."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-basic-conversational-system",children:"Exercise 1: Basic Conversational System"}),"\n",(0,s.jsx)(n.p,{children:"Implement a simple conversational system with ASR, NLU, and TTS components that can handle basic commands."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-context-awareness",children:"Exercise 2: Context Awareness"}),"\n",(0,s.jsx)(n.p,{children:"Enhance your conversational system to consider environmental context from sensors and camera."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-dialogue-state-management",children:"Exercise 3: Dialogue State Management"}),"\n",(0,s.jsx)(n.p,{children:"Implement a dialogue manager that maintains conversation context across multiple turns."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-4-privacy-implementation",children:"Exercise 4: Privacy Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Add privacy-preserving features to your conversational system to protect user data."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-5-hardware-optimization",children:"Exercise 5: Hardware Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Optimize your conversational system for your specific hardware configuration (GPU/Jetson) and measure performance."}),"\n",(0,s.jsx)(n.h2,{id:"mcqs-quiz",children:"MCQs Quiz"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What does ASR stand for in conversational robotics?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Automatic Speech Recognition"}),"\n",(0,s.jsx)(n.li,{children:"B) Advanced Speech Robotics"}),"\n",(0,s.jsx)(n.li,{children:"C) Audio Signal Reception"}),"\n",(0,s.jsx)(n.li,{children:"D) Automated Speech Response"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Answer: A"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Which component is responsible for interpreting user intent in conversational robotics?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) ASR"}),"\n",(0,s.jsx)(n.li,{children:"B) TTS"}),"\n",(0,s.jsx)(n.li,{children:"C) NLU (Natural Language Understanding)"}),"\n",(0,s.jsx)(n.li,{children:"D) NLG"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Answer: C"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is the role of dialogue management in conversational robotics?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Converting text to speech only"}),"\n",(0,s.jsx)(n.li,{children:"B) Maintaining conversation context and state"}),"\n",(0,s.jsx)(n.li,{children:"C) Recognizing speech only"}),"\n",(0,s.jsx)(n.li,{children:"D) Generating random responses"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Which of these is NOT a component of a conversational robotics system?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Automatic Speech Recognition"}),"\n",(0,s.jsx)(n.li,{children:"B) Natural Language Understanding"}),"\n",(0,s.jsx)(n.li,{children:"C) Computer Vision Processing"}),"\n",(0,s.jsx)(n.li,{children:"D) Text-to-Speech Synthesis"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Answer: C"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"What is a key challenge in conversational robotics?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A) Too much processing power"}),"\n",(0,s.jsx)(n.li,{children:"B) Maintaining context in multi-turn conversations"}),"\n",(0,s.jsx)(n.li,{children:"C) Excessive storage requirements"}),"\n",(0,s.jsx)(n.li,{children:"D) Overly simple user requests"}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Answer: B"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Conversational Robotics: An Introduction" by Brian Scassellati'}),"\n",(0,s.jsx)(n.li,{children:'"Human-Robot Interaction: A Survey" - Foundations and Trends in Robotics'}),"\n",(0,s.jsx)(n.li,{children:'"Spoken Language Processing in Robotics" - IEEE Transactions on Robotics'}),"\n",(0,s.jsxs)(n.li,{children:["Rasa Documentation: ",(0,s.jsx)(n.a,{href:"https://rasa.com/docs/",children:"https://rasa.com/docs/"})]}),"\n",(0,s.jsxs)(n.li,{children:["ROS 2 Navigation: ",(0,s.jsx)(n.a,{href:"https://navigation.ros.org/",children:"https://navigation.ros.org/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Speech Recognition with Python: ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/SpeechRecognition/",children:"https://pypi.org/project/SpeechRecognition/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Google Cloud Speech-to-Text: ",(0,s.jsx)(n.a,{href:"https://cloud.google.com/speech-to-text",children:"https://cloud.google.com/speech-to-text"})]}),"\n",(0,s.jsxs)(n.li,{children:["Amazon Polly (TTS): ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/polly/",children:"https://aws.amazon.com/polly/"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Generated with reusable Claude Subagents & Spec-Kit Plus"})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);